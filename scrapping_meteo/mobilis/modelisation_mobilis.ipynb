{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e28423",
   "metadata": {},
   "source": [
    "# Scrapping de la balise méteo de mobilis DB8000 du phrare du planier\n",
    "https://portal.mobilis-sa.com:3000/d/e9ad019c-7bc0-4d90-ab9a-3146bffa7b1d/db-planier-meteo?from=now-30d&to=now&orgId=2&refresh=auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91356a",
   "metadata": {},
   "source": [
    "pour windsup \n",
    "https://www.winds-up.com/spot-marseille-pointe-rouge-digue-windsurf-kitesurf-44-observations-releves-vent.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da808b87",
   "metadata": {},
   "source": [
    "### drive de sotckage des datasets\n",
    "\n",
    "https://drive.google.com/drive/folders/1vG0EGdLS91CY-TJV6FK100vkKFna_dzv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e79a98",
   "metadata": {},
   "source": [
    "# Parametrage notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c82fd",
   "metadata": {},
   "source": [
    "## Installation modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb7b6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install certifi\n",
    "# !python -m pip install certifi\n",
    "# !pip install --upgrade pyppeteer\n",
    "# !pip install urllib3\n",
    "# !pip install selenium\n",
    "# !pip install webdriver-manager\n",
    "# !pip install selenium --upgrade\n",
    "\n",
    "# !pip install numpy==1.24.3\n",
    "# !pip install h5py\n",
    "# !pip install wrapt>=1.11.0\n",
    "# !pip install click==7.1.1\n",
    "# !pip install networkx>=2.4\n",
    "# !pip install pandas>=0.25.3\n",
    "# !pip install pydantic\n",
    "# !pip install requests\n",
    "# !pip install smart-open\n",
    "# !pip install statsmodels\n",
    "# !pip install dateparser\n",
    "# !pip install tensorflow\n",
    "# ! pip install \"unsloth[cu121-ampere-torch220] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !setx PATH \"%PATH%;C:\\Users\\romar\\AppData\\Roaming\\Python\\Python311\\Scripts\"\n",
    "# !pip install xgboost\n",
    "# !pip install tensorboard\n",
    "# !pip install pmdarima\n",
    "# !pip install pytz\n",
    "# !pip install --upgrade pmdarima statsmodels numpy pandas\n",
    "# !pip install line_profiler\n",
    "# !pip install memory_profiler\n",
    "# !pip install --upgrade  pip\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ffe5d",
   "metadata": {},
   "source": [
    "## Ajout du chemin outils dans le PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ab762b",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#chargement du profiler de charge vscode line_profiler\n",
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler\n",
    "print(sys.executable)\n",
    "file_to_add = r'C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\outils'\n",
    "sys.path.append(file_to_add.replace('\\\\', '/'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711678d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3eaa13",
   "metadata": {},
   "source": [
    "## Mise en place logging \n",
    "appel à la fonction ***def reconfigurer_logging(params)***: \n",
    "\n",
    "    \"\"\"\n",
    "    Reconfigure le logging avec le niveau et le format spécifiés à partir du dictionnaire params,\n",
    "    en ajoutant des couleurs au logs selon leur niveau de gravité.\n",
    "    \"\"\"\n",
    "\n",
    "    # Assurez-vous que 'niveau_log' et 'format_log' sont définis dans params\n",
    "    \n",
    "    niveau = params['niveau_log'].upper()\n",
    "    format_log = params['format_log']\n",
    "    format_log_file = params['format_log_file']......\n",
    "\n",
    "\n",
    "debugeur avec le mot magique %debug\n",
    "\n",
    "- n (next) : Exécute la ligne suivante du programme. Si la ligne suivante est une fonction, elle exécutera la fonction sans entrer dans celle-ci.\n",
    "- s (step) : Exécute la ligne suivante du programme et entre dans les fonctions appelées.\n",
    "- c (continue) : Continue l'exécution jusqu'au prochain point d'arrêt.\n",
    "- q (quit) : Quitte le débogueur et termine l'exécution du programme.\n",
    "- l (list) : Affiche l'emplacement actuel dans le code.\n",
    "- p : Imprime la valeur de l'expression qui suit (par exemple, p variable_name affiche la valeur de variable_name).\n",
    "- h (help) : Affiche une liste des commandes disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b8848a",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from get_path_dossier_or_files import trouver_chemin_element_depuis_workspace\n",
    "\n",
    "# projet_meteo\\Projet_Meteo\\outils\\get_path_dossier_or_files.py\n",
    "\n",
    "chemin_outils = trouver_chemin_element_depuis_workspace(\"outils\", est_un_dossier=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee7aeb",
   "metadata": {},
   "source": [
    "## Import des outils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db36b3a",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=gestion_logging.py:ligne=160:fonction=reconfigurer_logging():'chemin_du_script  C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\outils'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=gestion_logging.py:ligne=163:fonction=reconfigurer_logging():'chemin_racine_projet  C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\outils\\..\\..'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=gestion_logging.py:ligne=166:fonction=reconfigurer_logging():'chemin_racine_projet  C:\\programmation\\IA\\Projet_Meteo\\projet_meteo'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=24:fonction=trouver_nom_workspace():'\n",
      " fichier_marqueur:\n",
      ".vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=26:fonction=trouver_nom_workspace():'\n",
      " chemin_courant:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=59:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " nom_dossier_workspace:\n",
      "projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=60:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin_workspace:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[35m-log:30-Apr-2024 à 10h19\n",
      "INFO>>>fichier=gestion_logging.py:ligne=176:fonction=reconfigurer_logging():'path_fichier_log via fonction    C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\log\\fichier.log'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=gestion_logging.py:ligne=196:fonction=reconfigurer_logging():'max_taille_fichier  104857600,nb_fichiers_backup 10'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=24:fonction=trouver_nom_workspace():'\n",
      " fichier_marqueur:\n",
      ".vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=26:fonction=trouver_nom_workspace():'\n",
      " chemin_courant:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=59:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " nom_dossier_workspace:\n",
      "projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=60:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin_workspace:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=66:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " sys.path :\n",
      "['c:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Scrapping\\\\mobilis', 'c:\\\\ProgramData\\\\Anaconda3\\\\python311.zip', 'c:\\\\ProgramData\\\\Anaconda3\\\\DLLs', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib', 'c:\\\\ProgramData\\\\Anaconda3', '', 'C:\\\\Users\\\\romar\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\Pythonwin', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\outils', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\log\\\\fichier.log'] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=67:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\data_mobilis\\upload_data_depuis_api \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=sauve_charge_df_csv_json_pkl.py:ligne=26:fonction=<module>():'\n",
      " path_data_mobilis_upload_data_depuis_api:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\data_mobilis\\upload_data_depuis_api \n",
      "'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "#ajout dans le path du dossier outils afin de faire les autres imports\n",
    "file_to_add = r'C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\outils'\n",
    "sys.path.append(file_to_add.replace('\\\\', '/'))\n",
    "# import des outils externes\n",
    "from sauve_charge_df_csv_json_pkl import *\n",
    "from descriptif_dataset_meteo_france import *\n",
    "from gestion_logging import *\n",
    "from get_path_dossier_or_files import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b789e556",
   "metadata": {},
   "source": [
    "## ajout dans le path des dossiers pour upload les datasets upload_data_depuis_api ou autres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f222a3ae",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=24:fonction=trouver_nom_workspace():'\n",
      " fichier_marqueur:\n",
      ".vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=26:fonction=trouver_nom_workspace():'\n",
      " chemin_courant:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=59:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " nom_dossier_workspace:\n",
      "projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=60:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin_workspace:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=66:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " sys.path :\n",
      "['c:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Scrapping\\\\mobilis', 'c:\\\\ProgramData\\\\Anaconda3\\\\python311.zip', 'c:\\\\ProgramData\\\\Anaconda3\\\\DLLs', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib', 'c:\\\\ProgramData\\\\Anaconda3', '', 'C:\\\\Users\\\\romar\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\Pythonwin', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\outils', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\log\\\\fichier.log', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\data_mobilis\\\\upload_data_depuis_api', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo'] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=67:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\data_mobilis\\upload_data_depuis_api \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=709704786.py:ligne=11:fonction=<module>():'\n",
      " path_data_mobilis_upload_data_depuis_api:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\data_mobilis\\upload_data_depuis_api '\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=24:fonction=trouver_nom_workspace():'\n",
      " fichier_marqueur:\n",
      ".vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=26:fonction=trouver_nom_workspace():'\n",
      " chemin_courant:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=59:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " nom_dossier_workspace:\n",
      "projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=60:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin_workspace:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=66:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " sys.path :\n",
      "['c:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Scrapping\\\\mobilis', 'c:\\\\ProgramData\\\\Anaconda3\\\\python311.zip', 'c:\\\\ProgramData\\\\Anaconda3\\\\DLLs', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib', 'c:\\\\ProgramData\\\\Anaconda3', '', 'C:\\\\Users\\\\romar\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\Pythonwin', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\outils', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\log\\\\fichier.log', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\data_mobilis\\\\upload_data_depuis_api', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo'] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=67:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\upload_dataset_depuis_api \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=709704786.py:ligne=17:fonction=<module>():'\n",
      " path_data_meteo_france_upload_data_depuis_api:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\upload_dataset_depuis_api '\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=24:fonction=trouver_nom_workspace():'\n",
      " fichier_marqueur:\n",
      ".vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=26:fonction=trouver_nom_workspace():'\n",
      " chemin_courant:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=59:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " nom_dossier_workspace:\n",
      "projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=60:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin_workspace:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=66:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " sys.path :\n",
      "['c:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Scrapping\\\\mobilis', 'c:\\\\ProgramData\\\\Anaconda3\\\\python311.zip', 'c:\\\\ProgramData\\\\Anaconda3\\\\DLLs', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib', 'c:\\\\ProgramData\\\\Anaconda3', '', 'C:\\\\Users\\\\romar\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\Pythonwin', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\outils', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\log\\\\fichier.log', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\data_mobilis\\\\upload_data_depuis_api', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\upload_dataset_depuis_api'] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=67:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\upload_dataset_depuis_api2 \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=709704786.py:ligne=22:fonction=<module>():'\n",
      " path_data_meteo_france_upload_data_depuis_api2:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\upload_dataset_depuis_api2 '\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=24:fonction=trouver_nom_workspace():'\n",
      " fichier_marqueur:\n",
      ".vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=26:fonction=trouver_nom_workspace():'\n",
      " chemin_courant:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=59:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " nom_dossier_workspace:\n",
      "projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=60:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin_workspace:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=66:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " sys.path :\n",
      "['c:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Scrapping\\\\mobilis', 'c:\\\\ProgramData\\\\Anaconda3\\\\python311.zip', 'c:\\\\ProgramData\\\\Anaconda3\\\\DLLs', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib', 'c:\\\\ProgramData\\\\Anaconda3', '', 'C:\\\\Users\\\\romar\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\Pythonwin', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\outils', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\log\\\\fichier.log', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\data_mobilis\\\\upload_data_depuis_api', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\upload_dataset_depuis_api', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\upload_dataset_depuis_api2'] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=67:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\upload_dataset_depuis_api2\\sauve_df_col_renamed \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=709704786.py:ligne=27:fonction=<module>():'\n",
      " path_data_meteo_france_api2_col_renamed:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\upload_dataset_depuis_api2\\sauve_df_col_renamed '\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=24:fonction=trouver_nom_workspace():'\n",
      " fichier_marqueur:\n",
      ".vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=26:fonction=trouver_nom_workspace():'\n",
      " chemin_courant:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=59:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " nom_dossier_workspace:\n",
      "projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=60:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin_workspace:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=66:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " sys.path :\n",
      "['c:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Scrapping\\\\mobilis', 'c:\\\\ProgramData\\\\Anaconda3\\\\python311.zip', 'c:\\\\ProgramData\\\\Anaconda3\\\\DLLs', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib', 'c:\\\\ProgramData\\\\Anaconda3', '', 'C:\\\\Users\\\\romar\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\Pythonwin', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\outils', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\log\\\\fichier.log', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\data_mobilis\\\\upload_data_depuis_api', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\upload_dataset_depuis_api', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\upload_dataset_depuis_api2', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\upload_dataset_depuis_api2\\\\sauve_df_col_renamed'] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=67:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\DONNEES_6_MNs_POUR_WIND \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=709704786.py:ligne=37:fonction=<module>():'\n",
      " path_datasets_fourni_par_meteo_france_6min_source:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\DONNEES_6_MNs_POUR_WIND '\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=24:fonction=trouver_nom_workspace():'\n",
      " fichier_marqueur:\n",
      ".vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=26:fonction=trouver_nom_workspace():'\n",
      " chemin_courant:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=59:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " nom_dossier_workspace:\n",
      "projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=60:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin_workspace:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=66:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " sys.path :\n",
      "['c:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Scrapping\\\\mobilis', 'c:\\\\ProgramData\\\\Anaconda3\\\\python311.zip', 'c:\\\\ProgramData\\\\Anaconda3\\\\DLLs', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib', 'c:\\\\ProgramData\\\\Anaconda3', '', 'C:\\\\Users\\\\romar\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\Pythonwin', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\outils', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\log\\\\fichier.log', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\data_mobilis\\\\upload_data_depuis_api', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\upload_dataset_depuis_api', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\upload_dataset_depuis_api2', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\upload_dataset_depuis_api2\\\\sauve_df_col_renamed', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\DONNEES_6_MNs_POUR_WIND'] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=67:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\DONNEES_6_MNs_POUR_WIND\\traitement_datasets_fourni_par_meteo_france_6min \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=709704786.py:ligne=42:fonction=<module>():'\n",
      " path_sauvegarde_datasets_meteo_france_6min_clean:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\DONNEES_6_MNs_POUR_WIND\\traitement_datasets_fourni_par_meteo_france_6min '\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=24:fonction=trouver_nom_workspace():'\n",
      " fichier_marqueur:\n",
      ".vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=26:fonction=trouver_nom_workspace():'\n",
      " chemin_courant:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=59:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " nom_dossier_workspace:\n",
      "projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=60:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin_workspace:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=66:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " sys.path :\n",
      "['c:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Scrapping\\\\mobilis', 'c:\\\\ProgramData\\\\Anaconda3\\\\python311.zip', 'c:\\\\ProgramData\\\\Anaconda3\\\\DLLs', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib', 'c:\\\\ProgramData\\\\Anaconda3', '', 'C:\\\\Users\\\\romar\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\Pythonwin', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\outils', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\log\\\\fichier.log', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\data_mobilis\\\\upload_data_depuis_api', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\upload_dataset_depuis_api', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\upload_dataset_depuis_api2', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\upload_dataset_depuis_api2\\\\sauve_df_col_renamed', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\DONNEES_6_MNs_POUR_WIND', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\DONNEES_6_MNs_POUR_WIND\\\\traitement_datasets_fourni_par_meteo_france_6min'] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=67:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\DONNEES_6_MNs_POUR_WIND\\traitement_datasets_fourni_par_meteo_france_6min_clean \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=709704786.py:ligne=47:fonction=<module>():'\n",
      " path_sauvegarde_datasets_meteo_france_6min_suppression_col_vide:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\DONNEES_6_MNs_POUR_WIND\\traitement_datasets_fourni_par_meteo_france_6min_clean '\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=24:fonction=trouver_nom_workspace():'\n",
      " fichier_marqueur:\n",
      ".vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=26:fonction=trouver_nom_workspace():'\n",
      " chemin_courant:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=59:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " nom_dossier_workspace:\n",
      "projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=60:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin_workspace:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=66:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " sys.path :\n",
      "['c:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Scrapping\\\\mobilis', 'c:\\\\ProgramData\\\\Anaconda3\\\\python311.zip', 'c:\\\\ProgramData\\\\Anaconda3\\\\DLLs', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib', 'c:\\\\ProgramData\\\\Anaconda3', '', 'C:\\\\Users\\\\romar\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\Pythonwin', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\outils', 'C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/outils', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\log\\\\fichier.log', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\data_mobilis\\\\upload_data_depuis_api', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\upload_dataset_depuis_api', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\upload_dataset_depuis_api2', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\upload_dataset_depuis_api2\\\\sauve_df_col_renamed', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\DONNEES_6_MNs_POUR_WIND', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\DONNEES_6_MNs_POUR_WIND\\\\traitement_datasets_fourni_par_meteo_france_6min', 'C:\\\\programmation\\\\IA\\\\Projet_Meteo\\\\projet_meteo\\\\Projet_Meteo\\\\Datasets\\\\meteo_france\\\\DONNEES_6_MNs_POUR_WIND\\\\traitement_datasets_fourni_par_meteo_france_6min_clean'] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=67:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\DONNEES_6_MNs_POUR_WIND\\traitement_datasets_fourni_par_meteo_france_6min_clean_sans_outliers \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=709704786.py:ligne=52:fonction=<module>():'\n",
      " path_sauvegarde_datasets_meteo_france_6min_clean_sans_outlier:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\DONNEES_6_MNs_POUR_WIND\\traitement_datasets_fourni_par_meteo_france_6min_clean_sans_outliers '\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=24:fonction=trouver_nom_workspace():'\n",
      " fichier_marqueur:\n",
      ".vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=26:fonction=trouver_nom_workspace():'\n",
      " chemin_courant:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\mobilis\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Scrapping\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=29:fonction=trouver_nom_workspace():'\n",
      "list(chemin_courant.parents) :\n",
      "    [WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Scrapping'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo/projet_meteo'), WindowsPath('C:/programmation/IA/Projet_Meteo'), WindowsPath('C:/programmation/IA'), WindowsPath('C:/programmation'), WindowsPath('C:/')] \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=30:fonction=trouver_nom_workspace():'\n",
      "chemin :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=31:fonction=trouver_nom_workspace():'\n",
      "chemin / fichier_marqueur :\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\.vscode \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=59:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " nom_dossier_workspace:\n",
      "projet_meteo \n",
      "'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:30-Apr-2024 à 10h19\n",
      "DEBUG>>>fichier=get_path_dossier_or_files.py:ligne=60:fonction=trouver_chemin_element_depuis_workspace():'\n",
      " chemin_workspace:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo \n",
      "'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## %%capture\n",
    "# Ajout dans le path du dossier de sauvegarde des datasets\n",
    "# Formatage et sauvegarde du df   C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\data_mobilis\\upload_data_depuis_api\n",
    "\n",
    "from regex import P\n",
    "\n",
    "# DATA\n",
    "# MOBILIS\n",
    "nom_element = \"data_mobilis/upload_data_depuis_api\"\n",
    "path_data_mobilis_upload_data_depuis_api = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_data_mobilis_upload_data_depuis_api:\\n{path_data_mobilis_upload_data_depuis_api} \")\n",
    "\n",
    "# METEO FRANCE\n",
    "# nom_element = \"meteo_france/upload_dataset_depuis_api/13001009/horaire/01-Jan-2004_at_00h00_to_31-Dec-2004_at_23h59\"\n",
    "nom_element = \"meteo_france/upload_dataset_depuis_api\"\n",
    "path_data_meteo_france_upload_data_depuis_api = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_data_meteo_france_upload_data_depuis_api:\\n{path_data_meteo_france_upload_data_depuis_api} \")\n",
    "\n",
    "# chemin de sauvegarde des datasets meteo france scrappé par l'api, donnnées brutes\n",
    "nom_element = \"meteo_france/upload_dataset_depuis_api2\"\n",
    "path_data_meteo_france_upload_data_depuis_api2 = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_data_meteo_france_upload_data_depuis_api2:\\n{path_data_meteo_france_upload_data_depuis_api2} \")\n",
    "\n",
    "# chemin de sauvegarde des datasets meteo france scrappé par l'api dont les colonnes sont renommées et converties\n",
    "nom_element = \"sauve_df_col_renamed\"\n",
    "path_data_meteo_france_api2_col_renamed = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_data_meteo_france_api2_col_renamed:\\n{path_data_meteo_france_api2_col_renamed} \")\n",
    "\n",
    "# # chemin de sauvegarde des datasets meteo france, mobilis, windsup, fusionnés\n",
    "# nom_element = \"datasets_fusionned\"\n",
    "# path_data_meteo_france_api2_datasets_fusionned = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "# logger.debug(f\"\\n path_data_meteo_france_api2_datasets_fusionned:\\n{path_data_meteo_france_api2_datasets_fusionned} \\n\")\n",
    "\n",
    "# chemin du dossier contenant les fichiers donnés par meteo france au pas de 6 min sources .txt\n",
    "nom_element = \"DONNEES_6_MNs_POUR_WIND\"\n",
    "path_datasets_fourni_par_meteo_france_6min_source = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_datasets_fourni_par_meteo_france_6min_source:\\n{path_datasets_fourni_par_meteo_france_6min_source} \")\n",
    "\n",
    "# chemin de sauvegarde des fichiers onnés par meteo france au pas de 6 min traités, renommage des colonnes, conversion,\n",
    "nom_element = \"traitement_datasets_fourni_par_meteo_france_6min\"\n",
    "path_sauvegarde_datasets_meteo_france_6min_clean = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_sauvegarde_datasets_meteo_france_6min_clean:\\n{path_sauvegarde_datasets_meteo_france_6min_clean} \")\n",
    "\n",
    "# chemin de sauvegarde des fichiers onnés par meteo france au pas de 6 min traités, suppression des colonnes vides\n",
    "nom_element = \"traitement_datasets_fourni_par_meteo_france_6min_clean\"\n",
    "path_sauvegarde_datasets_meteo_france_6min_suppression_col_vide = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_sauvegarde_datasets_meteo_france_6min_suppression_col_vide:\\n{path_sauvegarde_datasets_meteo_france_6min_suppression_col_vide} \")\n",
    "\n",
    "# chemin de sauvegarde des fichiers donnés par meteo france au pas de 6 min traités, suppression des outliers\n",
    "nom_element = \"traitement_datasets_fourni_par_meteo_france_6min_clean_sans_outliers\"\n",
    "path_sauvegarde_datasets_meteo_france_6min_clean_sans_outlier = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_sauvegarde_datasets_meteo_france_6min_clean_sans_outlier:\\n{path_sauvegarde_datasets_meteo_france_6min_clean_sans_outlier} \")\n",
    "\n",
    "# WINDSUP\n",
    "nom_element = \"data_windsup\"\n",
    "path_data_windsup_upload_data_depuis_api = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_data_windsup_upload_data_depuis_api:\\n{path_data_windsup_upload_data_depuis_api} \")\n",
    "\n",
    "nom_element = \"df_preprocessing\"\n",
    "path_dossier_windsup_renamed = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_dossier_windsup_renamed:\\n{path_dossier_windsup_renamed} \")\n",
    "\n",
    "nom_element = \"df_windsup_renamed.csv\"\n",
    "# chemin_dossier_windsups_renamed = os.path.join(path_data_windsup_upload_data_depuis_api, chemin_sauvegarde_df_windsup_preprocessed)\n",
    "path_fichier_windsup_renamed = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=False)\n",
    "logger.debug(f\"\\n path_fichier_windsup_renamed:\\n{path_fichier_windsup_renamed} \")\n",
    "\n",
    "# MODELES\n",
    "\n",
    "## WINDSUP\n",
    "nom_element = \"sauvegarde_modeles\\model_windsup\"\n",
    "path_sauve_modeles_windsup = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_sauve_modeles_windsup:\\n{path_sauve_modeles_windsup} \")\n",
    "## Mobilis\n",
    "nom_element = \"sauvegarde_modeles\\model_mobilis\"\n",
    "path_sauve_modeles_mobilis = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_sauve_modeles_mobilis:\\n{path_sauve_modeles_mobilis} \")\n",
    "## Meteo france 6min\n",
    "nom_element = \"sauvegarde_modeles\\model_meteo_france_6min\"\n",
    "path_sauve_modeles_meteo_france_6min = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_sauve_modeles_meteo_france_6min:\\n{path_sauve_modeles_meteo_france_6min} \")\n",
    "## Meteo france 1h\n",
    "nom_element = \"sauvegarde_modeles\\model_meteo_france_1h\"\n",
    "path_sauve_modeles_meteo_france_1h = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_sauve_modeles_meteo_france_1h:\\n{path_sauve_modeles_meteo_france_1h} \")\n",
    "\n",
    "# CONCATENATION ALL DATASETS\n",
    "\n",
    "# projet_meteo\\Projet_Meteo\\Datasets\\Concatenation_all_datasets\\dataset_interpolated_pas_moyen\n",
    "## dossier principal\n",
    "nom_element = \"Concatenation_all_datasets\"\n",
    "path_Concatenation_all_datasets = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_Concatenation_all_datasets:\\n{path_Concatenation_all_datasets} \")\n",
    "\n",
    "## dossier des dataset interpolés au pas moyen de 12 min par exemple\n",
    "nom_element = \"Concatenation_all_datasets\\dataset_interpolated_pas_moyen\"\n",
    "path_dataset_interpolated_pas_moyen = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_dataset_interpolated_pas_moyen:\\n{path_dataset_interpolated_pas_moyen} \")\n",
    "\n",
    "## dossier des datasets normalisés\n",
    "nom_element = \"Concatenation_all_datasets\\datasets_normalized\"\n",
    "path_datasets_normalized = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_datasets_normalized:\\n{path_datasets_normalized} \")\n",
    "\n",
    "## dossier de sauvegarde des datasets concatener\n",
    "nom_element = \"Concatenation_all_datasets\\sauvegarde\"\n",
    "path_sauve_all_dataset_concatenated = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_sauve_all_dataset_concatenated:\\n{path_sauve_all_dataset_concatenated} \")\n",
    "\n",
    "# sous-dossiers des dataset interpolés au pas moyen\n",
    "## dossier des dataset interpolés au pas moyen -Meteo france\n",
    "nom_element = 'Concatenation_all_datasets\\dataset_interpolated_pas_moyen\\meteo_france'\n",
    "path_dataset_interpolated_pas_moyen_meteo_france = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_dataset_interpolated_pas_moyen_meteo_france:\\n{path_dataset_interpolated_pas_moyen_meteo_france} \\n\")\n",
    "## dossier des dataset interpolés au pas moyen -Meteo france\n",
    "nom_element = 'Concatenation_all_datasets\\dataset_interpolated_pas_moyen\\meteo_france_pas_1h'\n",
    "path_dataset_interpolated_pas_moyen_meteo_france_pas_1h = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_dataset_interpolated_pas_moyen_meteo_france_pas_1h:\\n{path_dataset_interpolated_pas_moyen_meteo_france_pas_1h} \\n\")\n",
    "## dossier des dataset interpolés au pas moyen -Meteo france\n",
    "nom_element = 'Concatenation_all_datasets\\dataset_interpolated_pas_moyen\\meteo_france_pas_6min'\n",
    "path_dataset_interpolated_pas_moyen_meteo_france_pas_6min = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_dataset_interpolated_pas_moyen_meteo_france_pas_6min:\\n{path_dataset_interpolated_pas_moyen_meteo_france_pas_6min} \\n\")\n",
    "\n",
    "# sous-dossiers\n",
    "## dossier des dataset interpolés au pas moyen -mobiliis\n",
    "nom_element = 'Concatenation_all_datasets\\dataset_interpolated_pas_moyen\\mobilis'\n",
    "path_dataset_interpolated_pas_moyen_mobilis = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_dataset_interpolated_pas_moyen_mobilis:\\n{path_dataset_interpolated_pas_moyen_mobilis} \\n\")\n",
    "# sous-dossiers\n",
    "## dossier des dataset interpolés au pas moyen -windsup\n",
    "nom_element = 'Concatenation_all_datasets\\dataset_interpolated_pas_moyen\\windsup'\n",
    "path_dataset_interpolated_pas_moyen_windsup = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_dataset_interpolated_pas_moyen_windsup:\\n{path_dataset_interpolated_pas_moyen_windsup} \\n\")\n",
    "\n",
    "# sous-dossiers des datasets normalisés\n",
    "## dossier des dataset interpolés au pas moyen -Meteo france\n",
    "nom_element = 'Concatenation_all_datasets\\datasets_normalized\\meteo_france'\n",
    "path_datasets_normalized_meteo_france = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_datasets_normalized_meteo_france:\\n{path_datasets_normalized_meteo_france} \\n\")\n",
    "# sous-dossiers\n",
    "## dossier des dataset interpolés au pas moyen -mobiliis\n",
    "nom_element = 'Concatenation_all_datasets\\dataset_interpolated_pas_moyen\\mobilis'\n",
    "path_datasets_normalized_mobilis = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_ddatasets_normalized_mobilis:\\n{path_datasets_normalized_mobilis} \\n\")\n",
    "# sous-dossiers\n",
    "## dossier des dataset interpolés au pas moyen -windsup\n",
    "nom_element = 'Concatenation_all_datasets\\dataset_interpolated_pas_moyen\\windsup'\n",
    "path_datasets_normalized_windsup = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_datasets_normalized_windsup:\\n{path_datasets_normalized_windsup} \\n\")\n",
    "\n",
    "## dossier des dataset CONCATENED\n",
    "nom_element = 'Concatenation_all_datasets\\datasets_concatened'\n",
    "path_datasets_concatened = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=True)\n",
    "logger.debug(f\"\\n path_datasets_concatened:\\n{path_datasets_concatened} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48fc0f6",
   "metadata": {},
   "source": [
    "#### choix du mode de log entre 'DEBUG' 'INFO' 'WARNING' 'ERROR'  'CRITICAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec2e3e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[01m\u001b[32m-log:29-Apr-2024 à 15h47\n",
      "DEBUG>>>fichier=gestion_logging.py:ligne=117:fonction=reconfigurer_logging():' niveau de log WARNING'\u001b[0m\n",
      "\u001b[01m\u001b[32m-log:29-Apr-2024 à 15h47\n",
      "DEBUG>>>fichier=gestion_logging.py:ligne=118:fonction=reconfigurer_logging():' logger <Logger colorlog_example (DEBUG)>'\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Logger colorlog_example (WARNING)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # %%capture capturechoixlog\n",
    "import logging\n",
    "from gestion_logging import reconfigurer_logging\n",
    "\n",
    "logger = logging.getLogger('colorlog_example')\n",
    "# Utilisation de la fonction\n",
    "path_datasets = \"Projet_Meteo/Datasets/meteo_france/upload_dataset_depuis_api/\".replace(\n",
    "    '\\\\', '/')\n",
    "# path_fichier_logs=\"projet_meteo\\\\Projet_Meteo\\\\log\\\\fichier.log\"\n",
    "params = {\n",
    "    \"path_datasets\": path_datasets,\n",
    "    # \"path_fichier_logs\":path_fichier_logs,\n",
    "    \"niveau_log\": 'WARNING',  # 'DEBUG' 'INFO' 'WARNING' 'ERROR'  'CRITICAL'\n",
    "}\n",
    "\n",
    "# Configuration du logging\n",
    "reconfigurer_logging(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9927458f",
   "metadata": {},
   "source": [
    "## Fonction de chargement dataset mobilis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21df9f0e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from gestion_logging import reconfigurer_logging\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# Remonter de trois niveaux à partir du notebook\n",
    "path_to_outils = os.path.join(os.getcwd(), '..', '..', 'Projet_Meteo','outils')\n",
    "# Normaliser le chemin pour résoudre tous les liens symboliques et les références relatives\n",
    "path_to_outils = os.path.normpath(path_to_outils)\n",
    "# Ajouter le chemin à sys.path\n",
    "sys.path.append(path_to_outils)\n",
    "logger = logging.getLogger('colorlog_example')\n",
    "\n",
    "def load_dataframe(df_name, path, format_type='csv', sep=\",\"):\n",
    "    \"\"\"\n",
    "    Charge un DataFrame à partir d'un fichier en spécifiant le format souhaité.\n",
    "    Paramètres :\n",
    "    - df_name (str) : Nom de base du DataFrame à charger (sans extension).\n",
    "    - path (str) : Chemin du dossier contenant le fichier.\n",
    "    - format_type (str) : Format du fichier ('csv', 'json', 'pkl').\n",
    "    Retourne :\n",
    "    - DataFrame chargé.\n",
    "    \"\"\"\n",
    "    # Construction du chemin complet du fichier\n",
    "    file_basename = os.path.join(path, df_name)\n",
    "    logger.debug(f\" file_basename:  {file_basename}\")\n",
    "    try:\n",
    "        # Chargement du DataFrame en fonction du format spécifié\n",
    "        if format_type == 'csv':\n",
    "            filepath = f\"{file_basename}.csv\"\n",
    "            # df = pd.read_csv(filepath)\n",
    "            df = pd.read_csv(filepath, on_bad_lines='skip',sep=sep)\n",
    "\n",
    "            logger.info(f\"DataFrame loaded from CSV at {filepath}\")\n",
    "        elif format_type == 'json':\n",
    "            filepath = f\"{file_basename}.json\"\n",
    "            df = pd.read_json(filepath, orient='split')\n",
    "            logger.info(f\"DataFrame loaded from JSON at {filepath}\")\n",
    "        elif format_type == 'pkl':\n",
    "            filepath = f\"{file_basename}.pkl\"\n",
    "            df = pd.read_pickle(filepath)\n",
    "            logger.info(f\"DataFrame loaded from pickle HDF5 at {filepath}\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Please choose 'csv', 'json', or 'h5'.\" )\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while loading the file: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fad6ef",
   "metadata": {},
   "source": [
    "### Chargement dataset mobilis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc374f",
   "metadata": {},
   "source": [
    "#### charge les 2 datasets bruts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d26cbc4",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Chargement du dataset mobilis df_mobilis\n",
    "# df_mobilis = load_dataframe(\"df_mobilis\", r\"../../Projet_Meteo/Datasets/data_mobilis/upload_data_depuis_api\", format_type='pkl')\n",
    "df_mobilis3_7col = load_dataframe(\"df_mobilis3_7col\",  path_data_mobilis_upload_data_depuis_api,\n",
    "                                  format_type='pkl')\n",
    "df_mobilis3_12col = load_dataframe(\"df_mobilis3_12col\", path_data_mobilis_upload_data_depuis_api,\n",
    "                                   format_type='pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d27fe",
   "metadata": {},
   "source": [
    "#### charge les 2 datasets sans NA ni outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb14268",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Chargement du dataset mobilis df_mobilis\n",
    "# df_mobilis = load_dataframe(\"df_mobilis\", r\"../../Projet_Meteo/Datasets/data_mobilis/upload_data_depuis_api\", format_type='pkl')\n",
    "df_clean_7col = load_dataframe(\"df_clean_7col\",  path_data_mobilis_upload_data_depuis_api,\n",
    "                                  format_type='pkl')\n",
    "df_clean_12col = load_dataframe(\"df_clean_12col\", path_data_mobilis_upload_data_depuis_api,\n",
    "                                   format_type='pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c1077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_12col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8660f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mobilis3_7col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e72751",
   "metadata": {},
   "source": [
    "# METEO FRANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d02b644",
   "metadata": {},
   "source": [
    "### parcourir les sous-repertoires et selectionne les stations entre 2 dates, retourne des dico contenant la liste des fichiers csv pour chaque station slectionnées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac241ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# import os\n",
    "\n",
    "# logger = logging.getLogger('colorlog_example')\n",
    "\n",
    "# def selectionner_stations_between_date_retourne_dico(chemin, date_debut, date_fin):\n",
    "#     \"\"\"\n",
    "#     Sélectionne les dossiers parent \"horaire\" contenant des fichiers CSV entre les dates spécifiées.\n",
    "\n",
    "#     Args:\n",
    "#         chemin (str): Chemin du dossier racine à parcourir.\n",
    "#         date_debut (str): Date de début au format 'dd-MM-yyyy'.\n",
    "#         date_fin (str): Date de fin au format 'dd-MM-yyyy'.\n",
    "\n",
    "#     Returns:\n",
    "#         dict1: Dictionnaire avec le nom du dossier parent et les fichiers CSV entre les dates spécifiées,\n",
    "#         uniquement si toutes les années prévues entre les dates sont presentes\n",
    "#         dict2: Dictionnaire avec le nom du dossier parent et les fichiers CSV entre les dates spécifiées.\n",
    "#         uniquement si toutes les années prévues entre les dates ne sont pas presentes\n",
    "#         dict3: Dictionnaire avec le nom du dossier parent et les fichiers CSV entre les dates spécifiées,\n",
    "#     \"\"\"\n",
    "#     # Conversion des chaînes de date en objets datetime pour la comparaison\n",
    "#     date_debut_obj = datetime.strptime(date_debut, '%d-%m-%Y')\n",
    "#     date_fin_obj = datetime.strptime(date_fin, '%d-%m-%Y')\n",
    "#     # Convertir les chaînes de caractères en objets datetime\n",
    "#     date_debut_formated = datetime.strptime(date_debut, '%d-%m-%Y')\n",
    "#     date_fin_formated = datetime.strptime(date_fin, '%d-%m-%Y')\n",
    "\n",
    "#     number_of_years = date_fin_formated.year - date_debut_formated.year\n",
    "#     if (date_fin_formated.month > date_debut_formated.month) or (\n",
    "#             date_fin_formated.month == date_debut_formated.month\n",
    "#             and date_fin_formated.day > date_debut_formated.day):\n",
    "#         number_of_years += 1\n",
    "#     logger.debug(f\"\\n number_of_years:\\n{number_of_years} \\n\")\n",
    "#     stations_between_dates_total = {}\n",
    "#     stations_between_dates_complet = {}\n",
    "#     stations_between_dates_incompletes={}\n",
    "#     for root, dirs, files in os.walk(chemin):\n",
    "#         if 'horaire' in dirs:\n",
    "#             chemin_horaire = os.path.join(root, 'horaire')\n",
    "#             nom_parent = os.path.basename(root)\n",
    "#             fichiers_csv = []\n",
    "#             try:\n",
    "#                 for sous_dossier in next(os.walk(chemin_horaire))[1]:\n",
    "#                     # Extraction de la date de début à partir du nom de dossier\n",
    "#                     date_str = sous_dossier.split('_')[1]\n",
    "#                     # Conversion de la date en objet datetime\n",
    "#                     date_sous_dossier_obj = datetime.strptime(\n",
    "#                         date_str, '%d-%b-%Y')\n",
    "#                     if date_debut_obj <= date_sous_dossier_obj <= date_fin_obj:\n",
    "#                         chemin_sous_dossier = os.path.join(\n",
    "#                             chemin_horaire, sous_dossier)\n",
    "#                         fichiers_csv += [\n",
    "#                             f for f in os.listdir(chemin_sous_dossier)\n",
    "#                             if f.endswith('.csv')\n",
    "#                         ]\n",
    "#                 if fichiers_csv:\n",
    "#                     stations_between_dates_total[nom_parent] = {\n",
    "#                         'compteur': len(fichiers_csv),\n",
    "#                         'fichiers': fichiers_csv\n",
    "#                     }\n",
    "#                     # ne selectionne que les stations dont les dates sont completes\n",
    "#                     # Sélectionner uniquement les stations avec un historique complet\n",
    "#                     if len(fichiers_csv) == number_of_years:\n",
    "#                         stations_between_dates_complet[nom_parent] =  {\n",
    "#                         'compteur': len(fichiers_csv),\n",
    "#                         'fichiers': fichiers_csv\n",
    "#                     }\n",
    "#                     else:\n",
    "#                         stations_between_dates_incompletes[nom_parent] =  {\n",
    "#                         'compteur': len(fichiers_csv),\n",
    "#                         'fichiers': fichiers_csv\n",
    "#                     }\n",
    "#                 # for station, infos in stations_between_dates_total.items():\n",
    "#                 #     if infos['compteur'] == number_of_years:\n",
    "#                 #         stations_between_dates_complet.append(station)\n",
    "#             except Exception as e:\n",
    "#                 logger.error(f\"Erreur lors de l'accès à {chemin_horaire}: {e}\")\n",
    "\n",
    "#     return  stations_between_dates_complet,stations_between_dates_incompletes,stations_between_dates_total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde2b991",
   "metadata": {},
   "source": [
    "### Parcourir les sous-repertoires et selectionner les stations entre 2 dates, retourner des df et les sauvegarder en csv, json et pkl, les df contenant la liste des fichiers csv pour chaque station selectionnées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe97e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sauve_charge_df_csv_json_pkl import save_dataframe, load_dataframe\n",
    "# Configuration de base du logger pour éviter les erreurs liées à l'utilisation de logger non défini\n",
    "logger = logging.getLogger('colorlog_example')\n",
    "\n",
    "def selectionner_stations_between_date_sauve(chemin, date_debut, date_fin):\n",
    "    \"\"\"\n",
    "    Sélectionne les dossiers parent \"horaire\" contenant des fichiers CSV entre les dates spécifiées\n",
    "    et retourne les résultats sous forme de DataFrames pandas.\n",
    "    Sauvegarde les df au froamt csv, json et pkl\n",
    "\n",
    "    Args:\n",
    "        chemin (str): Chemin du dossier racine à parcourir.\n",
    "        date_debut (str): Date de début au format 'dd-MM-yyyy'.\n",
    "        date_fin (str): Date de fin au format 'dd-MM-yyyy'.\n",
    "\n",
    "    Returns:\n",
    "        Tuple de pandas DataFrames: DataFrames pour les stations complètes, incomplètes et totales.\n",
    "    \"\"\"\n",
    "    date_debut_obj = datetime.strptime(date_debut, '%d-%m-%Y')\n",
    "    date_fin_obj = datetime.strptime(date_fin, '%d-%m-%Y')\n",
    "\n",
    "    number_of_years = date_fin_obj.year - date_debut_obj.year + int((date_fin_obj.month, date_fin_obj.day) >= (date_debut_obj.month, date_debut_obj.day))\n",
    "    logger.debug(f\"\\n number_of_years:\\n{number_of_years} \\n\")\n",
    "\n",
    "    stations_between_dates_total = {}\n",
    "    stations_between_dates_complet = {}\n",
    "    stations_between_dates_incompletes = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(chemin):\n",
    "        if 'horaire' in dirs:\n",
    "            chemin_horaire = os.path.join(root, 'horaire')\n",
    "            nom_parent = os.path.basename(root)\n",
    "            fichiers_csv = []\n",
    "\n",
    "            try:\n",
    "                for sous_dossier in next(os.walk(chemin_horaire))[1]:\n",
    "                    date_str = sous_dossier.split('_')[1]\n",
    "                    date_sous_dossier_obj = datetime.strptime(date_str, '%d-%b-%Y')\n",
    "                    if date_debut_obj <= date_sous_dossier_obj <= date_fin_obj:\n",
    "                        chemin_sous_dossier = os.path.join(chemin_horaire, sous_dossier)\n",
    "                        fichiers_csv += [f for f in os.listdir(chemin_sous_dossier) if f.endswith('.csv')]\n",
    "\n",
    "                if fichiers_csv:\n",
    "                    stations_between_dates_total[nom_parent] = {\n",
    "                        'compteur': len(fichiers_csv),\n",
    "                        'fichiers': fichiers_csv\n",
    "                    }\n",
    "                    if len(fichiers_csv) == number_of_years:\n",
    "                        stations_between_dates_complet[nom_parent] = {\n",
    "                            'compteur': len(fichiers_csv),\n",
    "                            'fichiers': fichiers_csv\n",
    "                        }\n",
    "                    else:\n",
    "                        stations_between_dates_incompletes[nom_parent] = {\n",
    "                            'compteur': len(fichiers_csv),\n",
    "                            'fichiers': fichiers_csv\n",
    "                        }\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Erreur lors de l'accès à {chemin_horaire}: {e}\")\n",
    "\n",
    "    # Conversion des dictionnaires en DataFrames\n",
    "    df_complet = pd.DataFrame.from_dict(stations_between_dates_complet, orient='index').reset_index().rename(columns={'index': 'station'})\n",
    "    df_incompletes = pd.DataFrame.from_dict(stations_between_dates_incompletes, orient='index').reset_index().rename(columns={'index': 'station'})\n",
    "    df_total = pd.DataFrame.from_dict(stations_between_dates_total, orient='index').reset_index().rename(columns={'index': 'station'})\n",
    "    # sauvegarde les 3 df\n",
    "    chemin_dossier_sauve_liste_df = os.path.join(chemin_dossier, \"sauve_liste_df\")\n",
    "    save_dataframe(df_stations_between_dates_complet,\"stations_between_dates_complet\",chemin_dossier_sauve_liste_df)\n",
    "    save_dataframe(df_stations_between_dates_incompletes,\"stations_between_dates_incompletes\",chemin_dossier_sauve_liste_df)\n",
    "    save_dataframe(df_stations_between_dates_total,\"stations_between_dates_total\",chemin_dossier_sauve_liste_df)\n",
    "\n",
    "    return df_complet, df_incompletes, df_total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93f26bf",
   "metadata": {},
   "source": [
    "### Utilisation de la fonction selectionner_stations_between_date_sauve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77200f3",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Utilisation de la fonction selectionner_stations_between_date_sauve\n",
    "\n",
    "chemin_dossier = path_data_meteo_france_upload_data_depuis_api2\n",
    "chemin_dossier_sauve_liste_df = os.path.join(chemin_dossier, \"sauve_liste_df\")\n",
    "# date_debut = '01-01-2014'\n",
    "# date_fin = '01-03-2024'\n",
    "\n",
    "# lancement fonction\n",
    "# df_stations_between_dates_complet, df_stations_between_dates_incompletes, df_stations_between_dates_total = selectionner_stations_between_date_sauve(chemin_dossier_sauve_liste_df, date_debut, date_fin)\n",
    "\n",
    "# logger.debug(f\"\\n stations_between_dates_total:\\n{df_stations_between_dates_total.head(2)} \\n\")\n",
    "# logger.debug(f\"\\n stations_between_dates_complet:\\n{df_stations_between_dates_complet.head(2)} \\n\")\n",
    "# logger.debug(f\"\\n stations_between_dates_incompletes:\\n{df_stations_between_dates_incompletes.head(2)} \\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec878c24",
   "metadata": {},
   "source": [
    "### sauve les dataframes meteofrance contenant la liste des stations selectionnées avec le nom des fichiers csv, inutile si les df ont déja été sauvés avec la fonction selectionner_stations_between_date_sauve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560365bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sauve_charge_df_csv_json_pkl import save_dataframe, load_dataframe\n",
    "# chemin_dossier = path_data_meteo_france_upload_data_depuis_api2\n",
    "chemin_dossier_sauve_liste_df = os.path.join(chemin_dossier, \"sauve_liste_df\")\n",
    "# save_dataframe(df_stations_between_dates_complet,\"stations_between_dates_complet\",chemin_dossier_sauve_liste_df)\n",
    "# save_dataframe(df_stations_between_dates_incompletes,\"stations_between_dates_incompletes\",chemin_dossier_sauve_liste_df)\n",
    "# save_dataframe(df_stations_between_dates_total,\"stations_between_dates_total\",chemin_dossier_sauve_liste_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e57d462",
   "metadata": {},
   "source": [
    "### Charge les listes des  dataframes meteofrance contenant la liste des stations selectionnées avec le nom des fichiers csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sauve_charge_df_csv_json_pkl import save_dataframe, load_dataframe\n",
    "chemin_dossier = path_data_meteo_france_upload_data_depuis_api2\n",
    "chemin_dossier_sauve_liste_df = os.path.join(chemin_dossier, \"sauve_liste_df\")\n",
    "\n",
    "df_stations_between_dates_complet=load_dataframe(\"df_stations_between_dates_complet\", chemin_dossier_sauve_liste_df, format_type='pkl')\n",
    "# load_dataframe(\"df_stations_between_dates_incompletes\", chemin_dossier, format_type='pkl')\n",
    "# load_dataframe(\"df_stations_between_dates_total\", chemin_dossier, format_type='pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a830d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_stations_between_dates_complet.head(2  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5053ac75",
   "metadata": {},
   "source": [
    "### Concatene les df par station, supprime les colonnes ayant trop de NA, sauve les df en json, csv, pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "from sauve_charge_df_csv_json_pkl import save_dataframe\n",
    "from descriptif_dataset_meteo_france import expliquer_et_renommer_colonne\n",
    "\n",
    "# Configuration du logger\n",
    "logger = logging.getLogger('colorlog_example')\n",
    "# Initialisation de global_df_dict en tant que dictionnaire vide\n",
    "global_df_dict = {}\n",
    "\n",
    "\n",
    "def remove_columns_with_na(df, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Supprime les colonnes d'un DataFrame ayant un pourcentage de valeurs manquantes\n",
    "    supérieur au seuil spécifié.\n",
    "\n",
    "    :param df: Le DataFrame pandas à traiter.\n",
    "    :param threshold: Le seuil de pourcentage de valeurs manquantes pour la suppression des colonnes.\n",
    "                      La valeur par défaut est 0.2 (20%).\n",
    "    :return: Un DataFrame avec les colonnes ayant un pourcentage de valeurs manquantes\n",
    "             inférieur ou égal au seuil spécifié.\n",
    "    \"\"\"\n",
    "    # Calculer le pourcentage de valeurs manquantes pour chaque colonne\n",
    "    missing_percentage = df.isna().mean()\n",
    "    # print(f\"\\n missing_percentage:\\n{missing_percentage} \\n\")\n",
    "    # Convertir en pourcentage et formater chaque valeur\n",
    "    formatted_missing_percentage = missing_percentage.apply(lambda x: f\"{x*100:.2f}%\")\n",
    "\n",
    "    print(f\"\\nPourcentage de valeurs manquantes par colonne:\\n{formatted_missing_percentage}\\n\")\n",
    "    # Identifier les colonnes à conserver (celles avec un pourcentage de Na <= seuil)\n",
    "    columns_to_keep = missing_percentage[missing_percentage <=\n",
    "                                         threshold].index.tolist()\n",
    "\n",
    "    # Filtrer le DataFrame pour ne conserver que les colonnes sélectionnées\n",
    "    filtered_df = df[columns_to_keep]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def extraire_info_fichier(premier_fichier, dernier_fichier):\n",
    "    \"\"\"\n",
    "    Extrait le numéro de la station, la date de début et de fin à partir des noms du premier et dernier fichier.\n",
    "    \"\"\"\n",
    "    logger.debug(f\"Extraction des informations à partir des fichiers : {premier_fichier}, {dernier_fichier}\")\n",
    "    try:\n",
    "        pattern = r'(\\d+)_(\\d{2})-(\\w+)-(\\d{4})_at.*to_(\\d{2})-(\\w+)-(\\d{4})_at.*\\.csv'\n",
    "        debut_match = re.search(pattern, premier_fichier)\n",
    "        fin_match = re.search(pattern, dernier_fichier)\n",
    "\n",
    "        if debut_match and fin_match:\n",
    "            station = debut_match.group(1)\n",
    "            date_debut = debut_match.group(2) + debut_match.group(3)[:3].lower() + debut_match.group(4)\n",
    "            date_fin = fin_match.group(5) + fin_match.group(6)[:3].lower() + fin_match.group(7)\n",
    "            nom_df = f\"df_{station}_{date_debut}_{date_fin}\"\n",
    "            logger.debug(f\"Nom du DataFrame formé : {nom_df}\")\n",
    "            return nom_df\n",
    "        else:\n",
    "            logger.error(\"Impossible d'extraire correctement les informations des fichiers\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l'extraction des informations : {e}\")\n",
    "        return None\n",
    "\n",
    "def trouver_chemin_complet(chemin_base, station, nom_fichier):\n",
    "    \"\"\"\n",
    "    Recherche de manière récursive le chemin complet d'un fichier donné, sans supposer la nature du sous-dossier (comme 'horaire' ou 'quotidienne').\n",
    "\n",
    "    Args:\n",
    "        chemin_base (str): Le chemin de base où débuter la recherche.\n",
    "        station (str): Identifiant de la station concernée.\n",
    "        nom_fichier (str): Nom du fichier à rechercher.\n",
    "\n",
    "    Returns:\n",
    "        str: Chemin complet du fichier si trouvé, None sinon.\n",
    "    \"\"\"\n",
    "    for racine, dossiers, fichiers in os.walk(os.path.join(chemin_base, station)):\n",
    "        if nom_fichier in fichiers:\n",
    "            return os.path.join(racine, nom_fichier)\n",
    "    logger.warning(f\"Fichier {nom_fichier} non trouvé pour la station {station}.\")\n",
    "    return None\n",
    "\n",
    "def concatener_dataframes_station(df_stations_between_dates_complet, chemin_dossier):\n",
    "    \"\"\"\n",
    "    Concatène les DataFrames de chaque station basé sur les fichiers spécifiés,\n",
    "    et forme un nom approprié pour chaque DataFrame concaténé.\n",
    "    supprime les colonnes ayant trop de NA\n",
    "    \"\"\"\n",
    "    # global_df_dict=[]\n",
    "    global global_df_dict\n",
    "\n",
    "    # Assurez-vous que global_df_dict est bien un dictionnaire\n",
    "    if not isinstance(global_df_dict, dict):\n",
    "        logger.warning(\"global_df_dict n'est pas un dictionnaire. Réinitialisation effectuée.\")\n",
    "        global_df_dict = {}  # Réinitialise global_df_dict comme un dictionnaire vide\n",
    "\n",
    "    noms_df = []\n",
    "\n",
    "    for _, row in df_stations_between_dates_complet.iterrows():\n",
    "        station = row['station']\n",
    "        fichiers = sorted(row['fichiers']) if isinstance(row['fichiers'], list) else sorted(json.loads(row['fichiers']))\n",
    "        logger.debug(f\"Traitement de la station {station} avec {len(fichiers)} fichiers.\")\n",
    "\n",
    "        if fichiers:\n",
    "            # Trouver le chemin complet et charger les fichiers\n",
    "            dfs = []\n",
    "            for fichier in fichiers:\n",
    "                chemin_complet = trouver_chemin_complet(chemin_dossier, station, fichier)\n",
    "                if chemin_complet:\n",
    "                    try:\n",
    "                        df = pd.read_csv(chemin_complet, sep=\";\",on_bad_lines='skip')\n",
    "                        dfs.append(df)\n",
    "                        logger.debug(f\"Fichier {fichier} chargé avec succès.\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Erreur lors du chargement du fichier {fichier}: {e}\")\n",
    "\n",
    "            # Concaténer, nettoyer  et stocker le DataFrame\n",
    "            if dfs:\n",
    "                logger.debug(f\"dfs   {dfs}\")\n",
    "                df_concatene = pd.concat(dfs, ignore_index=True)\n",
    "                nom_df = extraire_info_fichier(fichiers[0], fichiers[-1])\n",
    "                if nom_df:\n",
    "                    # supprime les colonnes ayant plus de 20 % de na\n",
    "                    df_concatene_cleaned =remove_columns_with_na(df_concatene)\n",
    "                    # rend le df accessible gloabalement\n",
    "                    global_df_dict[nom_df] = df_concatene_cleaned\n",
    "                    logger.debug(f\"nom_df   {nom_df}\")\n",
    "                    noms_df.append(nom_df)\n",
    "                    logger.debug(f\"DataFrame {nom_df} créé et ajouté au dictionnaire global.\")\n",
    "                    # enregistre le df\n",
    "                    chemin_dossier_sauve_df = os.path.join(chemin_dossier, \"sauve_df\")\n",
    "                    save_dataframe(df_concatene_cleaned,nom_df.replace(\"df_\",\"\"),chemin_dossier_sauve_df)\n",
    "            else:\n",
    "                logger.warning(f\"Aucun DataFrame créé pour la station {station}.\")\n",
    "    # enregistre la liste des df df\n",
    "    chemin_dossier_sauve_liste_df_clean = os.path.join(chemin_dossier, \"sauve_liste_df_clean\")\n",
    "    save_dataframe(noms_df,\"liste_df_clean\",chemin_dossier_sauve_liste_df_clean)\n",
    "\n",
    "    logger.debug(\"Concaténation terminée pour toutes les stations.\")\n",
    "    return noms_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47817bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture\n",
    "\n",
    "# #chemin définition\n",
    "# chemin_dossier = path_data_meteo_france_upload_data_depuis_api2\n",
    "# # Obtenir la liste des df_concantés, clean et sauvegardés\n",
    "# noms_df=concatener_dataframes_station(df_stations_between_dates_complet, chemin_dossier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789879e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fed93d83",
   "metadata": {},
   "source": [
    "### Charge et renomme les colonnes des datasets déja nettoyés "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9628c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # %%capture captchargerenomme\n",
    "# from sauve_charge_df_csv_json_pkl import charger_all_dataframes\n",
    "# from descriptif_dataset_meteo_france import expliquer_et_renommer_colonne\n",
    "# # dossier = r\"..\\Datasets\\saved_dataframe3\"\n",
    "# # utilisation de la methode externe pour cahrger tous les df d'un dossier\n",
    "# dossier = r\"..\\..\\Datasets\\meteo_france\\upload_dataset_depuis_api2\\sauve_df\"\n",
    "# # # Chargez les DataFrames en utilisant la fonction\n",
    "# dataframes = charger_all_dataframes(dossier, default_format=\"csv\", sep=\",\")\n",
    "# print(list(dataframes.keys()))\n",
    "\n",
    "# # Parcourez chaque DataFrame pour renommer les colonnes\n",
    "# for df_name, df_object in dataframes.items():\n",
    "#     # Assurez-vous que la fonction expliquer_et_renommer_colonne() retourne bien un tuple avec le nouveau nom en seconde position\n",
    "#     colonnes_renommees = {col: expliquer_et_renommer_colonne(col)[1] for col in df_object.columns}\n",
    "#     logger.debug(f\" df_name: {df_name}  colonnes_renommees:{colonnes_renommees}\")\n",
    "#     df_renomme = df_object.rename(columns=colonnes_renommees)\n",
    "\n",
    "#     # Assignez le DataFrame renommé à la variable globale correspondante\n",
    "#     globals()[df_name] = df_renomme\n",
    "\n",
    "#     # Affichez les premières lignes du DataFrame renommé pour vérification\n",
    "#     print(f\"{df_name} après renommage:\")\n",
    "#     print(df_renomme.head(2))\n",
    "\n",
    "# # Affichez la liste des noms de DataFrame pour vérification\n",
    "# print(\"Liste des DataFrames chargés et renommés :\")\n",
    "# print(list(dataframes.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec7332e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# # %%capture capturechargeglobale\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "from sauve_charge_df_csv_json_pkl import charger_all_dataframes, load_dataframe\n",
    "from sauve_charge_df_csv_json_pkl import format_and_save_dataframe\n",
    "from descriptif_dataset_meteo_france import expliquer_et_renommer_colonne\n",
    "\n",
    "logger = logging.getLogger(\"colorlog_example\")\n",
    "\n",
    "# Initialisation du dictionnaire global pour stocker les DataFrames\n",
    "global_df_dict = {}\n",
    "\n",
    "\n",
    "# Fonction pour charger et rendre accessible globalement chaque DataFrame\n",
    "def charge_format_rename_convert_df_globalement(chemin_dossier_df, chemin_liste_df_clean, chemin_sauvagarde_df_col_renamed):\n",
    "\n",
    "    #charge la liste des df cleaned\n",
    "    # chargement\n",
    "    # df_liste_df_clean = load_dataframe(\"df_liste_df_clean\", chemin_liste_df_clean,format_type='pkl')\n",
    "    noms_df = pd.read_csv(os.path.join(chemin_liste_df_clean, \"df_liste_df_clean.csv\"), header=0, names=['Nom_df'])\n",
    "    # noms_df = load_dataframe(\"df_liste_df_clean\", chemin_liste_df_clean, format_type='csv', sep=\",\")\n",
    "\n",
    "    noms_df = pd.DataFrame(noms_df)\n",
    "    logger.debug(f\" noms_df :{noms_df.head(2)}\")\n",
    "    # noms_df.columns.tolist()\n",
    "    # renomme la colonne\n",
    "    # noms_df.rename(columns={0: 'Nom_df'}, inplace=True)\n",
    "    logger.info(f\" df_liste_df_clean :{noms_df}\")\n",
    "\n",
    "    for index, row in noms_df.iterrows():\n",
    "        nom_df = row['Nom_df']\n",
    "        chemin_complet = os.path.join(chemin_dossier_df, f\"{nom_df}.csv\")\n",
    "\n",
    "        try:\n",
    "            # Charger le DataFrame à partir du fichier CSV\n",
    "            df = pd.read_csv(chemin_complet)\n",
    "            # Renommer les colonnes du DataFrame en utilisant la fonction importée\n",
    "            colonnes_renommees = {col: expliquer_et_renommer_colonne(col)[1] for col in df.columns}\n",
    "            df_renomme = df.rename(columns=colonnes_renommees)\n",
    "            # Stocker le DataFrame dans le dictionnaire global\n",
    "            # global_df_dict[nom_df] = df\n",
    "\n",
    "            logger.debug(f\" liste colonne df {df.columns.tolist()} \")\n",
    "            logger.debug(f\" liste colonne df_renomme {df_renomme.columns.tolist()} \")\n",
    "            logger.debug(f\"{nom_df} chargé et ajouté au dictionnaire global.\")\n",
    "            # Stocker le DataFrame dans le dictionnaire global\n",
    "            global_df_dict[nom_df] = df_renomme\n",
    "            #sauvegarde des df renommés dans le repertoire sauve_df_col_renamed\n",
    "            format_and_save_dataframe(df_renomme, nom_df.replace(\"df_\", \"\"), chemin_sauvagarde_df_col_renamed)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du chargement de {nom_df}: {e}\")\n",
    "# df_13001009_01jan2014_01mar2024\n",
    "# df_13004003_01jan2014_01mar2024\n",
    "# df_13005003_01jan2014_01mar2024\n",
    "# df_13022003_01jan2014_01mar2024\n",
    "# df_13036003_01jan2014_01mar2024\n",
    "# df_13047001_01jan2014_01mar2024\n",
    "# df_13055029_01jan2014_01mar2024\n",
    "# df_13056002_01jan2014_01mar2024\n",
    "# df_13091002_01jan2014_01mar2024\n",
    "# df_13103001_01jan2014_01mar2024\n",
    "# df_13108004_01jan2014_01mar2024\n",
    "# df_13110003_01jan2014_01mar2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dacfc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appel de la fonction\n",
    "chemin_dossier = path_data_meteo_france_upload_data_depuis_api2\n",
    "#chemin de stockage de la liste des df cleaned noms_df\n",
    "chemin_dossier_sauve_liste_df_clean = os.path.join(chemin_dossier, \"sauve_liste_df_clean\")\n",
    "#chemin de stockage des df cleaned\n",
    "chemin_dossier_sauve_df = os.path.join(chemin_dossier, \"sauve_df\")\n",
    "chemin_dossier_sauve_df_col_renamed = os.path.join(chemin_dossier, \"sauve_df_col_renamed\")\n",
    "\n",
    "# lancement fonction\n",
    "# dataframes = charge_format_rename_convert_df_globalement(chemin_dossier_sauve_df, chemin_dossier_sauve_liste_df_clean, chemin_dossier_sauve_df_col_renamed)\n",
    "\n",
    "\n",
    "\n",
    "# # ['df_13001009_01jan2014_01mar2024', 'df_13004003_01jan2014_01mar2024', 'df_13005003_01jan2014_01mar2024', 'df_13022003_01jan2014_01mar2024', 'df_13036003_01jan2014_01mar2024', 'df_13047001_01jan2014_01mar2024', 'df_13055029_01jan2014_01mar2024', 'df_13056002_01jan2014_01mar2024', 'df_13091002_01jan2014_01mar2024', 'df_13103001_01jan2014_01mar2024', 'df_13108004_01jan2014_01mar2024', 'df_13110003_01jan2014_01mar2024']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e2dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Chargement des df formatés et renommés\n",
    "df = pd.read_csv(os.path.join(chemin_dossier_sauve_df_col_renamed, \"df_13056002_01jan2014_01mar2024.csv\"), sep=\",\")\n",
    "nom_fichier = r'C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\upload_dataset_depuis_api2\\13056002\\horaire\\13056002_01-Jan-2006_at_00h00_to_31-Dec-2006_at_23h59\\13056002_01-Jan-2006_at_00h00_to_31-Dec-2006_at_23h59_1.csv'\n",
    "df2=pd.read_csv(nom_fichier,header=0, sep=';', encoding='utf-8')\n",
    "df2\n",
    "\n",
    "nom_fichier = r'C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\upload_dataset_depuis_api2\\13111002\\horaire\\13111002_01-Jan-2004_at_00h00_to_31-Dec-2004_at_23h59\\13111002_01-Jan-2004_at_00h00_to_31-Dec-2004_at_23h59_1.csv'\n",
    "df3=pd.read_csv(nom_fichier,header=0, sep=';', encoding='utf-8' )\n",
    "df3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0579a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12452cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sauve_charge_df_csv_json_pkl import charger_all_dataframes, load_dataframe\n",
    "\n",
    "# df_13005003_01jan2014_01mar2024 = load_dataframe('df_13005003_01jan2014_01mar2024', chemin_dossier_sauve_df_col_renamed, format_type='csv', sep=\",\")\n",
    "# df_13005003_01jan2014_01mar2024\n",
    "# chargement station de cap couronnes\n",
    "df_13056002_01jan2014_01mar2024 = load_dataframe('df_13056002_01jan2014_01mar2024', chemin_dossier_sauve_df_col_renamed, format_type='json', sep=\",\")\n",
    "# load_dataframe(df_name, path, format_type='csv', sep=\",\")\n",
    "# df_13001009_01jan2014_01mar2024 = global_df_dict['df_13001009_01jan2014_01mar2024']\n",
    "df_13056002_01jan2014_01mar2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sauve_charge_df_csv_json_pkl import charger_all_dataframes\n",
    "from descriptif_dataset_meteo_france import expliquer_et_renommer_colonne\n",
    "# dossier = r\"..\\Datasets\\saved_dataframe3\"\n",
    "# utilisation de la methode externe pour cahrger tous les df d'un dossier\n",
    "# dossier = r\"..\\..\\Datasets\\meteo_france\\upload_dataset_depuis_api2\\sauve_df\"\n",
    "# dataframes = charger_all_dataframes(dossier, default_format='csv', sep=\",\")\n",
    "\n",
    "# dataframes = charger_all_dataframes(chemin_dossier_sauve_df_col_renamed, default_format='csv', sep=\",\")\n",
    "\n",
    "for df_name, df_object in global_df_dict.items():\n",
    "    logger.debug(f\" df_name: {df_name}  df_object:{df_object}\")\n",
    "    df_object.head(2)\n",
    "    globals()[df_name] = df_object\n",
    "\n",
    "    logger.debug(f\"{df_name} est maintenant accessible comme une variable globale.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f30251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nom_du_df = 'df_13022003_01jan2014_01mar2024'\n",
    "nom_du_df = 'df_13001009_01jan2014_01mar2024'\n",
    "nom_du_df = 'df_13110003_01jan2014_01mar2024'\n",
    "nom_du_df = 'df_13005003_01jan2014_01mar2024'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #charge les df contenu dans la liste\n",
    "# #chemin de stockage des df cleaned\n",
    "# chemin_dossier_sauve_df = os.path.join(chemin_dossier, \"sauve_df\")\n",
    "\n",
    "# df = load_dataframe(\"liste_df_clean\", chemin_dossier_sauve_df, format_type='pkl')\n",
    "\n",
    "# Supposons que df_13001009 soit le nom du DataFrame que vous souhaitez inspecter\n",
    "nom_df = noms_df[0]  # 'df_13001009_01jan2014-01mar2024'\n",
    "\n",
    "# Accéder au DataFrame depuis global_df_dict en utilisant le nom\n",
    "df = global_df_dict.get(nom_df)\n",
    "nom_df=noms_df[0]\n",
    "if df is not None:\n",
    "    # Maintenant que vous avez le DataFrame, vous pouvez utiliser .info() pour afficher ses informations\n",
    "    print(df.head(2))\n",
    "else:\n",
    "    print(f\"Le DataFrame nommé {nom_df} n'a pas été trouvé.\")\n",
    "\n",
    "# print(noms_df)\n",
    "# # ['df_13001009_01jan2014-01mar2024', 'df_13004003_01jan2014-01mar2024', 'df_13005003_01jan2014-01mar2024', 'df_13022003_01jan2014-01mar2024', 'df_13036003_01jan2014-01mar2024', 'df_13047001_01jan2014-01mar2024', 'df_13055029_01jan2014-01mar2024', 'df_13056002_01jan2014-01mar2024', 'df_13091002_01jan2014-01mar2024', 'df_13103001_01jan2014-01mar2024', 'df_13108004_01jan2014-01mar2024', 'df_13110003_01jan2014-01mar2024']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c76656",
   "metadata": {},
   "source": [
    "### Parcourir les sous-répertoires et évaluer les stations dont l'historique est complet, en ne conservant que les stations qui ont 20 ans d'historique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c688b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def selectionner_stations_avec_20_csv(chemin,taille_histo=19):\n",
    "    \"\"\"\n",
    "    Sélectionne uniquement les dossiers parent \"horaire\" contenant exactement 20 fichiers CSV.\n",
    "    Le dictionnaire retourné inclut le nom des fichiers CSV et un compteur de fichiers.\n",
    "\n",
    "    Args:\n",
    "        chemin (str): Chemin du dossier racine à parcourir.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionnaire avec le nom du dossier parent, le nombre de fichiers CSV et leurs noms.\n",
    "    \"\"\"\n",
    "    stations_avec_20_csv = {}\n",
    "    for root, dirs, files in os.walk(chemin):\n",
    "        if 'horaire' in dirs:\n",
    "            chemin_horaire = os.path.join(root, 'horaire')\n",
    "            nom_parent = os.path.basename(root)\n",
    "            fichiers_csv = []\n",
    "            try:\n",
    "                for sous_dossier in next(os.walk(chemin_horaire))[1]:\n",
    "                    chemin_sous_dossier = os.path.join(chemin_horaire, sous_dossier)\n",
    "                    fichiers_csv += [f for f in os.listdir(chemin_sous_dossier) if f.endswith('.csv')]\n",
    "                # Ajouter seulement si le nombre de fichiers CSV est égal à 20\n",
    "                if len(fichiers_csv) >= taille_histo:\n",
    "                    stations_avec_20_csv[nom_parent] = {\n",
    "                        'compteur': len(fichiers_csv),\n",
    "                        'fichiers': fichiers_csv\n",
    "                    }\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de l'accès à {chemin_horaire}: {e}\")\n",
    "    return stations_avec_20_csv\n",
    "\n",
    "\n",
    "# Utilisation de la fonction\n",
    "# resultat = selectionner_stations_avec_20_csv(chemin_dossier)\n",
    "# for station, infos in resultat.items():\n",
    "#     print(f\"Station: {station}, Nombre de fichiers: {infos['compteur']}\")\n",
    "#     for fichier in infos['fichiers']:\n",
    "#         print(f\"  {fichier}\")\n",
    "#     print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b634fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# logger=logging.getLogger('colorlog_example')\n",
    "\n",
    "\n",
    "# def charger_dataframes(chemin_base, taille_histo=19):\n",
    "#     \"\"\"\n",
    "#     Charge les dataframes à partir des fichiers CSV dans les dossiers spécifiés\n",
    "#     et les stocke dans un dictionnaire avec un nom spécifique basé sur la station et l'année,\n",
    "#     tout en enregistrant des informations de débogage.\n",
    "\n",
    "#     Args:\n",
    "#         chemin_base (str): Le chemin de base où les fichiers CSV sont stockés.\n",
    "#         taille_histo(int): permet de selectionner les stations possédant un historique complet sur un certain nombre d'années\n",
    "\n",
    "#     Returns:\n",
    "#         dict: Un dictionnaire contenant les DataFrames chargés.\n",
    "#     \"\"\"\n",
    "#     stations_avec_20_csv = selectionner_stations_avec_20_csv(chemin_base, taille_histo)\n",
    "#     dataframes = {}\n",
    "\n",
    "#     for station, infos in stations_avec_20_csv.items():\n",
    "#         if infos[\n",
    "#                 'compteur'] >= taille_histo:  #selectionne uniquement les stations dont l'historique est complet sur par ex au moins 19 ans\n",
    "#             logger.debug(\n",
    "#                 f\"Traitement de la station {station} avec {infos['compteur']} fichiers.\"\n",
    "#             )\n",
    "#             for fichier_csv in infos['fichiers']:\n",
    "#                 nom_fichier_sans_extension = fichier_csv.rsplit('.', 1)[0]\n",
    "#                 annee = nom_fichier_sans_extension.split('-')[2].split('_')[0]\n",
    "#                 nom_dataframe = f\"df_meteo_france_{station}_{annee}\"\n",
    "#                 # Construction du chemin\n",
    "#                 chemin_dossier = os.path.join(chemin_base, station, \"horaire\",\n",
    "#                                               nom_fichier_sans_extension)\n",
    "#                 # Séparer le nom du fichier par les underscores et retirer le dernier élément\n",
    "#                 parts = chemin_dossier.rsplit('_', 1)\n",
    "#                 chemin_dossier = parts[0] if len(parts) > 1 else chemin_dossier\n",
    "#                 # Remplacer les séparateurs de chemin par '/'\n",
    "#                 chemin_fichier_complet = os.path.join(chemin_dossier,\n",
    "#                                                       fichier_csv).replace(\n",
    "#                                                           '\\\\', '/')\n",
    "\n",
    "#                 # if not os.path.isfile(chemin_fichier_complet):\n",
    "#                 #     logger.debug(f\"Le fichier {chemin_fichier_complet} n'existe pas.\")\n",
    "#                 #     continue\n",
    "\n",
    "#                 logger.debug(\n",
    "#                     f\"Chargement du fichier {chemin_fichier_complet}.\")\n",
    "#                 dataframe = pd.read_csv(chemin_fichier_complet, sep=\";\")\n",
    "#                 globals()[nom_dataframe] = dataframe\n",
    "#                 dataframes[nom_dataframe] = dataframe\n",
    "#                 logger.debug(f\"DataFrame {nom_dataframe} chargé avec succès.\")\n",
    "\n",
    "#     return dataframes\n",
    "\n",
    "# # Exemple d'utilisation de la fonction\n",
    "# base_path = r'C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\upload_dataset_depuis_api'\n",
    "# base_path=base_path.replace(\"\\\\\", \"/\")\n",
    "# dataframes_charges = charger_dataframes(base_path)\n",
    "\n",
    "# # Afficher le résultat\n",
    "# print(\"DataFrames chargés :\")\n",
    "# for nom_df in dataframes_charges:\n",
    "#     print(f\" df:{nom_df}\")\n",
    "# # C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\upload_dataset_depuis_api\\13036003\\horaire\\01-Jan-2004_at_00h00_to_31-Dec-2004_at_23h59\\01-Jan-2004_at_00h00_to_31-Dec-2004_at_23h59_1.csv\n",
    "# # C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Datasets/meteo_france/upload_dataset_depuis_api/13005003/horaire/01-Jan-2004_at_00h00_to_31-Dec-2004_at_23h59/01-Jan-2004_at_00h00_to_31-Dec-2004_at_23h59_1.csv.\n",
    "# # C:/programmation/IA/Projet_Meteo/projet_meteo/Projet_Meteo/Datasets/meteo_france/upload_dataset_depuis_api/13036003/horaire/01-Jan-2011_at_00h00_to_31-Dec-2011_at_23h59/01-Jan-2011_at_00h00_to_31-Dec-2011_at_23h59_1.csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemin_dossier = path_data_meteo_france_upload_data_depuis_api2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8dcea0",
   "metadata": {},
   "source": [
    "### Chargement dataset Meteo France au pas de 6 min donné par météo france données brutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c690da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_meteo_france_T92438_2021=pd.read_csv(\"C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\DATA-20240208T162707Z-001\\DATA\\METEO_FRANCE\\T92438_2021.txt\", header=None, sep=\" \")\n",
    "# df_meteo_france_T92438_2021\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263a1610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_meteo_france_13009 = load_dataframe(\"01-Jan-2004_at_00h00_to_31-Dec-2004_at_23h59_1\",path_data_meteo_france_upload_data_depuis_api, format_type='csv', sep=\";\")\n",
    "# filepath = r\"C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\upload_dataset_depuis_api\\13001009\\horaire\\01-Jan-2017_at_00h00_to_31-Dec-2017_at_23h59\\01-Jan-2017_at_00h00_to_31-Dec-2017_at_23h59_1.csv\"\n",
    "# path_data_mobilis_upload_data_depuis_api=r\"meteo_france\\upload_dataset_depuis_api\\13001009\\horaire\\01-Jan-2017_at_00h00_to_31-Dec-2017_at_23h59\"\n",
    "# filepath=filepath.replace(\"\\\\\",\"/\")\n",
    "# path_data_mobilis_upload_data_depuis_api=path_data_mobilis_upload_data_depuis_api.replace(\"\\\\\",\"/\")\n",
    "# # df_meteo_france_13009 = pd.read_csv(filepath, sep=\";\")\n",
    "# # df_meteo_france_13009 = pd.read_csv(filepath, sep=\";\")\n",
    "# df_meteo_france_13009 = load_dataframe(\"01-Jan-2017_at_00h00_to_31-Dec-2017_at_23h59_1\", path_data_mobilis_upload_data_depuis_api,\n",
    "#                                    format_type='csv', sep=\";\")\n",
    "# df_meteo_france_13009.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30494cca",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# # %%capture captchargedataframes\n",
    "# dataframes_charges = charger_dataframes(base_path)\n",
    "dataframes_charges_filtered={}\n",
    "\n",
    "for nom_df in dataframes_charges:\n",
    "    logger.debug(f\"Traitement du DataFrame {nom_df}\")\n",
    "    df_actuel = dataframes_charges[\n",
    "        nom_df]  # Accéder à l'objet DataFrame à partir du dictionnaire\n",
    "    logger.debug(f\"df_actuel {df_actuel}\")\n",
    "    df_filtered = remove_columns_with_na( df_actuel, threshold=0.4)  # Appliquer la fonction de filtrage\n",
    "    globals()[nom_df] = df_filtered\n",
    "    dataframes_charges_filtered[nom_df] = df_filtered  # Stocker le résultat filtré\n",
    "\n",
    "dataframes_charges_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab3d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0883c317",
   "metadata": {},
   "source": [
    "## Traitement des datasets fournis par méteo France au pas de  6 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e458f52f",
   "metadata": {},
   "source": [
    "### suppresssion des colonnes vides\n",
    "\n",
    "pour meteo france ces colonnes contiennent uniquemment des 00000 ou des 999999, on ne supprime que les colonnes contenant 100% de veleurs nulles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12700150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "from sauve_charge_df_csv_json_pkl import charger_all_dataframes, load_dataframe, save_dataframe\n",
    "\n",
    "logger = logging.getLogger(\"colorlog_example\")\n",
    "\n",
    "\n",
    "def supprimer_colonnes_vides_et_sauvegarder(chemin_des_fichiers_source, chemin_sauvegarde):\n",
    "    \"\"\"\n",
    "    Vérifier et supprimer les colonnes de chaque DataFrame dans un dictionnaire qui ont 100% des cellules\n",
    "    vides ou égales à 999999 et sauvegarder les DataFrames nettoyés.\n",
    "\n",
    "    Args:\n",
    "        dataframes (dict): Dictionnaire de DataFrames à vérifier.\n",
    "        chemin_sauvegarde (str): Chemin de base pour la sauvegarde des fichiers nettoyés.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionnaire des DataFrames nettoyés.\n",
    "    \"\"\"\n",
    "    dataframes_without_colonnes_vides = {}\n",
    "    dataframes = {}\n",
    "\n",
    "    dataframes = charger_all_dataframes(chemin_des_fichiers_source, default_format=\"txt\", sep=\" \")\n",
    "\n",
    "    for df_name, df in dataframes.items():\n",
    "        logger.info(f\"Traitement du DataFrame: {df_name}\")\n",
    "        colonnes_a_supprimer = [col for col in df.columns if (df[col].isnull().all() or (df[col] == 999999).all())]\n",
    "\n",
    "        # Supprimer les colonnes identifiées\n",
    "        if colonnes_a_supprimer:\n",
    "            logger.info(f\"Suppression des colonnes suivantes dans '{df_name}': {colonnes_a_supprimer}\")\n",
    "            df.drop(columns=colonnes_a_supprimer, inplace=True)\n",
    "            logger.info(f\"Les colonnes ont été supprimées dans le DataFrame '{df_name}'\")\n",
    "        else:\n",
    "            logger.info(f\"Aucune colonne à supprimer dans le DataFrame '{df_name}'\")\n",
    "        df_name = df_name.replace('df_', '')\n",
    "        logger.warning(f\"\\n df_name:\\n{df_name} \")\n",
    "        # Sauvegarder le DataFrame nettoyé\n",
    "        # fichier_sauvegarde = os.path.join(chemin_sauvegarde, f\"{df_name}_clean.csv\")\n",
    "        df_clean = save_dataframe(df, df_name, chemin_sauvegarde)\n",
    "        # df_clean=save_dataframe(df, df_name, chemin_sauvegarde)\n",
    "        # df.to_csv(fichier_sauvegarde, index=False)\n",
    "        # logger.info(f\"DataFrame '{df_name}' sauvegardé en tant que: {fichier_sauvegarde}\")\n",
    "        logger.info(f\"DataFrame '{df_name}' sauvegardé \")\n",
    "\n",
    "        # Ajouter le DataFrame nettoyé au dictionnaire\n",
    "        dataframes_without_colonnes_vides[df_name] = df_clean\n",
    "\n",
    "    return dataframes_without_colonnes_vides\n",
    "\n",
    "\n",
    "#['df_station_13005003', 'df_station_13022003', 'df_station_13028004', 'df_station_13030001', 'df_station_13031002', 'df_station_13036003', 'df_station_13047001', 'df_station_13054001', 'df_station_13055001', 'df_station_13055029', 'df_station_13056002', 'df_station_13062002', 'df_station_13074003', 'df_station_13091002', 'df_station_13092001', 'df_station_13103001', 'df_station_13108004', 'df_station_13110003', 'df_station_13111002']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6d5ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemin du dossier contenant les fichiers donnés par meteo france au pas de 6 min sources .txt\n",
    "path_datasets_fourni_par_meteo_france_6min_source\n",
    "logger.info(f\"\\n path_datasets_fourni_par_meteo_france_6min_source:\\n{path_datasets_fourni_par_meteo_france_6min_source} \")\n",
    "# chemin de sauvegarde des fichiers onnés par meteo france au pas de 6 min traités, renommage des colonnes, conversion, suppression des colonnes vides\n",
    "path_sauvegarde_datasets_meteo_france_6min_clean\n",
    "logger.info(f\"\\n path_sauvegarde_datasets_meteo_france_6min_clean:\\n{path_sauvegarde_datasets_meteo_france_6min_clean} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c85fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataframes_without_colonnes_vides = supprimer_colonnes_vides_et_sauvegarder(path_datasets_fourni_par_meteo_france_6min_source,\n",
    "                                                                            # path_sauvegarde_datasets_meteo_france_6min_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5cc97",
   "metadata": {},
   "source": [
    "### formatage, conversion , renommage des colonnes\n",
    "\n",
    "nommage des colonnes en francais, conversion dse unités,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4559eeea",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "#debugage\n",
    "# %debug\n",
    "# import pdb\n",
    "# pdb.set_trace()\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "from sauve_charge_df_csv_json_pkl import charger_all_dataframes, load_dataframe\n",
    "from sauve_charge_df_csv_json_pkl import format_and_save_dataframe\n",
    "from descriptif_dataset_meteo_france import renommer_colonne_df_meteo_france_6min\n",
    "\n",
    "logger = logging.getLogger(\"colorlog_example\")\n",
    "\n",
    "\n",
    "\n",
    "def traiter_et_renommer_dataset(chemin_des_fichiers_source,  chemin_sauvegarde):\n",
    "    \"\"\"\n",
    "    Lit un dataset de Météo France depuis un fichier .txt et renomme ses colonnes.\n",
    "    \"\"\"\n",
    "    dataframes={}\n",
    "    dataframes_formated={}\n",
    "\n",
    "    dataframes=charger_all_dataframes(chemin_des_fichiers_source, default_format=\"csv\", sep=\",\")\n",
    "    # logger.debug(f\"\\n dataframes:\\n{dataframes} \")\n",
    "    # # Lecture du fichier .txt\n",
    "    # df = pd.read_csv(fichier, sep=' ', header=None)\n",
    "    for df_name, df in dataframes.items():\n",
    "        # Renommage des colonnes\n",
    "        logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "        logger.debug(f\"\\n df_name:\\n{df_name} \")\n",
    "        logger.debug(f\"\\n nombre de colonnes:\\n{len(df.columns)} \")\n",
    "\n",
    "        #compte le nombre de colonne du df car le renommage en dépend, en particulier pour la colonne direction\n",
    "        nombre_colonne_df=len(df.columns)\n",
    "        # le nom de la station est necessaire pour traiter la station 13054001 dont les colonnes ne sont pas comme les autres stations\n",
    "        nom_station = df.iloc[0, 0]\n",
    "        logger.debug(f\"\\n nombre_colonne_df:\\n{nombre_colonne_df} \")\n",
    "        # Renommage des colonnes\n",
    "        df.columns = [renommer_colonne_df_meteo_france_6min(f'col{i}',nom_station, nombre_colonne_df) for i in range(len(df.columns))]\n",
    "\n",
    "        logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "        logger.debug(f\"\\n liste colonnes:\\n{df.head(2)} \")\n",
    "\n",
    "        # Formate convertie et sauve les datasets\n",
    "        logger.info(f\" sauvegarde des datasets \")\n",
    "        df_name = df_name.replace('df_', '')\n",
    "        df=format_and_save_dataframe(df, df_name, chemin_sauvegarde)\n",
    "        # Ajouter le DataFrame chargé au dictionnaire\n",
    "        dataframes_formated[df_name] = df\n",
    "        logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "        logger.debug(f\"\\n nombre de colonnes:\\n{len(df.columns)} \")\n",
    "\n",
    "    logger.debug(f\"\\n dataframes:\\n{dataframes_formated} \")\n",
    "    return dataframes_formated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e7916b",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# # chemin du dossier contenant les fichiers donnés par meteo france au pas de 6 min sources .txt\n",
    "# path_datasets_fourni_par_meteo_france_6min_source\n",
    "# logger.info(f\"\\n path_datasets_fourni_par_meteo_france_6min_source:\\n{path_datasets_fourni_par_meteo_france_6min_source} \")\n",
    "\n",
    "# chemin de sauvegarde des fichiers onnés par meteo france au pas de 6 min traités, renommage des colonnes, conversion, suppression des colonnes vides\n",
    "path_sauvegarde_datasets_meteo_france_6min_clean\n",
    "logger.info(f\"\\n path_sauvegarde_datasets_meteo_france_6min_clean:\\n{path_sauvegarde_datasets_meteo_france_6min_clean} \")\n",
    "\n",
    "# chemin de sauvegarde des fichiers onnés par meteo france au pas de 6 min traités,suppression des colonnes vides\n",
    "path_sauvegarde_datasets_meteo_france_6min_suppression_col_vide\n",
    "logger.info(f\"\\n path_sauvegarde_datasets_meteo_france_6min_suppression_col_vide:\\n{path_sauvegarde_datasets_meteo_france_6min_suppression_col_vide} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296af756",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "#test de la fonction\n",
    "# dataframes_formated = traiter_et_renommer_dataset(path_sauvegarde_datasets_meteo_france_6min_clean, path_sauvegarde_datasets_meteo_france_6min_suppression_col_vide)\n",
    "# dataframes_formated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4de974c",
   "metadata": {},
   "source": [
    "## Chargement des dataset meteo france au pas de 6 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead3793",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from sauve_charge_df_csv_json_pkl import charger_all_dataframes, load_dataframe\n",
    "\n",
    "logger = logging.getLogger(\"colorlog_example\")\n",
    "\n",
    "# chemin de sauvegarde des fichiers onnés par meteo france au pas de 6 min traités,suppression des colonnes vides\n",
    "path_sauvegarde_datasets_meteo_france_6min_suppression_col_vide\n",
    "logger.info(f\"\\n path_sauvegarde_datasets_meteo_france_6min_suppression_col_vide:\\n{path_sauvegarde_datasets_meteo_france_6min_suppression_col_vide} \")\n",
    "\n",
    "\n",
    "# # chemin de sauvegarde des fichiers onnés par meteo france au pas de 6 min traités, renommage des colonnes, conversion, suppression des colonnes vides\n",
    "# path_sauvegarde_datasets_meteo_france_6min_clean\n",
    "# logger.info(f\"\\n path_sauvegarde_datasets_meteo_france_6min_clean:\\n{path_sauvegarde_datasets_meteo_france_6min_clean} \")\n",
    "dataframes={}\n",
    "dataframes = charger_all_dataframes(path_sauvegarde_datasets_meteo_france_6min_suppression_col_vide, default_format=\"csv\", sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b0235",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>date</th>\n",
       "      <th>précipitations_(mm)</th>\n",
       "      <th>direction_(°)</th>\n",
       "      <th>humidity_(%)</th>\n",
       "      <th>insolation_(mn)</th>\n",
       "      <th>rayonnement_(J/m²)</th>\n",
       "      <th>vitesse_vent_(km/h)</th>\n",
       "      <th>temperature_(°C)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13054001</td>\n",
       "      <td>2021-01-01 00:06:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.28</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13054001</td>\n",
       "      <td>2021-01-01 00:12:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.20</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13054001</td>\n",
       "      <td>2021-01-01 00:18:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.76</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13054001</td>\n",
       "      <td>2021-01-01 00:24:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.60</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13054001</td>\n",
       "      <td>2021-01-01 00:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.80</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262674</th>\n",
       "      <td>13054001</td>\n",
       "      <td>2023-12-31 23:36:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.24</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262675</th>\n",
       "      <td>13054001</td>\n",
       "      <td>2023-12-31 23:42:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.76</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262676</th>\n",
       "      <td>13054001</td>\n",
       "      <td>2023-12-31 23:48:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>260</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.12</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262677</th>\n",
       "      <td>13054001</td>\n",
       "      <td>2023-12-31 23:54:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>260</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.92</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262678</th>\n",
       "      <td>13054001</td>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.36</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262679 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         station                 date  précipitations_(mm)  direction_(°)  \\\n",
       "0       13054001  2021-01-01 00:06:00                  0.0             70   \n",
       "1       13054001  2021-01-01 00:12:00                  0.0             70   \n",
       "2       13054001  2021-01-01 00:18:00                  0.0             70   \n",
       "3       13054001  2021-01-01 00:24:00                  0.0             80   \n",
       "4       13054001  2021-01-01 00:30:00                  0.0             80   \n",
       "...          ...                  ...                  ...            ...   \n",
       "262674  13054001  2023-12-31 23:36:00                  0.0            280   \n",
       "262675  13054001  2023-12-31 23:42:00                  0.0            270   \n",
       "262676  13054001  2023-12-31 23:48:00                  0.0            260   \n",
       "262677  13054001  2023-12-31 23:54:00                  0.0            260   \n",
       "262678  13054001  2024-01-01 00:00:00                  0.0            270   \n",
       "\n",
       "        humidity_(%)  insolation_(mn)  rayonnement_(J/m²)  \\\n",
       "0                 87                0                   0   \n",
       "1                 86                0                   0   \n",
       "2                 86                0                   0   \n",
       "3                 85                0                   0   \n",
       "4                 84                0                   0   \n",
       "...              ...              ...                 ...   \n",
       "262674            80                0                   0   \n",
       "262675            81                0                   0   \n",
       "262676            82                0                   0   \n",
       "262677            82                0                   0   \n",
       "262678            82                0                   0   \n",
       "\n",
       "        vitesse_vent_(km/h)  temperature_(°C)  \n",
       "0                     26.28               7.6  \n",
       "1                     25.20               7.6  \n",
       "2                     23.76               7.7  \n",
       "3                     21.60               7.9  \n",
       "4                     19.80               8.0  \n",
       "...                     ...               ...  \n",
       "262674                12.24               7.0  \n",
       "262675                14.76               7.1  \n",
       "262676                15.12               7.5  \n",
       "262677                16.92               7.4  \n",
       "262678                18.36               7.4  \n",
       "\n",
       "[262679 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df=dataframes['df_station_13092001']\n",
    "# df = dataframes['df_station_13054001']\n",
    "# df = dataframes['df_station_13092001']\n",
    "\n",
    "df_station_13054001 = dataframes[\"df_station_13054001\"]\n",
    "df_station_13054001\n",
    "\n",
    "\n",
    "#obtenir le nom de la station\n",
    "# df_station_13054001[\"station\"].loc[0]\n",
    "# df_station_13054001.iloc[0,0].dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb75d0",
   "metadata": {},
   "source": [
    "### Test rapide sur les dataframes contenant un certain pourcentage de colonnes contenant des valeurs nulles ou égales à 99999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61739bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test avec un pourcentage de 2 %\n",
    "pourcentage_filtre=2\n",
    "pourcentage_filtre=pourcentage_filtre/100\n",
    "# # Parcourir chaque DataFrame dans le dictionnaire\n",
    "for df_name, df in dataframes.items():\n",
    "    # colonnes_a_supprimer = [col for col in df.columns if (df[col].isnull().sum() / len(df) > 10 or (df[col] == 999999).sum() / len(df) > 10)]\n",
    "    colonnes_avec_40_pourcent_de_nul = [col for col in df.columns if (df[col].isnull().sum() / len(df) > pourcentage_filtre) or (df[col] == 999999).sum() / len(df) > pourcentage_filtre]\n",
    "\n",
    "    if colonnes_avec_40_pourcent_de_nul:\n",
    "        logger.critical(f\"Le DataFrame '{df_name}' contient plus de {pourcentage_filtre*100}%. \\n Nom des colonnes: {colonnes_avec_40_pourcent_de_nul}\")\n",
    "    else:\n",
    "        logger.info(f\"Le DataFrame '{df_name}' ne contient pas plus de {pourcentage_filtre*100}%. \\n Nom des colonnes: {colonnes_avec_40_pourcent_de_nul}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11438da5",
   "metadata": {},
   "source": [
    "# WINDSUP\n",
    "\n",
    "attention il faut supprimer le ; de fin de ligne du dataset car il produit des décalages de colonne\n",
    "\n",
    "Station Id;Date Timestamp [UTC];Date Txt [UTC]; Average Wind Speed [nds];Min Wind Speed [nds];Max Wind Speed [nds];Wind Direction [degree];Wind Direction [txt]\n",
    "\n",
    "44;1210558680;2008-05-12 04:18;2;1;3;180;S ***;***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c14fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import logging\n",
    "# import os\n",
    "# from sauve_charge_df_csv_json_pkl import charger_all_dataframes, load_dataframe\n",
    "# from sauve_charge_df_csv_json_pkl import format_and_save_dataframe\n",
    "# from descriptif_dataset_meteo_france import expliquer_et_renommer_colonne\n",
    "\n",
    "# logger = logging.getLogger(\"colorlog_example\")\n",
    "\n",
    "\n",
    "# # # Création d'un dictionnaire pour le renommage\n",
    "# # colonnes_renommees = {}\n",
    "\n",
    "\n",
    "\n",
    "# def traitement_windsup(nom_fichier_original, nom_fichier_modifie, chemin_sauvegarde_df_windsup_preprocessed):\n",
    "#     # SUPPRESSION DU ; DE FIN DE LIGNE\n",
    "#     # Ouvrir le fichier original en mode lecture et le fichier modifié en mode écriture\n",
    "#     with open(nom_fichier_original, 'r', encoding='utf-8') as fichier_original, open(nom_fichier_modifie, 'w', encoding='utf-8') as fichier_modifie:\n",
    "#         for ligne in fichier_original:\n",
    "#             # Enlever le dernier point-virgule de la ligne (s'il existe) et écrire la ligne modifiée dans le nouveau fichier\n",
    "#             fichier_modifie.write(ligne.rstrip(';\\n') + '\\n')\n",
    "\n",
    "#     #CHARGEMENT FICHIER\n",
    "\n",
    "#     df_windsup = pd.read_csv(nom_fichier_modifie, header=0, sep=';', encoding='utf-8')\n",
    "#     df=df_windsup.copy()\n",
    "#     # Station Id;Date Timestamp [UTC];Date Txt [UTC]; Average Wind Speed [nds];Min Wind Speed [nds];Max Wind Speed [nds];Wind Direction [degree];Wind Direction [txt]\n",
    "#     logger.debug(f\"\\n liste colonnes:\\n{df_windsup.columns.tolist()} \")\n",
    "\n",
    "#     # SUPPRESSION DES COLONNES\n",
    "#     # Supprimer les colonnes spécifiques si elles existent\n",
    "#     colonnes_a_supprimer = ['Date Txt [UTC]', 'Wind Direction [txt]']\n",
    "#     for col in colonnes_a_supprimer:\n",
    "#         if col in df.columns:\n",
    "#             df.drop(columns=[col], inplace=True)\n",
    "#             logger.debug(f\"\\nSuppression de la colonne : {col}\")\n",
    "\n",
    "#     # RENOMMAGE DES COLONNES\n",
    "#     # Création d'un dictionnaire pour le renommage\n",
    "#     colonnes_renommees = {}\n",
    "#     for col in df.columns:\n",
    "#         try:\n",
    "#             # Renommer les colonnes du DataFrame en utilisant la fonction importée\n",
    "#             # Utilisation de la deuxième partie du tuple retourné par `expliquer_et_renommer_colonne` pour le renommage\n",
    "#             _, nouveau_nom = expliquer_et_renommer_colonne(col)\n",
    "#             colonnes_renommees[col] = nouveau_nom\n",
    "\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Erreur lors du chargement de {df}: {e}\")\n",
    "#     # df_renomme['date'] = pd.to_datetime(df_renomme['date'], format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "#     df_renomme = df.rename(columns=colonnes_renommees)\n",
    "\n",
    "#     logger.debug(f\" liste colonne df {df.columns.tolist()} \")\n",
    "#     logger.debug(f\" liste colonne df_renomme {df_renomme.columns.tolist()} \")\n",
    "\n",
    "#     # CONVERSION COLONNE DATE\n",
    "#     # df_renomme['date'] = pd.to_datetime(df_renomme['date'], format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "#     # Convertir les timestamps UNIX en objets datetime\n",
    "#     df_renomme['date'] = pd.to_datetime(df_renomme['date'], unit='s', utc=True)\n",
    "\n",
    "#     df_renomme['date'] = df_renomme['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#     # Vérifier si la colonne 'date' existe dans le DataFrame\n",
    "#     print('date' in df_renomme.columns)\n",
    "\n",
    "#     # Afficher les premières lignes de la colonne 'date' si elle existe\n",
    "#     if 'date' in df_renomme.columns:\n",
    "#         print(df_renomme['date'].head())\n",
    "#     else:\n",
    "#         print(\"La colonne 'date' n'existe pas dans df_renomme.\")\n",
    "\n",
    "#     # SAUVEGARDE\n",
    "#     format_and_save_dataframe(df_renomme, \"windsup_renamed\", chemin_sauvegarde_df_windsup_preprocessed)\n",
    "\n",
    "#     return df_renomme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4804d5",
   "metadata": {},
   "source": [
    "## traitement windsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60312d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "from sauve_charge_df_csv_json_pkl import charger_all_dataframes, load_dataframe\n",
    "from sauve_charge_df_csv_json_pkl import format_and_save_dataframe\n",
    "from descriptif_dataset_meteo_france import expliquer_et_renommer_colonne\n",
    "\n",
    "logger = logging.getLogger(\"colorlog_example\")\n",
    "\n",
    "\n",
    "def traitement_windsup(nom_fichier_original, nom_fichier_modifie, chemin_sauvegarde_df_windsup_preprocessed):\n",
    "    \"\"\"\n",
    "    Traite un fichier CSV de données windSUP en supprimant le dernier caractère ';',\n",
    "    supprime certaines colonnes non nécessaires, renomme les colonnes selon un dictionnaire prédéfini,\n",
    "    et enregistre le nouveau DataFrame dans un chemin spécifié.\n",
    "\n",
    "    Parameters:\n",
    "    - nom_fichier_original: Chemin vers le fichier original.\n",
    "    - nom_fichier_modifie: Chemin vers le fichier temporaire modifié.\n",
    "    - chemin_sauvegarde_df_windsup_preprocessed: Chemin pour sauvegarder le DataFrame prétraité.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame prétraité et renommé.\n",
    "\n",
    "    >>> traitement_windsup('data/windsup_original.csv', 'data/windsup_modifie.csv', 'data/preprocessed')\n",
    "    DataFrame avec les colonnes renommées et nettoyées.\n",
    "    \"\"\"\n",
    "    # Suppression du caractère ';' de fin de ligne\n",
    "    with open(nom_fichier_original, 'r', encoding='utf-8') as fichier_original, open(nom_fichier_modifie, 'w', encoding='utf-8') as fichier_modifie:\n",
    "        for ligne in fichier_original:\n",
    "            fichier_modifie.write(ligne.rstrip(';\\n') + '\\n')\n",
    "\n",
    "    # Chargement du fichier après prétraitement\n",
    "    try:\n",
    "        df_windsup = pd.read_csv(nom_fichier_modifie, header=0, sep=';', encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors du chargement du fichier modifié: {e}\")\n",
    "        return None\n",
    "\n",
    "    df = df_windsup.copy()\n",
    "    logger.debug(f\"\\n liste colonnes:\\n{df_windsup.columns.tolist()} \")\n",
    "\n",
    "    # Suppression des colonnes non désirées\n",
    "    for col in ['Date Txt [UTC]', 'Wind Direction [txt]']:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "            logger.debug(f\"\\nSuppression de la colonne : {col}\")\n",
    "\n",
    "    # Renommage des colonnes\n",
    "    colonnes_renommees = {}\n",
    "    for col in df.columns:\n",
    "        _, nouveau_nom = expliquer_et_renommer_colonne(col)\n",
    "        colonnes_renommees[col] = nouveau_nom\n",
    "\n",
    "    df_renomme = df.rename(columns=colonnes_renommees)\n",
    "    logger.debug(f\"\\n liste colonne df_renomme {df_renomme.columns.tolist()} \")\n",
    "\n",
    "    # Conversion de la colonne 'date'\n",
    "    df_renomme['date'] = pd.to_datetime(df_renomme['date'], unit='s', utc=True)\n",
    "    df_renomme['date'] = df_renomme['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Vérification de l'existence de la colonne 'date'\n",
    "    if 'date' in df_renomme.columns:\n",
    "        logger.debug(df_renomme['date'].head())\n",
    "    else:\n",
    "        logger.error(\"La colonne 'date' n'existe pas dans df_renomme.\")\n",
    "\n",
    "    # Sauvegarde du DataFrame prétraité\n",
    "    try:\n",
    "        format_and_save_dataframe(df_renomme, \"windsup_renamed\", chemin_sauvegarde_df_windsup_preprocessed)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de la sauvegarde du DataFrame: {e}\")\n",
    "\n",
    "    return df_renomme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80792ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"traitement_windsup\"\n",
    "nom_fichier = r'C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\data_windsup\\Station44-2008-1-1_to_2024-1-12_23_59_59.csv'\n",
    "chemin_sauvegarde_df_windsup_preprocessed = r'C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\data_windsup\\df_preprocessing'\n",
    "\n",
    "nom_fichier_original = r'C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\data_windsup\\Station44-2008-1-1_to_2024-1-12_23_59_59.csv'\n",
    "nom_fichier_modifie = r'C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\data_windsup\\Station44-2008-1-1_to_2024-1-12_23_59_59_modified.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf992f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lancement fonction\n",
    "# df_windsup=traitement_windsup(nom_fichier_original, nom_fichier_modifie, chemin_sauvegarde_df_windsup_preprocessed)\n",
    "# df_windsup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba1341e",
   "metadata": {},
   "source": [
    "## Chargement dataset windsup renommé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e330b2c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " path_fichier_windsup_renamed:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\data_windsup\\df_preprocessing\\df_windsup_renamed.csv \n",
      "\n",
      "\n",
      " path_dossier_windsup_renamed:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\data_windsup\\df_preprocessing \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>date</th>\n",
       "      <th>direction_(°)</th>\n",
       "      <th>vitesse_vent_(km/h)</th>\n",
       "      <th>vitesse_vent_mini_(km/h)</th>\n",
       "      <th>vitesse_vent_max_(km/h)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>2008-05-12 02:18:00</td>\n",
       "      <td>180.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>2008-05-12 01:10:00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>2008-05-12 02:10:00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>2008-05-11 22:58:00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>2008-05-12 02:32:00</td>\n",
       "      <td>180.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297310</th>\n",
       "      <td>44</td>\n",
       "      <td>2024-01-12 15:26:28</td>\n",
       "      <td>270.0</td>\n",
       "      <td>46.8</td>\n",
       "      <td>43.2</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297311</th>\n",
       "      <td>44</td>\n",
       "      <td>2024-01-12 15:28:26</td>\n",
       "      <td>270.0</td>\n",
       "      <td>43.2</td>\n",
       "      <td>39.6</td>\n",
       "      <td>46.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297312</th>\n",
       "      <td>44</td>\n",
       "      <td>2024-01-12 15:30:29</td>\n",
       "      <td>270.0</td>\n",
       "      <td>43.2</td>\n",
       "      <td>36.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297313</th>\n",
       "      <td>44</td>\n",
       "      <td>2024-01-12 15:32:28</td>\n",
       "      <td>270.0</td>\n",
       "      <td>43.2</td>\n",
       "      <td>36.0</td>\n",
       "      <td>57.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297314</th>\n",
       "      <td>44</td>\n",
       "      <td>2024-01-12 15:34:35</td>\n",
       "      <td>315.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>46.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1297315 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         station                 date  direction_(°)  vitesse_vent_(km/h)  \\\n",
       "0             44  2008-05-12 02:18:00          180.0                  7.2   \n",
       "1             44  2008-05-12 01:10:00           45.0                  3.6   \n",
       "2             44  2008-05-12 02:10:00           45.0                  3.6   \n",
       "3             44  2008-05-11 22:58:00           45.0                  3.6   \n",
       "4             44  2008-05-12 02:32:00          180.0                  3.6   \n",
       "...          ...                  ...            ...                  ...   \n",
       "1297310       44  2024-01-12 15:26:28          270.0                 46.8   \n",
       "1297311       44  2024-01-12 15:28:26          270.0                 43.2   \n",
       "1297312       44  2024-01-12 15:30:29          270.0                 43.2   \n",
       "1297313       44  2024-01-12 15:32:28          270.0                 43.2   \n",
       "1297314       44  2024-01-12 15:34:35          315.0                 36.0   \n",
       "\n",
       "         vitesse_vent_mini_(km/h)  vitesse_vent_max_(km/h)  \n",
       "0                             3.6                     10.8  \n",
       "1                             0.0                      7.2  \n",
       "2                             0.0                      7.2  \n",
       "3                             0.0                      7.2  \n",
       "4                             0.0                      7.2  \n",
       "...                           ...                      ...  \n",
       "1297310                      43.2                     54.0  \n",
       "1297311                      39.6                     46.8  \n",
       "1297312                      36.0                     54.0  \n",
       "1297313                      36.0                     57.6  \n",
       "1297314                      32.4                     46.8  \n",
       "\n",
       "[1297315 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "from sauve_charge_df_csv_json_pkl import  load_dataframe\n",
    "\n",
    "logger = logging.getLogger(\"colorlog_example\")\n",
    "\n",
    "# chemin_sauvegarde_df_windsup_preprocessed = \"df_preprocessing\"\n",
    "# chemin_dossier_windsups_renamed = os.path.join(path_data_windsup_upload_data_depuis_api, chemin_sauvegarde_df_windsup_preprocessed)\n",
    "# logger.debug(f\"\\npath_data_windsup_upload_data_depuis_api :\\n{path_data_windsup_upload_data_depuis_api} \")\n",
    "# logger.debug(f\"\\nchemin_dossier_windsups_renamed :\\n{chemin_dossier_windsups_renamed} \")\n",
    "\n",
    "# path_fichier_windsup_renamed = trouver_chemin_element_depuis_workspace(nom_element, est_un_dossier=False)\n",
    "# logger.debug(f\"\\n nom_element:\\n{nom_element} \")\n",
    "print(f\"\\n path_fichier_windsup_renamed:\\n{path_fichier_windsup_renamed} \\n\")\n",
    "print(f\"\\n path_dossier_windsup_renamed:\\n{path_dossier_windsup_renamed} \\n\")\n",
    "\n",
    "df_windsup = load_dataframe(\"df_windsup_renamed\", path_dossier_windsup_renamed, format_type='csv', sep=\",\")\n",
    "df_windsup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28563dcd",
   "metadata": {},
   "source": [
    "# Chargement  des dataset meteo france / mobilis/ windsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9028455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture\n",
    "# Charger les datasets\n",
    "# windsup\n",
    "path = path_dossier_windsup_renamed\n",
    "df_name = 'df_windsup_renamed'\n",
    "df_windsup_renamed = load_dataframe(df_name, path, format_type='csv', sep=\",\")\n",
    "logger.debug(f\"\\n windsup:\\n{df_windsup_renamed.head(3)} \\n\")\n",
    "\n",
    "# mobilis\n",
    "path = path_data_mobilis_upload_data_depuis_api\n",
    "df_name = 'df_clean_7col'\n",
    "df_clean_7col = load_dataframe(df_name, path, format_type='csv', sep=\",\")\n",
    "logger.debug(f\"\\n mobilis:\\n{df_clean_7col.head(3)} \\n\")\n",
    "\n",
    "# meteo fance 1h\n",
    "path = path_dataset_interpolated_pas_moyen_meteo_france_pas_1h\n",
    "df_name = 'df_meteo_france_pas_1h_fusionned'\n",
    "df_meteo_france_pas_1h_fusionned = load_dataframe(df_name, path, format_type='csv', sep=\",\")\n",
    "logger.debug(f\"\\n df_meteo_france_pas_1h_fusionned:\\n{df_meteo_france_pas_1h_fusionned.head(3)} \\n\")\n",
    "\n",
    "# meteo fance 6min\n",
    "path = path_dataset_interpolated_pas_moyen_meteo_france_pas_6min\n",
    "df_name = 'df_meteo_france_pas_6min_fusionned'\n",
    "df_meteo_france_pas_6min_fusionned = load_dataframe(df_name, path, format_type='csv', sep=\",\")\n",
    "logger.debug(f\"\\n df_meteo_france_pas_6min_fusionned:\\n{df_meteo_france_pas_6min_fusionned.head(3)} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe55fda5",
   "metadata": {},
   "source": [
    "## chargement des df resample sur une plage de dates communes et resampled au pas de 12 min à partir du dataset fusionné de meteo france au pas de 1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c55aba",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# # %%capture\n",
    "# Charger les datasets ayant une plage commune entre meteo france 1 ou 6 min et mobilis et windsup, les datasets sont clean et resample au pas de 12 min\n",
    "\n",
    "## DATASETS FABRIQUES A PARTIR DE METEO FRANCE AU PAS DE 1H\n",
    "# windsup  associe à meteo france 1h\n",
    "path = os.path.join(path_dataset_interpolated_pas_moyen_windsup, 'windsup_1h_ranged_resampled_cleaned')\n",
    "df_name = 'df_windsup_1h_ranged_resampled_cleaned'\n",
    "df_windsup_1h_ranged_resampled_cleaned = load_dataframe(df_name, path, format_type='csv', sep=\",\")\n",
    "logger.debug(f\"\\n windsup 1h:\\n{df_windsup_1h_ranged_resampled_cleaned.head(3)} \\n\")\n",
    "\n",
    "# mobilis associe à meteo france 1h\n",
    "# création de l'url\n",
    "path = os.path.join(path_dataset_interpolated_pas_moyen_mobilis, 'mobilis_1h_ranged_resampled_cleaned')\n",
    "df_name = 'df_mobilis_1h_ranged_resampled_cleaned'\n",
    "df_mobilis_1h_ranged_resampled_cleaned = load_dataframe(df_name, path, format_type='csv', sep=\",\")\n",
    "logger.debug(f\"\\n mobilis 1h:\\n{df_mobilis_1h_ranged_resampled_cleaned.head(3)} \\n\")\n",
    "\n",
    "# meteo france 1h\n",
    "path = path = os.path.join(path_dataset_interpolated_pas_moyen_meteo_france, 'meteo_france_pas_1h_ranged_resampled_cleaned')\n",
    "df_name = 'df_meteo_france_pas_1h_choix_dataset_meteo_france_ranged_resampled_cleaned'\n",
    "df_meteo_france_pas_1h_choix_dataset_meteo_france_ranged_resampled_cleaned = load_dataframe(df_name, path, format_type='csv', sep=\",\")\n",
    "logger.debug(\n",
    "    f\"\\n df_meteo_france_pas_1h_choix_dataset_meteo_france_ranged_resampled_cleaned:\\n{df_meteo_france_pas_1h_choix_dataset_meteo_france_ranged_resampled_cleaned.head(3)} \\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaa3107",
   "metadata": {},
   "source": [
    "## Chargement des 2 datasets concatenés à partir de meteo_france au pas de 1h et au pas de 6 min avec mobililset windsu^p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84bdc58",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "## DATASET CONCATENE FABRIQUES A PARTIR DE METEO FRANCE AU PAS DE 6 MIN\n",
    "# windsup  associe à meteo france 6 min\n",
    "path = os.path.join(path_datasets_concatened, 'concatenated_6min')\n",
    "df_name = 'df_concatenated_6min'\n",
    "df_concatenated_6min = load_dataframe(df_name, path, format_type='csv', sep=\",\")\n",
    "logger.debug(f\"\\n df_concatenated_6min:\\n{df_concatenated_6min.head(3)} \\n\")\n",
    "\n",
    "## DATASET CONCATENE FABRIQUES A PARTIR DE METEO FRANCE AU PAS DE 1H\n",
    "# windsup  associe à meteo france 6 min\n",
    "path = os.path.join(path_datasets_concatened, 'concatenated_1h')\n",
    "df_name = 'df_concatenated_1h'\n",
    "df_concatenated_1h = load_dataframe(df_name, path, format_type='csv', sep=\",\")\n",
    "logger.debug(f\"\\n df_concatenated_1h:\\n{df_concatenated_1h.head(3)} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a60e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconfigurer_logging(params={\"niveau_log\": 'DEBUG'})\n",
    "\n",
    "df_concatenated_6min\n",
    "df =df_concatenated_1h\n",
    "logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "\n",
    "df_concatenated_1h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e1baa8",
   "metadata": {},
   "source": [
    "# Graphique "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fdc6f4",
   "metadata": {},
   "source": [
    "## Fonction courbes temperature, vent, rafales, direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562468cf",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from gestion_logging import reconfigurer_logging\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import dateparser\n",
    "from dateparser import parse\n",
    "\n",
    "logger = logging.getLogger('colorlog_example')\n",
    "\n",
    "\n",
    "def est_date_valide(date_str):\n",
    "    \"\"\"\n",
    "  Fonction pour valider le format de la date et de l'heure\n",
    "  \"\"\"\n",
    "    try:\n",
    "        dateparser.parse(date_str)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def graphiques_plot(df, cols, date_debut, date_fin, pas,method_interpolation=\"linear\", order=None):\n",
    "\n",
    "    # # Contrôle de saisie du format de la date de fin\n",
    "    # if not est_date_valide(date_fin):\n",
    "    #     raise ValueError(\"Format de la date de fin invalide\")\n",
    "    # elif date_fin == \"now()\":\n",
    "    #     date_fin = pd.to_datetime('now')\n",
    "    df=df.copy()\n",
    "    logger.debug(f\"\\n cols[0]:\\n{cols[0]}, cols[1]:\\n{cols[1]}, cols[2]:\\n{cols[2]} ,cols[3]:\\n{cols[3]}\")\n",
    "    if 'date' in df.columns :\n",
    "        # Conversion de la colonne 'date' en format datetime, si ce n'est pas déjà fait\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df.set_index('date', inplace=True)\n",
    "    else:\n",
    "        raise KeyError(\"La colonne 'date' n'existe pas dans le DataFrame.\")\n",
    "\n",
    "    df = df[cols]\n",
    "    # Conversion des strings date_debut et date_fin en datetime, si ce n'est pas déjà fait\n",
    "    date_debut = pd.to_datetime(date_debut)\n",
    "    date_fin = pd.to_datetime(date_fin) if date_fin != \"now()\" else pd.to_datetime('now')\n",
    "\n",
    "    # # Filtrage des dates en utilisant l'index\n",
    "    # df_filtered = df.loc[date_debut:date_fin]\n",
    "\n",
    "\n",
    "    # Assurez-vous que l'index du DataFrame est de type DateTimeIndex\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"L'index du DataFrame doit être de type DateTimeIndex.\")\n",
    "\n",
    "    # Filtrage du DataFrame basé sur les dates\n",
    "    flitre_dates = (df.index >= date_debut) & (df.index <= date_fin)\n",
    "    df_filtered = df.loc[flitre_dates]\n",
    "\n",
    "\n",
    "    # Conversion de la valeur de pas en type timedelta64[ns] et rééchantillonnage\n",
    "    pas = pd.to_timedelta(pas)\n",
    "    df_grouped = df_filtered.resample(pas).mean()\n",
    "\n",
    "    # Traitement des valueurs manquantes liées au resample\n",
    "    # Interpolation des valeurs manquantes après le rééchantillonnage\n",
    "    df_interpolated = df_grouped.interpolate(method=method_interpolation, order=order)\n",
    "    df_grouped=df_interpolated\n",
    "    # # Contrôle de saisie du format de la date de début\n",
    "    # if not est_date_valide(date_debut):\n",
    "    #     raise ValueError(\"Format de la date de début invalide\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # #  filtrage des dates\n",
    "    # df = df.loc[df['date'] >= date_debut]\n",
    "    # df = df.loc[df['date'] <= date_fin]\n",
    "\n",
    "    # # pas='7D'\n",
    "    # # Regroupement des données par heure et calcul des moyennes\n",
    "    # df_grouped = df.resample(pas, on='date').mean()\n",
    "\n",
    "    # Configuration du style de matplotlib pour utiliser un fond sombre\n",
    "    plt.style.use('classic')\n",
    "\n",
    "    # Création de trois subplots verticalement\n",
    "    fig, (ax1, ax22, ax3) = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "    # Graphique de la Température\n",
    "    ax1.plot(df_grouped.index,\n",
    "             df_grouped[cols[0]],\n",
    "             marker='o',\n",
    "             linestyle='-',\n",
    "             color='tomato',\n",
    "             label=cols[0])\n",
    "    ax1.set_title(f'{cols[0]} Moyenne par Heure')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel(cols[0])\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.legend()\n",
    "\n",
    "    # # Graphique de la Vitesse du Vent\n",
    "    # ax2.plot(df_grouped.index, df_grouped['vitesse_(km/h)'], marker='s', linestyle='-', color='royalblue', label='Vitesse du Vent (m/s)')\n",
    "    # ax2.set_title('Vitesse du Vent Moyenne par Heure')\n",
    "    # ax2.set_xlabel('Date')\n",
    "    # ax2.set_ylabel('Vitesse (m/s)')\n",
    "    # ax2.legend()\n",
    "\n",
    "    # Graphique des rafales\n",
    "    ax22.plot(df_grouped.index, df_grouped[cols[1]], marker='.', linestyle='-', color='royalblue', label=cols[1])\n",
    "    ax22.plot(df_grouped.index, df_grouped[cols[2]], marker='.', linestyle='-', color='seagreen', label=cols[2])\n",
    "    ax22.set_title(f'{cols[1]} et {cols[2]} Moyenne par Heure')\n",
    "    ax22.set_xlabel('Date')\n",
    "    ax22.set_ylabel(cols[2])\n",
    "    ax22.tick_params(axis='x', rotation=45)\n",
    "    ax22.legend()\n",
    "\n",
    "    # Graphique de la Direction du Vent\n",
    "    ax3.plot(df_grouped.index, df_grouped[cols[3]], marker='^', linestyle='-', color='seagreen', label=cols[3])\n",
    "    ax3.set_title(f'{cols[3]} Moyenne par Heure')\n",
    "    ax3.set_xlabel('Date')\n",
    "    ax3.set_ylabel('Direction (degrés)')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.legend()\n",
    "\n",
    "    # Ajustement de l'espacement entre les graphiques pour éviter le chevauchement\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Affichage des graphiques\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ba817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df\n",
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "df = df_13005003_01jan2014_01mar2024.copy()\n",
    "# # Convertir la colonne 'date' en datetime si ce n'est pas déjà fait\n",
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "# # Définir la colonne 'date' comme nouvel index du DataFrame\n",
    "# df.set_index('date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c947d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4abc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture capturegraphique\n",
    "# df_mobilis3_12col['date']\n",
    "# df_mobilis3_12col.set_index('date', inplace=False)\n",
    "\n",
    "df_mobilis3_12col[\"temperature_(°C)\"].plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c8bd6",
   "metadata": {},
   "source": [
    "#### Test des differentes possibilités d'interpoler les valeurs manquantes, visualisation graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c938dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_toutes_methodes_interpolation(df, col, date_debut, date_fin, pas, liste_methodes_interpolation):\n",
    "    \"\"\"\n",
    "    Teste différentes méthodes d'interpolation sur une colonne spécifique d'un DataFrame entre deux dates.\n",
    "\n",
    "    Pour chaque méthode d'interpolation fournie, cette fonction génère un graphique pour visualiser\n",
    "    les résultats de l'interpolation.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Le DataFrame sur lequel effectuer l'interpolation.\n",
    "        col (str): Le nom de la colonne du DataFrame à interpoler.\n",
    "        date_debut (str): Date de début pour la période d'interpolation (format YYYY-MM-DD).\n",
    "        date_fin (str): Date de fin pour la période d'interpolation (format YYYY-MM-DD).\n",
    "        pas (str): Le pas de temps pour l'interpolation (ex: '1D' pour 1 jour ou '0.01D' pour 14.4 minutes ).\n",
    "        liste_methodes_interpolation (list): Une liste de dictionnaires, chaque dictionnaire contient\n",
    "                                              'method' pour le nom de la méthode d'interpolation et\n",
    "                                              'order' (optionnel) pour l'ordre d'interpolation si nécessaire.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Itération sur chaque méthode d'interpolation fournie dans la liste\n",
    "    for method_info in liste_methodes_interpolation:\n",
    "        logger.info(f\"Interpolation en cours avec la méthode : {method_info['method']}\")\n",
    "\n",
    "        # Affichage de la méthode d'interpolation utilisée\n",
    "        print(f\"\\nMéthode d'interpolation : {method_info['method']}\")\n",
    "\n",
    "        # Vérification de la présence de l'argument 'order' pour les méthodes qui le nécessitent\n",
    "        if 'order' in method_info:\n",
    "            # Appel de la fonction graphiques_plot avec 'order' si spécifié\n",
    "            graphiques_plot(df, col, date_debut, date_fin, pas, method_interpolation=method_info['method'], order=method_info['order'])\n",
    "        else:\n",
    "            # Appel de la fonction graphiques_plot sans 'order'\n",
    "            graphiques_plot(df, col, date_debut, date_fin, pas, method_interpolation=method_info['method'])\n",
    "\n",
    "\n",
    "\n",
    "# Convertir la colonne 'date' en datetime si ce n'est pas déjà fait\n",
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "# Définir la colonne 'date' comme nouvel index du DataFrame\n",
    "# df.set_index('date', inplace=True)\n",
    "# Choix colonne\n",
    "\n",
    "col =[ 'temperature_sol_(°C)', \"vitesse_vent_moyen_at_1_m_(km/h)\", 'vitesse_vent_max_(km/h)', 'Direction_(°)']\n",
    "\n",
    "liste_methodes_interpolation= [\n",
    "    # {'method': 'linear'},                         # Interpolation linéaire simple.\n",
    "    # {'method': 'time'},                           # Interpolation linéaire prenant en compte l'index de temps.\n",
    "    # {'method': 'index', 'order': None},           # Interpolation linéaire prenant en compte l'index numérique.\n",
    "    # {'method': 'values', 'order': None},          # Identique à 'index'.\n",
    "    # {'method': 'pad'},                            # Remplissage avec la dernière valeur connue.\n",
    "    {'method': 'nearest'},                        # Utilisation de la valeur la plus proche pour l'imputation.\n",
    "    # {'method': 'zero'},                           # Interpolation de type step, qui crée un palier jusqu'à la prochaine valeur.\n",
    "    # {'method': 'slinear'},                        # Interpolation spline linéaire (ordre 1).\n",
    "    # {'method': 'quadratic', 'order': 2},          # Interpolation spline d'ordre 2.\n",
    "    # {'method': 'cubic', 'order': 3},              # Interpolation spline d'ordre 3 (cubique).\n",
    "    # {'method': 'barycentric'},                    # Interpolation barycentrique.\n",
    "    # {'method': 'polynomial', 'order': 3},         # Interpolation polynomiale. Vous devez spécifier l'ordre.\n",
    "    # {'method': 'krogh'},                          # Interpolation de Krogh.\n",
    "    # {'method': 'piecewise_polynomial', 'order': None}, # Interpolation polynomiale par morceaux.\n",
    "    # {'method': 'spline', 'order': 3},             # Interpolation spline d'ordre spécifié.\n",
    "    # {'method': 'pchip'},                          # Interpolation PCHIP (Hermite polynomiale par morceaux cubique).\n",
    "    # {'method': 'akima'},                          # Interpolation Akima.\n",
    "    # {'method': 'cubicspline'}                     # Interpolation par spline cubique, spécifique à pandas.\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1093e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_13005003_01jan2014_01mar2024.copy()\n",
    "df = df_station_13054001.copy()\n",
    "date_debut = '2023-02-01'\n",
    "date_fin = '2023-02-10'\n",
    "pas = '0.01D'\n",
    "\n",
    "#test fpnction\n",
    "# test_toutes_methodes_interpolation(df, col, date_debut, date_fin, pas, liste_methodes_interpolation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003265ac",
   "metadata": {},
   "source": [
    "### Calcul de la durée d'un pas en minute traduit en fraction de jour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace84a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pas='0.01D'\n",
    "# pas='D'\n",
    "# Pour avoir un pas de 2 min :\n",
    "duree_pas_en_min=2\n",
    "fraction_dun_jour = duree_pas_en_min / (60 * 24)\n",
    "# fraction_dun_jour\n",
    "print(f\"\\npour avoir un pas de {duree_pas_en_min}min il faut indiquer:  \\npas='{round(fraction_dun_jour,5)}D' \\n\")\n",
    "\n",
    "fraction_dun_jour='0.01D'\n",
    "duree_pas_en_min = float(fraction_dun_jour.replace(\"D\",\"\")) * 24 * 60\n",
    "print(f\"Un pas de \\npas=('{fraction_dun_jour}')  \\ncorrespond à une durée de  \\n'{round(duree_pas_en_min,5)} min \\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9693628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_pas_temps(pas_ou_duree):\n",
    "    \"\"\"\n",
    "    Convertit une durée en minutes en fraction de jour en format 'xD' ou l'inverse.\n",
    "\n",
    "    Args:\n",
    "        pas_ou_duree (str or int): Soit une durée en minutes (int) à convertir en fraction de jour,\n",
    "                                   soit une fraction de jour en format 'xD' (str) à convertir en minutes.\n",
    "\n",
    "    Returns:\n",
    "        str or float: Conversion de la durée en 'xD' si l'entrée est un int,\n",
    "                      ou conversion de 'xD' en durée en minutes si l'entrée est un str.\n",
    "    \"\"\"\n",
    "    if isinstance(pas_ou_duree, int):  # Si l'entrée est une durée en minutes\n",
    "        fraction_dun_jour = pas_ou_duree / (60 * 24)\n",
    "        pas = round(fraction_dun_jour, 5)\n",
    "        logger.info(f\"pour avoir un pas de \\n{pas_ou_duree} min \\nil faut indiquer: \\npas='{pas}D'\")\n",
    "        return pas\n",
    "        # return f\"pour avoir un pas de {pas_ou_duree} min il faut indiquer: \\npas='{round(fraction_dun_jour,5)}D'\"\n",
    "    elif isinstance(pas_ou_duree, str) and pas_ou_duree.endswith(\"D\"):  # Si l'entrée est une fraction de jour\n",
    "        duree_pas_en_min = float(pas_ou_duree.replace(\"D\", \"\")) * 24 * 60\n",
    "        duree=round(duree_pas_en_min,5)\n",
    "        logger.info(f\"Un pas de \\npas=('{pas_ou_duree}') \\ncorrespond à une durée de \\n{duree} min\")\n",
    "        return duree\n",
    "    else:\n",
    "        logger.error(\"Format non reconnu. Veuillez entrer un entier pour les minutes ou une chaîne en format 'xD' pour les jours.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34906bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "print(convertir_pas_temps(2))  # Convertir une durée en minutes en fraction de jour\n",
    "print(convertir_pas_temps('0.01D'))  # Convertir une fraction de jour en durée en minutes\n",
    "duree_pas_en_min = convertir_pas_temps('0.01D')\n",
    "pas = convertir_pas_temps(2)\n",
    "print(f\"\\n duree_pas_en_min:\\n{duree_pas_en_min} \\n\")\n",
    "print(f\"\\n 2 min correspondent à un pas:\\n'{pas}D' \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3350d0",
   "metadata": {},
   "source": [
    "### Modif niveau de log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259af1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du logging\n",
    "# 'DEBUG' 'INFO' 'WARNING' 'ERROR'  'CRITICAL'\n",
    "\n",
    "# params = {\"niveau_log\": 'DEBUG'}\n",
    "# params = {\"niveau_log\": 'INFO'}\n",
    "# params = {\"niveau_log\": 'WARNING'}\n",
    "params = {\"niveau_log\": 'ERROR'}\n",
    "# params = {\"niveau_log\": 'CRITICAL'}\n",
    "reconfigurer_logging(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a1d6a",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## remplacement des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d57dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def remplir_valeurs_manquantes(data):\n",
    "    \"\"\"\n",
    "    Remplit les valeurs manquantes dans le dataset.\n",
    "\n",
    "    :param data: DataFrame Pandas contenant les données.\n",
    "    :return: DataFrame avec les valeurs manquantes remplies, nombre de NA avant et après le traitement.\n",
    "    \"\"\"\n",
    "    # Compter le nombre de valeurs NA avant le traitement\n",
    "    na_avant = data.isna().sum()\n",
    "    print(\"Nombre de valeurs NA avant le traitement :\")\n",
    "    print(na_avant)\n",
    "\n",
    "    # Utiliser la méthode 'interpolate' pour une approximation linéaire des valeurs manquantes\n",
    "    data_interpolated = data.interpolate()\n",
    "\n",
    "    # et 'fillna' avec la méthode 'bfill' et 'ffill' pour les valeurs restantes au début et à la fin\n",
    "    data_filled = data_interpolated.fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "    # Compter le nombre de valeurs NA après le traitement\n",
    "    na_apres = data_filled.isna().sum()\n",
    "    print(\"\\nNombre de valeurs NA après le traitement :\")\n",
    "    print(na_apres)\n",
    "\n",
    "    return data_filled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df30336",
   "metadata": {},
   "source": [
    "## suppression des outliers\n",
    "### Visualisation et suppression des outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b971eb",
   "metadata": {},
   "source": [
    "#### traitement 1 par 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e583d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def traitement_outliers_na(data, col):\n",
    "    \"\"\"\n",
    "    Traite les valeurs manquantes\n",
    "    Visualise et supprime les outliers des colonnes numeriques,\n",
    "    graphiques avant et après le traitement.\n",
    "\n",
    "    :param data: DataFrame contenant les données.\n",
    "    :param col: Nom de la colonne à visualiser.\n",
    "    \"\"\"\n",
    "    #  suppression des na\n",
    "    data = remplir_valeurs_manquantes(data)\n",
    "    # Configuration du style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Visualisation avant le traitement des outliers\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(y=data[col])\n",
    "    plt.title(f\"Avant le traitement des outliers - {col}\")\n",
    "\n",
    "    # Calcul des limites pour identifier les outliers\n",
    "    q1 = data[col].quantile(0.25)\n",
    "    q3 = data[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    limite_basse = q1 - 1.5 * iqr\n",
    "    limite_haute = q3 + 1.5 * iqr\n",
    "\n",
    "    # Suppression des outliers\n",
    "    data_sans_outliers = data[(data[col] >= limite_basse) & (data[col] <= limite_haute)]\n",
    "\n",
    "    logger.info(f\" data {data.shape}\")\n",
    "\n",
    "    # Visualisation après le traitement des outliers\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(y=data_sans_outliers[col])\n",
    "    plt.title(f\"Après le traitement des outliers - {col}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Scatter plot pour visualiser la répartition des données avant et après\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(range(len(data)), data[col], alpha=0.5)\n",
    "    plt.title(f\"Avant le traitement des outliers - {col}\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(range(len(data_sans_outliers)), data_sans_outliers[col], alpha=0.5)\n",
    "    plt.title(f\"Après le traitement des outliers - {col}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    logger.info(f\" data_sans_outliers {data_sans_outliers.shape}\")\n",
    "    return data_sans_outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e4e8e",
   "metadata": {},
   "source": [
    "#### meteo france traitement outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f7c3f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def suppression_outliers_meteo_france_pas_6min(dataframes):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    liste_df_sans_outlier={}\n",
    "    liste_des_df = [station for station in dataframes.keys()]\n",
    "    print(f\"\\nliste_des_df :\\n{liste_des_df} \\n\")\n",
    "\n",
    "    df = dataframes[liste_des_df[0]]\n",
    "    print(f\"\\n describe df :\\n{df.describe()} \\n\")\n",
    "\n",
    "    for df_name in liste_des_df:\n",
    "        print(f\"\\n df:\\n{df} \\n\")\n",
    "        df = dataframes[df_name]\n",
    "        # df_name=str(df['station'].iloc[0])\n",
    "        print(f\"\\n df_name:\\n{df_name} \\n\")\n",
    "        print(f\"\\n describe df :\\n{df.describe()} \\n\")\n",
    "        list_col = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        print(f\"\\n list_col:\\n{list_col} \\n\")\n",
    "        # ['précipitations_(mm)', 'direction_(°)', 'insolation_(mn)', 'vitesse_vent_(km/h)', 'temperature_(°C)']\n",
    "        list_col.remove(\"station\")\n",
    "        list_col.remove('précipitations_(mm)') if 'précipitations_(mm)' in list_col else list_col\n",
    "        print(f\"\\n list_col:\\n{list_col} \\n\")\n",
    "\n",
    "        # Traitement des df\n",
    "        # Suppression des outliers\n",
    "        for col in list_col:\n",
    "            # Remplacer les valeurs 99999 par NaN pour utiliser ffill ensuite\n",
    "            df[col].replace(999999, np.nan, inplace=True)\n",
    "            #Comme il peut y avoir dautres valeurs aberrantes que 999999, on supprime toutes les valeurs superieures à 90000\n",
    "            df[col] = df[col].apply(lambda x: np.nan if x > 90000 else x)\n",
    "            # Appliquer forward fill pour les valeurs NaN\n",
    "            df[col].fillna(method='ffill', inplace=True)\n",
    "            valeur_max = df[col].max()\n",
    "            print(f\"\\n valeur_max:\\n{valeur_max} pour la colonne {col} \\n\")\n",
    "            # Clipper les valeurs au-delà de la valeur maximale\n",
    "            df[col] = df[col].clip(upper=valeur_max)\n",
    "\n",
    "            # affichage et remplacement des outliers restant\n",
    "            df_sans_outlier = traitement_outliers_na(df, col)\n",
    "\n",
    "            # ajout du df sans outliers dans la liste\n",
    "            liste_df_sans_outlier[df_name] = df_sans_outlier\n",
    "            # liste_df_sans_outlier.append(df_sans_outlier)\n",
    "            # liste_df_sans_outlier=pd.DataFrame()\n",
    "    return liste_df_sans_outlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d97031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# # Configuration du logger\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def suppression_outliers_meteo_france_pas_6min(dataframes):\n",
    "    \"\"\"\n",
    "    Supprime les outliers des DataFrames contenus dans un dictionnaire en se basant sur des critères prédéfinis.\n",
    "    Les valeurs aberrantes sont identifiées selon des seuils spécifiques et sont soit corrigées, soit supprimées.\n",
    "\n",
    "    Args:\n",
    "        dataframes (dict): Un dictionnaire contenant des identifiants de stations météo comme clés et des DataFrames comme valeurs.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un dictionnaire avec les mêmes clés que l'entrée, mais où chaque DataFrame a eu ses outliers supprimés ou corrigés.\n",
    "\n",
    "    Exemple:\n",
    "        >>> test_data = {'df_test': pd.DataFrame({'temperature_(°C)': [15, 999999, 20], 'précipitations_(mm)': [0, 0, 999999]})}\n",
    "        >>> result = suppression_outliers_meteo_france_pas_6min(test_data)\n",
    "        Le résultat sera un dictionnaire avec les DataFrames nettoyés.\n",
    "    \"\"\"\n",
    "    liste_df_sans_outlier = {}\n",
    "    liste_des_df = [station for station in dataframes.keys()]\n",
    "    logger.debug(f\"Liste des DataFrames à traiter: {liste_des_df}\")\n",
    "\n",
    "    for df_name in liste_des_df:\n",
    "        try:\n",
    "            df = dataframes[df_name]\n",
    "            logger.debug(f\"Traitement du DataFrame {df_name}\")\n",
    "            list_col = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            # Suppression de colonnes spécifiques de la liste de traitement\n",
    "            if \"station\" in list_col: list_col.remove(\"station\")\n",
    "            if 'précipitations_(mm)' in list_col: list_col.remove('précipitations_(mm)')\n",
    "            logger.debug(f\"Colonnes numériques traitées: {list_col}\")\n",
    "\n",
    "            for col in list_col:\n",
    "                try:\n",
    "                    # Suppression des outliers\n",
    "                    df[col].replace(999999, np.nan, inplace=True)\n",
    "                    df[col] = df[col].apply(lambda x: np.nan if x > 90000 else x)\n",
    "                    df[col].fillna(method='ffill', inplace=True)\n",
    "                    valeur_max = df[col].max()\n",
    "                    df[col] = df[col].clip(upper=valeur_max)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Erreur lors du traitement de la colonne {col} dans {df_name}: {e}\")\n",
    "\n",
    "            # Ici, inclure la logique pour traiter et nettoyer davantage le DataFrame df avant de l'ajouter au dictionnaire\n",
    "            df_sans_outlier = traitement_outliers_na(df, col)\n",
    "            liste_df_sans_outlier[df_name] = df_sans_outlier\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du traitement du DataFrame {df_name}: {e}\")\n",
    "\n",
    "    return liste_df_sans_outlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff499e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste_df_sans_outlier = suppression_outliers_meteo_france_pas_6min(dataframes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac0a0cf",
   "metadata": {},
   "source": [
    "#### sauvegarde des df meteo france au pas de 6 min clean et sans outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079cb487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugage\n",
    "# %debug\n",
    "# import pdb\n",
    "# pdb.set_trace()\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "from sauve_charge_df_csv_json_pkl import charger_all_dataframes, load_dataframe\n",
    "from sauve_charge_df_csv_json_pkl import format_and_save_dataframe, save_dataframe\n",
    "from descriptif_dataset_meteo_france import renommer_colonne_df_meteo_france_6min\n",
    "\n",
    "logger = logging.getLogger(\"colorlog_example\")\n",
    "\n",
    "\n",
    "def sauve_df_meteo_france_6min_clean_sans_outliers(liste_df_sans_outlier , chemin_sauvegarde):\n",
    "    \"\"\"\n",
    "    Sauvegarde les datafames donnés par meteo france au pas de 6 min clean et sans outliers\n",
    "    \"\"\"\n",
    "\n",
    "    dataframes_clean_sans_outlier = {}\n",
    "\n",
    "\n",
    "    for df_name, df in liste_df_sans_outlier.items():\n",
    "\n",
    "        # Formate convertie et sauve les datasets\n",
    "        logger.info(f\" sauvegarde des datasets \")\n",
    "        df_name = df_name.replace('df_', '')\n",
    "        df = save_dataframe(df, df_name, chemin_sauvegarde)\n",
    "        # Ajouter le DataFrame chargé au dictionnaire\n",
    "        dataframes_clean_sans_outlier[df_name] = df\n",
    "\n",
    "\n",
    "    logger.debug(f\"\\n dataframes:\\n{dataframes_clean_sans_outlier} \")\n",
    "    return dataframes_clean_sans_outlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cea5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste_df_sans_outlier.keys()\n",
    "# # chemin de sauvegarde des fichiers donnés par meteo france au pas de 6 min traités, suppression des outliers\n",
    "\n",
    "# path_sauvegarde_datasets_meteo_france_6min_clean_sans_outlier\n",
    "# print(f\"\\n path_sauvegarde_datasets_meteo_france_6min_clean_sans_outlier:\\n{path_sauvegarde_datasets_meteo_france_6min_clean_sans_outlier} \\n\")\n",
    "\n",
    "# dataframes_clean_sans_outlier=sauve_df_meteo_france_6min_clean_sans_outliers(liste_df_sans_outlier,path_sauvegarde_datasets_meteo_france_6min_clean_sans_outlier)\n",
    "# dataframes_clean_sans_outlier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e4ec63",
   "metadata": {},
   "source": [
    "#### Chargement des df meteo france au pas de 6 min clean et sans outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8b1ea",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "#debugage\n",
    "# %debug\n",
    "# import pdb\n",
    "# pdb.set_trace()\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "from sauve_charge_df_csv_json_pkl import charger_all_dataframes, load_dataframe\n",
    "from sauve_charge_df_csv_json_pkl import format_and_save_dataframe\n",
    "from descriptif_dataset_meteo_france import renommer_colonne_df_meteo_france_6min\n",
    "\n",
    "logger = logging.getLogger(\"colorlog_example\")\n",
    "\n",
    "\n",
    "def charge_df_meteo_france_6min_clean_sans_outliers(chemin_sauvegarde):\n",
    "    \"\"\"\n",
    "    Charge  les datafames donnés par meteo france au pas de 6 min clean et sans outliers\n",
    "    \"\"\"\n",
    "\n",
    "    dataframes_clean_sans_outlier = {}\n",
    "\n",
    "    dataframes = charger_all_dataframes(chemin_sauvegarde, default_format=\"csv\", sep=\",\")\n",
    "    # logger.debug(f\"\\n dataframes:\\n{dataframes} \")\n",
    "    # # Lecture du fichier .txt\n",
    "    # df = pd.read_csv(fichier, sep=' ', header=None)\n",
    "    for df_name, df in dataframes.items():\n",
    "        # Renommage des colonnes\n",
    "        # logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "        # logger.debug(f\"\\n df_name:\\n{df_name} \")\n",
    "        dataframes_clean_sans_outlier[df_name]=df\n",
    "        # logger.debug(f\"\\n df_name:\\n{df_name} \")\n",
    "\n",
    "\n",
    "    return dataframes_clean_sans_outlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb356da",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " path_sauvegarde_datasets_meteo_france_6min_clean_sans_outlier:\n",
      "C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\DONNEES_6_MNs_POUR_WIND\\traitement_datasets_fourni_par_meteo_france_6min_clean_sans_outliers \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['df_station_13005003', 'df_station_13022003', 'df_station_13028004', 'df_station_13030001', 'df_station_13031002', 'df_station_13036003', 'df_station_13047001', 'df_station_13054001', 'df_station_13055001', 'df_station_13055029', 'df_station_13056002', 'df_station_13062002', 'df_station_13074003', 'df_station_13091002', 'df_station_13092001', 'df_station_13103001', 'df_station_13108004', 'df_station_13110003', 'df_station_13111002'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chemin de sauvegarde des fichiers donnés par meteo france au pas de 6 min traités, suppression des outliers\n",
    "path_sauvegarde_datasets_meteo_france_6min_clean_sans_outlier\n",
    "print(f\"\\n path_sauvegarde_datasets_meteo_france_6min_clean_sans_outlier:\\n{path_sauvegarde_datasets_meteo_france_6min_clean_sans_outlier} \\n\")\n",
    "dataframes_meteo_france_6_min_clean_sans_outlier = charge_df_meteo_france_6min_clean_sans_outliers(path_sauvegarde_datasets_meteo_france_6min_clean_sans_outlier)\n",
    "\n",
    "\n",
    "dataframes_meteo_france_6_min_clean_sans_outlier.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc4d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframes_meteo_france_6_min_clean_sans_outlier[\"df_station_13005003\"]\n",
    "df=df.copy()\n",
    "# df=df_station_13054001.copy()\n",
    "# df=df_station_13005003.copy()\n",
    "\n",
    "logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "# ['station', 'date', 'précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'vitesse_vent_(km/h)', 'temperature_(°C)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d220718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\n df_mobilis3_7col_sans_outlier.describe():\\n{df_mobilis3_7col_sans_na.describe()} \\n\")\n",
    "# print(f\"\\n df_mobilis3_12col_sans_outlier.describe():\\n{df_mobilis3_12col_sans_na.describe()} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7cec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_mobilis3_7col_sans_na.copy()\n",
    "# list_col = df.select_dtypes(include=[np.number]).columns\n",
    "# print(f\"\\n list_col:\\n{list_col} \\n\")\n",
    "\n",
    "# # Initialisation d'un DataFrame pour recevoir les données sans outliers\n",
    "# df_mobilis3_7col_sans_outlier = df.copy()\n",
    "\n",
    "# Suppression colonne\n",
    "# col='ma_colonne'\n",
    "# df.drop(columns=[col], inplace=True)\n",
    "\n",
    "list_col = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# ['précipitations_(mm)', 'direction_(°)', 'insolation_(mn)', 'vitesse_vent_(km/h)', 'temperature_(°C)']\n",
    "list_col.remove(\"station\")\n",
    "print(f\"\\n list_col:\\n{list_col} \\n\")\n",
    "# for col in list_col:\n",
    "#     df_mobilis3_7col_sans_outlier = traitement_outliers_na(df_mobilis3_7col_sans_outlier, col)\n",
    "\n",
    "for col in list_col:\n",
    "    # Remplacer les valeurs 99999 par NaN pour utiliser ffill ensuite\n",
    "    df[col].replace(999999, np.nan, inplace=True)\n",
    "    #Comme il peut y avoir dautres valeurs aberrantes que 999999, on supprime toutes les valeurs superieures à 90000\n",
    "    df[col]=df[col].apply(lambda x: np.nan if x > 90000 else x)\n",
    "    # Appliquer forward fill pour les valeurs NaN\n",
    "    df[col].fillna(method='ffill', inplace=True)\n",
    "    valeur_max=df[col].max()\n",
    "    print(f\"\\n valeur_max:\\n{valeur_max} pour la colonne {col} \\n\")\n",
    "    # Clipper les valeurs au-delà de la valeur maximale\n",
    "    df[col] = df[col].clip(upper=valeur_max)\n",
    "\n",
    "    # affichage et remplacement des outliers restant\n",
    "    df_sans_outlier = traitement_outliers_na(df, col)\n",
    "# # Il faut s'assurer que df_sans_outliers est bien mis à jour dans la fonction traitement_outliers_na\n",
    "\n",
    "# print(f\"\\n df_sans_outliers.describe():\\n{df_mobilis3_7col_sans_outlier.describe()} \\n\")\n",
    "#  list_col:\n",
    "# ['précipitations_(mm)', 'direction_(°)', 'insolation_(mn)', 'vitesse_vent_(km/h)', 'temperature_(°C)']\n",
    "#  valeur_max:\n",
    "# 12.0 pour la colonne précipitations_(mm)\n",
    "#  valeur_max:\n",
    "# 360.0 pour la colonne direction_(°)\n",
    "#  valeur_max:\n",
    "# 100.0 pour la colonne insolation_(mn)\n",
    "#  valeur_max:\n",
    "# 3599996.4 pour la colonne vitesse_vent_(km/h)\n",
    "#  valeur_max:\n",
    "# 999725.85 pour la colonne temperature_(°C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746330b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_mobilis3_12col_sans_na.copy()\n",
    "# list_col = df.select_dtypes(include=[np.number]).columns\n",
    "# print(f\"\\n list_col:\\n{list_col} \\n\")\n",
    "\n",
    "# # Initialisation d'un DataFrame pour recevoir les données sans outliers\n",
    "# df_mobilis3_12col_sans_outlier = df.copy()\n",
    "\n",
    "# for col in list_col:\n",
    "#     df_mobilis3_12col_sans_outlier = traitement_outliers_na(df_mobilis3_12col_sans_outlier, col)\n",
    "\n",
    "# # Il faut s'assurer que df_sans_outliers est bien mis à jour dans la fonction traitement_outliers_na\n",
    "\n",
    "# print(f\"\\n df_sans_outliers.describe():\\n{df_mobilis3_12col_sans_outlier.describe()} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c070f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\n df_mobilis3_7col_sans_outlier.describe():\\n{df_mobilis3_7col_sans_outlier.describe()} \\n\")\n",
    "# print(f\"\\n df_mobilis3_12col_sans_outlier.describe():\\n{df_mobilis3_12col_sans_outlier.describe()} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f1a12",
   "metadata": {},
   "source": [
    "### sauvegarde dataset sans outiliers ni na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mobilis3_12col_sans_outlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0efac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df_clean_12col = format_and_save_dataframe(df_mobilis3_12col_sans_outlier, \"clean_12col\", path_data_mobilis_upload_data_depuis_api)\n",
    "# df_clean_7col = format_and_save_dataframe(df_mobilis3_7col_sans_outlier, \"clean_7col\", path_data_mobilis_upload_data_depuis_api)\n",
    "\n",
    "# df_clean_12col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d078fe59",
   "metadata": {},
   "source": [
    "### Chargement dataset df_sans_outlier_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b351f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean_12col = load_dataframe(\"df_clean_12col\",  path_data_mobilis_upload_data_depuis_api,  format_type='pkl')\n",
    "# df_clean_7col = load_dataframe(\"df_clean_7col\",  path_data_mobilis_upload_data_depuis_api,  format_type='pkl')\n",
    "# df_clean_7col\n",
    "\n",
    "# print(f\"\\n df_clean_7col:\\n{df_clean_7col.head(2)} \\n\")\n",
    "# print(f\"\\n df_clean_12col:\\n{df_clean_12col.head(2)} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea32caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\n df_clean_7col:\\n{df_clean_7col.describe()} \\n\")\n",
    "# print(f\"\\n df_clean_12col:\\n{df_clean_12col.describe()} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3571aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean_12col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae8f478",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610a58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def normaliser_donnees(data):\n",
    "    \"\"\"\n",
    "    Normalise les données numériques à des valeurs entre 0 et 1 tout en conservant les colonnes non numériques.\n",
    "\n",
    "    :param data: DataFrame contenant les données.\n",
    "    :return: DataFrame normalisé avec colonnes non numériques conservées.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Séparation des colonnes numériques et non numériques\n",
    "    data_numerique = data.select_dtypes(include=[np.number])\n",
    "    data_non_numerique = data.select_dtypes(exclude=[np.number])\n",
    "\n",
    "    # Normalisation des données numériques\n",
    "    scaler = MinMaxScaler()\n",
    "    data_numerique_scaled = scaler.fit_transform(data_numerique)\n",
    "\n",
    "    # Conversion en DataFrame et conservation des noms de colonnes\n",
    "    data_numerique_scaled = pd.DataFrame(data_numerique_scaled,\n",
    "                                         columns=data_numerique.columns)\n",
    "\n",
    "    # Réintégration des colonnes non numériques\n",
    "    data_final = pd.concat(\n",
    "        [data_numerique_scaled,\n",
    "         data_non_numerique.reset_index(drop=True)],\n",
    "        axis=1)\n",
    "    logger.info(data_final.head())\n",
    "\n",
    "    return data_final\n",
    "\n",
    "\n",
    "# # Exemple d'utilisation\n",
    "# # Supposons que df_sans_outlier_na soit votre DataFrame initial avec une colonne de date\n",
    "# df_sans_outlier_normalized = normaliser_donnees(df_sans_outlier_na)\n",
    "\n",
    "# Afficher le résultat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df75691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean_12col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_clean_12col.copy()\n",
    "# df_clean_12col_normalized = normaliser_donnees(df)\n",
    "# df = df_clean_7col.copy()\n",
    "# df_clean_7col_normalized = normaliser_donnees(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_clean_7col_normalized\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc801fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_clean_12col_normalized\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e676da7",
   "metadata": {},
   "source": [
    "### Sauvegarde dataset normalisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c7df55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# df_normalized_12col = format_and_save_dataframe(df_clean_12col_normalized, \"normalized_12col\", path_data_mobilis_upload_data_depuis_api)\n",
    "# df_normalized_7col = format_and_save_dataframe(df_clean_7col_normalized, \"normalized_7col\", path_data_mobilis_upload_data_depuis_api)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f67e5a7",
   "metadata": {},
   "source": [
    "### Chargement dataset normalisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea526901",
   "metadata": {
    "tags": [
     "parameters",
     "chargement_Df"
    ]
   },
   "outputs": [],
   "source": [
    "df_normalized_12col = load_dataframe(\"df_normalized_12col\",  path_data_mobilis_upload_data_depuis_api,  format_type='pkl')\n",
    "df_normalized_7col = load_dataframe(\"df_normalized_7col\",  path_data_mobilis_upload_data_depuis_api,  format_type='pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convertir la colonne 'date' au format datetime\n",
    "df_normalized_12col['date'] = pd.to_datetime(df_normalized_12col['date'])\n",
    "\n",
    "# Définir la colonne 'date' comme index du DataFrame\n",
    "df_normalized_12col = df_normalized_12col.set_index('date')\n",
    "\n",
    "\n",
    "# Convertir la colonne 'date' au format datetime\n",
    "df_normalized_7col['date'] = pd.to_datetime(df_normalized_7col['date'])\n",
    "\n",
    "# Définir la colonne 'date' comme index du DataFrame\n",
    "df_normalized_7col = df_normalized_7col.set_index('date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efec439",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_12col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9cfa44",
   "metadata": {},
   "source": [
    "# Analyse graphique du dataset\n",
    "## Fonction pour tracer les données de série temporelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d6d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_time_series(data, columns, title=\"Time Series Data\"):\n",
    "    \"\"\"\n",
    "    Trace les séries temporelles pour les colonnes spécifiées.\n",
    "\n",
    "    :param data: DataFrame contenant la série temporelle.\n",
    "    :param columns: Liste des noms de colonnes à tracer.\n",
    "    :param title: Titre du graphique.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    for col in columns:\n",
    "        plt.plot(data.index, data[col], label=col)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c001d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=df_normalized_12col.copy()\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe4f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_normalized_12col.copy()\n",
    "# df=df_normalized_7col.copy()\n",
    "print(f\"\\n df.columns.to_list:\\n{df.columns.tolist()} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a6eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_time_series(df_mobilis_sans_outlier,columns=df_mobilis2.select_dtypes(include=[np.number]).columns)\n",
    "# columns = ['temperature_(°)', 'vitesse_(km/h)', 'rafale_(km/h)']\n",
    "columns = ['wave_amplitude_(m)', 'wave_period_(s)', 'direction_de_surface_(°)', 'temperature_eau_(°C)', 'humidity_(%)',\n",
    "           'pression_(bar)', 'temperature_(°C)', 'direction_(°)', 'vitesse_(km/h)', 'rafale_(km/h)', 'vitesse_surface_(km/h)']\n",
    "columns = ['vitesse_(km/h)', 'rafale_(km/h)', 'vitesse_surface_(km/h)']\n",
    "plot_time_series(df, columns)\n",
    "columns = ['direction_de_surface_(°)',  'direction_(°)']\n",
    "plot_time_series(df, columns)\n",
    "columns = ['direction_de_surface_(°)',  'direction_(°)']\n",
    "plot_time_series(df, columns)\n",
    "columns = ['temperature_eau_(°C)']\n",
    "plot_time_series(df, columns)\n",
    "columns = ['wave_amplitude_(m)', 'wave_period_(s)']\n",
    "plot_time_series(df, columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867db3a",
   "metadata": {},
   "source": [
    "## Fonction pour afficher une matrice de corrélation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8045d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_correlation_matrix(data, title=\"Correlation Matrix\"):\n",
    "    \"\"\"\n",
    "    Affiche la matrice de corrélation des caractéristiques dans le DataFrame.\n",
    "\n",
    "    :param data: DataFrame dont on souhaite afficher la corrélation.\n",
    "    :param title: Titre du graphique.\n",
    "    !! ne choisir que des colonnes numeriques\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = data.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_correlation_matrix(df)\n",
    "print(f\"\\n df.columns.to_list:\\n{df.columns.tolist()} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supposons que df_13001009_01jan2014_01mar2024 est votre DataFrame original\n",
    "df = df_13001009_01jan2014_01mar2024.copy()\n",
    "\n",
    "# Étape 1 et 2: Exclure 'station' si elle existe et utiliser 'date' comme index\n",
    "if 'station' in df.columns:\n",
    "    df = df.drop(columns='station')\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y%m%d%H')  # Ajustez le format si nécessaire\n",
    "df = df.set_index('date')\n",
    "\n",
    "# Étape 3: Sélectionner uniquement les colonnes numériques\n",
    "df_numeric = df.select_dtypes(include=[np.number])\n",
    "\n",
    "\n",
    "# Étape 4: Tracer la matrice de corrélation\n",
    "def plot_correlation_matrix(df):\n",
    "    corr = df.corr()\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True)\n",
    "    plt.title(\"Matrice de Corrélation\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_correlation_matrix(df_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b0d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Exemple de DataFrame ajusté avec les vrais noms de colonnes\n",
    "# Note : Remplacez `df` par le nom de votre DataFrame réel\n",
    "# df = ...\n",
    "print(df.columns.tolist())\n",
    "#Assurez-vous que les données sont numériques (devraient déjà l'être selon vos noms de colonnes)\n",
    "df['vitesse_vent_moyen_at_1_m_(km/h)'] = pd.to_numeric(df['vitesse_vent_moyen_at_1_m_(km/h)'], errors='coerce')\n",
    "df['Direction_(°)'] = pd.to_numeric(df['Direction_(°)'], errors='coerce')\n",
    "\n",
    "# Calcul de la corrélation entre vitesse du vent et direction\n",
    "corr_matrix = df[['vitesse_vent_moyen_at_1_m_(km/h)', 'Direction_(°)']].corr()\n",
    "print(\"Matrice de corrélation entre la vitesse et la direction du vent :\")\n",
    "print(corr_matrix)\n",
    "\n",
    "# Visualisation avec un scatter plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(data=df, x='vitesse_vent_moyen_at_1_m_(km/h)', y='Direction_(°)')\n",
    "plt.title('Corrélation entre la vitesse et la direction du vent')\n",
    "plt.xlabel('Vitesse du vent à 1m (km/h)')\n",
    "plt.ylabel('Direction du vent (°)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_normalized_7col.copy()\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "plot_correlation_matrix(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb164a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[['wave_amplitude_(m)', 'wave_period_(s)', 'direction_de_surface_(°)','rafale_(km/h)', 'vitesse_(km/h)', 'direction_(°)']]\n",
    "# plot_correlation_matrix(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c74bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[['pression_(bar)', 'temperature_(°C)','rafale_(km/h)', 'vitesse_(km/h)', 'direction_(°)']]\n",
    "# plot_correlation_matrix(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28463f9b",
   "metadata": {},
   "source": [
    "## Fonction pour créer des scatter plots entre deux variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2143151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(data, x_col, y_col, title=\"Scatter Plot\"):\n",
    "    \"\"\"\n",
    "    Crée un scatter plot entre deux variables.\n",
    "\n",
    "    :param data: DataFrame contenant les données.\n",
    "    :param x_col: Nom de la colonne pour l'axe des x.\n",
    "    :param y_col: Nom de la colonne pour l'axe des y.\n",
    "    :param title: Titre du graphique.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=data, x=x_col, y=y_col)\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88edf090",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(df, df['vitesse_(km/h)'], df['temperature_(°C)'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3422bfc",
   "metadata": {},
   "source": [
    "## Fonction pour créer des diagrammes de dispersion avec régression linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe4193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression(data, x_col, y_col, title=\"Regression Plot\"):\n",
    "    \"\"\"\n",
    "    Crée un diagramme de dispersion avec une ligne de régression linéaire.\n",
    "\n",
    "    :param data: DataFrame contenant les données.\n",
    "    :param x_col: Nom de la colonne pour l'axe des x.\n",
    "    :param y_col: Nom de la colonne pour l'axe des y.\n",
    "    :param title: Titre du graphique.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.regplot(x=x_col,\n",
    "                y=y_col,\n",
    "                data=data,\n",
    "                scatter_kws={\n",
    "                    'alpha': 0.5,\n",
    "                    'color': 'blue'\n",
    "                },\n",
    "                line_kws={'color': 'orange'})\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb8669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_sans_outlier_normalized\n",
    "x_col = 'vitesse_(km/h)'\n",
    "y_col = 'temperature_(°C)'\n",
    "plot_regression(df, x_col, y_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb070b",
   "metadata": {},
   "source": [
    "## Fonction pour visualiser la distribution d'une variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38791fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(data, column, title=\"Distribution Plot\"):\n",
    "    \"\"\"\n",
    "    Affiche la distribution d'une variable.\n",
    "\n",
    "    :param data: DataFrame contenant les données.\n",
    "    :param column: Nom de la colonne à visualiser.\n",
    "    :param title: Titre du graphique.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data[column], kde=True)\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a9938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_normalized_12col.copy()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff11cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_col = df.select_dtypes(include=[np.number]).columns\n",
    "# ['temperature_(°)', 'vitesse_(km/h)', 'rafale_(km/h)', 'direction']\n",
    "print(f\"\\n list_col:\\n{list_col} \\n\")\n",
    "for col in list_col:\n",
    "    plot_distribution(data=df, column=col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf94dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_distribution(df_sans_outlier,'rafale_(km/h)')\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2af796",
   "metadata": {},
   "source": [
    "# Modelisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b538ba88",
   "metadata": {},
   "source": [
    "### Modèles Potentiels et Approches\n",
    "1. Régressions avancées (par exemple, Random Forests ou Gradient Boosting Machines comme XGBoost) : \n",
    "- Ces modèles peuvent capturer des relations non linéaires complexes entre les caractéristiques d'entrée (comme la température, l'humidité, la direction du vent précédente, etc.) et la vitesse ou la direction du vent. Ils sont flexibles et souvent performants sans nécessiter une grande quantité de données d'entraînement.\n",
    "\n",
    "2. Réseaux de neurones récurrents (RNN), en particulier LSTM et GRU :\n",
    "- Ces modèles sont conçus pour traiter des séquences de données, ce qui les rend particulièrement adaptés pour des prévisions temporelles où la relation temporelle entre les points de données est cruciale. Ils pourraient être plus adaptés pour prédire des séquences de vitesse de vent sur les trois horizons temporels.\n",
    "\n",
    "3. Modèles hybrides : \n",
    "- Une approche combinant des réseaux neuronaux pour capturer des dynamiques temporelles complexes, avec des modèles basés sur des arbres pour exploiter des relations spatiales et non linéaires entre les caractéristiques, peut offrir une flexibilité et une performance accrues.\n",
    "\n",
    "### Étapes \n",
    "1. Prétraitement des données :  \n",
    "- Assurez-vous que vos données sont nettoyées et normalisées. Pour les modèles temporels, structurez vos données en séquences qui reflètent les dépendances temporelles.\n",
    "\n",
    "2. Feature engineering : \n",
    "- Créez des caractéristiques supplémentaires qui pourraient aider le modèle à mieux comprendre les dynamiques météorologiques, telles que des indicateurs de changement de temps, des moyennes mobiles, ou des mesures de tendance.\n",
    "\n",
    "3. Sélection de modèles et validation croisée : \n",
    "- Testez plusieurs modèles et configurations pour identifier ceux qui offrent les meilleures performances sur vos données. Utilisez la validation croisée, en particulier une forme temporelle de la validation croisée pour les séries temporelles, pour évaluer la robustesse de vos modèles.\n",
    "\n",
    "4. Optimisation des hyperparamètres : \n",
    "- Ajustez les hyperparamètres de vos modèles sélectionnés pour maximiser la performance. Des outils comme Grid Search ou Random Search peuvent être utiles, ou des approches plus sophistiquées comme Bayesian Optimization.\n",
    "\n",
    "5. Évaluation et déploiement : Une fois que vous avez sélectionné le modèle le plus performant, évaluez-le sur un jeu de données de test pour vous assurer qu'il répond à vos attentes en termes de précision de prévision pour les différents horizons temporels. Déployez ensuite le modèle pour une utilisation en temps réel, en prévoyant une mise à jour régulière avec de nouvelles données pour maintenir sa précision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c280a",
   "metadata": {},
   "source": [
    "    1. Les stations:\n",
    "\n",
    " **13028004, 13030001, 13031002, 13055001, 13092001**\n",
    "\n",
    "contiennent les colonnes suivantes :\n",
    "\n",
    "station, date, précipitations_(mm), temperature_(°C)\n",
    "\n",
    "    2. Les stations:\n",
    "\n",
    "**13054001, 13005003, 13022003, 13036003, 13047001, 13055029, 13062002, 13074003, 13091002, 13103001, 13108004, 13110003, 13111002**\n",
    "\n",
    " contiennent les colonnes suivantes :\n",
    "\n",
    "station, date, précipitations_(mm), direction_(°), humidity_(%), vitesse_vent_(km/h), temperature_(°C)\n",
    "\n",
    "    3. La station:\n",
    "**13056002** contient les colonnes suivantes :\n",
    "\n",
    "station, date, direction_(°), temperature_(°C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427add2c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "## Regression linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c70ed50",
   "metadata": {},
   "source": [
    "### changement niveau de log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf25cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du logging\n",
    "# 'DEBUG' 'INFO' 'WARNING' 'ERROR'  'CRITICAL'\n",
    "\n",
    "# params = {\"niveau_log\": 'DEBUG'}\n",
    "# params = {\"niveau_log\": 'INFO'}\n",
    "# params = {\"niveau_log\": 'WARNING'}\n",
    "params = {\"niveau_log\": 'ERROR'}\n",
    "# params = {\"niveau_log\": 'CRITICAL'}\n",
    "reconfigurer_logging(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f629a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import logging\n",
    "\n",
    "\n",
    "\n",
    "# def preparer_donnees(df, params):\n",
    "#     \"\"\"\n",
    "#     Prépare les données pour la modélisation en filtrant les dates, extrayant les composants temporels,\n",
    "#     et en appliquant un encodage one-hot sur les données temporelles.\n",
    "\n",
    "#     :param df: DataFrame contenant les données à préparer.\n",
    "#     :return: DataFrame préparé avec les dummies des composants temporels.\n",
    "#     \"\"\"\n",
    "#     df['date'] = pd.to_datetime(df['date'])\n",
    "#     if filtre:\n",
    "#         # Filtrage des données entre le 1er juin et le 30 septembre de chaque année\n",
    "#         df = df[df['date'].dt.month.isin([6, 7, 8, 9])]\n",
    "#         # Filtrage pour les heures de 8h à 18h inclus\n",
    "#         df = df[(df['date'].dt.hour >= 8) & (df['date'].dt.hour <= 18)]\n",
    "\n",
    "\n",
    "#     # Extraction des composants temporels\n",
    "#     df['mois'] = df['date'].dt.month\n",
    "#     df['heure'] = df['date'].dt.hour\n",
    "#     df['minute'] = df['date'].dt.minute\n",
    "\n",
    "#     # Suppression des colonnes inutiles pour la modélisation\n",
    "#     df = df.drop(['station', 'date', 'précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], axis=1)\n",
    "#     logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "#     # Application des dummies\n",
    "#     df = pd.get_dummies(df, columns=['mois', 'heure', 'minute'])\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "\n",
    "\n",
    "# def prediction_vent_avec_filtre(df, params):\n",
    "#     resultats=[]\n",
    "#     # Préparation des données\n",
    "#     df_prepared = preparer_donnees(df, filtre=filtre_horaire)\n",
    "#     df_prepared.index = pd.to_datetime(df_prepared.index)\n",
    "\n",
    "\n",
    "#     # Division des données (assurez-vous que la colonne 'vitesse_vent_(km/h)' est bien dans votre df_prepared)\n",
    "#     train_data, val_data, test_data = split_time_series(df_prepared, train_size=0.7, val_size=0.25, date_column='index')\n",
    "\n",
    "#     # Modèle de régression linéaire\n",
    "#     X_train = train_data.drop(['vitesse_vent_(km/h)'], axis=1)\n",
    "#     y_train = train_data['vitesse_vent_(km/h)']\n",
    "\n",
    "\n",
    "#     model = LinearRegression()\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     # Prédiction\n",
    "#     X_test = test_data.drop(['vitesse_vent_(km/h)'], axis=1)\n",
    "#     y_test = test_data['vitesse_vent_(km/h)']\n",
    "#     predictions = model.predict(X_test)\n",
    "\n",
    "#     # Calcul du MSE\n",
    "#     mse = mean_squared_error(y_test, predictions)\n",
    "#     logger.info(f\"MSE sur l'ensemble de test: {round(mse, 2)}(km/h)²\")\n",
    "#     # Calcul du RMSE\n",
    "#     # Le MSE est exprimé dans le carré de l'unité de la variable cible, on calcul sa racine carré\n",
    "#     rmse = np.sqrt(mse)\n",
    "#     resultats.append(rmse)\n",
    "#     logger.info(f\"RMSE sur l'ensemble de test: {round(rmse,2)}km/h\")\n",
    "\n",
    "#     # Tracé\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(y_test.values, label='Valeurs réelles')\n",
    "#     plt.plot(predictions, color='red', label='Prédictions')\n",
    "#     plt.title('Prédictions vs Valeurs Réelles de la Vitesse du Vent')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     return resultats\n",
    "\n",
    "# # Resultat en prenant les mois heures minute du 1juin au 30 septembre\n",
    "# # RMSE sur l'ensemble de test: 3.78km/h\n",
    "\n",
    "# # Resultat en prenant les mois heures minute du 1juin au 30 septembre et en filtrant les données du jour de 8h à 18h\n",
    "# # RMSE sur l'ensemble de test: 3.84km/h\n",
    "\n",
    "# # Resultat en prenant les mois heures minute sur tout le dataset\n",
    "# # RMSE sur l'ensemble de test: 7.48km/h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7b16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import logging\n",
    "\n",
    "\n",
    "\n",
    "# def preparer_donnees(df, params):\n",
    "#     \"\"\"\n",
    "#     Prépare les données pour la modélisation en filtrant les dates, extrayant les composants temporels,\n",
    "#     et en appliquant un encodage one-hot sur les données temporelles.\n",
    "\n",
    "#     :param df: DataFrame contenant les données à préparer.\n",
    "#     :param params: Dictionnaire contenant les paramètres de filtre.\n",
    "#     :return: DataFrame préparé avec les dummies des composants temporels.\n",
    "#     \"\"\"\n",
    "#     df['date'] = pd.to_datetime(df['date'])\n",
    "#     # Filtrage mensuel\n",
    "#     if 'filtre_mensuel' in params:\n",
    "#         df = df[df['date'].dt.month.isin(params['filtre_mensuel'])]\n",
    "\n",
    "#     # Filtrage horaire\n",
    "#     if 'filtre_horaire' in params:\n",
    "#         plage_horaire = params['filtre_horaire']\n",
    "#         df = df[(df['date'].dt.hour >= plage_horaire[0]) & (df['date'].dt.hour <= plage_horaire[1])]\n",
    "\n",
    "#     # Suppression des colonnes inutiles\n",
    "#     if 'colonnes_to_drop' in params:\n",
    "#         df = df.drop(params['colonnes_to_drop'], axis=1)\n",
    "\n",
    "#     # Extraction et dummification des composants temporels\n",
    "#         #creation des colonnes\n",
    "#     df['mois'] = df['date'].dt.month\n",
    "#     df['heure'] = df['date'].dt.hour\n",
    "#     df['minute'] = df['date'].dt.minute\n",
    "#     df = pd.get_dummies(df, columns=['mois', 'heure', 'minute'])\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def prediction_vent_avec_filtre(df, params):\n",
    "#     resultats=[]\n",
    "#     # Préparation des données\n",
    "#     df_prepared = preparer_donnees(df, params)\n",
    "#     df_prepared.index = pd.to_datetime(df_prepared.index)\n",
    "\n",
    "\n",
    "#     # Division des données (assurez-vous que la colonne 'vitesse_vent_(km/h)' est bien dans votre df_prepared)\n",
    "#     train_data, val_data, test_data = split_time_series(df_prepared, train_size=0.7, val_size=0.25, date_column='index')\n",
    "\n",
    "#     # Modèle de régression linéaire\n",
    "#     X_train = train_data.drop(['vitesse_vent_(km/h)'], axis=1)\n",
    "#     y_train = train_data['vitesse_vent_(km/h)']\n",
    "\n",
    "\n",
    "#     model = LinearRegression()\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     # Prédiction\n",
    "#     X_test = test_data.drop(['vitesse_vent_(km/h)'], axis=1)\n",
    "#     y_test = test_data['vitesse_vent_(km/h)']\n",
    "#     predictions = model.predict(X_test)\n",
    "\n",
    "#     # Calcul du MSE\n",
    "#     mse = mean_squared_error(y_test, predictions)\n",
    "#     logger.info(f\"MSE sur l'ensemble de test: {round(mse, 2)}(km/h)²\")\n",
    "#     # Calcul du RMSE\n",
    "#     # Le MSE est exprimé dans le carré de l'unité de la variable cible, on calcul sa racine carré\n",
    "#     rmse = np.sqrt(mse)\n",
    "#     # Enregistrement des résultats avec détails des filtres\n",
    "#     details_filtres = f\"Mois: {params['filtre_mensuel']}, Heures: {params.get('filtre_horaire', 'Toutes')}, Colonnes supprimées: {params.get('colonnes_to_drop', 'Aucune')}\"\n",
    "#     resultats.append((rmse, details_filtres))\n",
    "\n",
    "#     # Affichage des résultats\n",
    "#     print(f\"RMSE: {rmse} - Filtres: {details_filtres}\")\n",
    "\n",
    "#     # Tracé\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(y_test.values, label='Valeurs réelles')\n",
    "#     plt.plot(predictions, color='red', label='Prédictions')\n",
    "#     plt.title('Prédictions vs Valeurs Réelles de la Vitesse du Vent')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     return resultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8544539",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "def preparer_donnees(df, filtre_mensuel, filtre_horaire, colonnes_to_drop):\n",
    "    \"\"\"\n",
    "    Prépare les données pour la modélisation en filtrant les dates, extrayant les composants temporels,\n",
    "    et en appliquant un encodage one-hot sur les données temporelles.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame contenant les données à préparer.\n",
    "        filtre_mensuel: Liste des mois à inclure dans l'analyse.\n",
    "        filtre_horaire: Tuple contenant les heures de début et de fin à inclure dans l'analyse.\n",
    "        colonnes_to_drop: Liste des noms des colonnes à supprimer du DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame préparé avec les dummies des composants temporels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df = df[df['date'].dt.month.isin(filtre_mensuel)]\n",
    "        df = df[(df['date'].dt.hour >= filtre_horaire[0]) & (df['date'].dt.hour <= filtre_horaire[1])]\n",
    "\n",
    "        df['mois'] = df['date'].dt.month\n",
    "        df['heure'] = df['date'].dt.hour\n",
    "        df['minute'] = df['date'].dt.minute\n",
    "\n",
    "        # Suppression des colonnes inutiles pour la modélisation\n",
    "        colonnes_a_supprimer = ['station', 'date'] + colonnes_to_drop\n",
    "        for col in colonnes_a_supprimer:\n",
    "            if col in df.columns:\n",
    "                df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "        df = pd.get_dummies(df, columns=['mois', 'heure', 'minute'])\n",
    "        # #suppression des colonnes, en plus de la colonne date\n",
    "        # df = df.drop( colonnes_to_drop, axis=1, errors='ignore')\n",
    "        # # df = df.drop( colonnes_to_drop, axis=1)\n",
    "        # df = df.drop(['station', 'date'], axis=1)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de la préparation des données: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def prediction_vent_avec_filtre(df_original, params):\n",
    "\n",
    "    i=0\n",
    "    resultats = []\n",
    "    colonne_to_predict=params['colonne_to_predict']\n",
    "    plages_mensuelles = params[\"plages_mensuelles\"]\n",
    "    plages_horaires = params[\"plages_horaires\"]\n",
    "    colonnes_possibles_a_supprimer = params[\"colonnes_possibles_a_supprimer\"]\n",
    "    logger.debug(f\"\\n plages_mensuelles:\\n{plages_mensuelles} \")\n",
    "    logger.debug(f\"\\n plages_horaires:\\n{plages_horaires} \")\n",
    "    logger.debug(f\"\\n colonnes_possibles_a_supprimer:\\n{colonnes_possibles_a_supprimer} \")\n",
    "\n",
    "    #test la presence de la colonne à predire dans le df\n",
    "    if colonne_to_predict not in df_original.columns.tolist():\n",
    "        return\n",
    "    # unite_colonne_to_predict =[colonne[colonne.find('('):] if '(' in colonne else '' for colonne in [colonne_to_predict]]\n",
    "    unite_colonne_to_predict = colonne_to_predict[colonne_to_predict.find('('):] if '(' in colonne_to_predict else ''\n",
    "    logger.debug(f\"\\n unite_colonne_to_predict:\\n{unite_colonne_to_predict} \")\n",
    "\n",
    "    for filtre_mensuel in plages_mensuelles:\n",
    "        for filtre_horaire in plages_horaires:\n",
    "            for colonnes_to_drop in colonnes_possibles_a_supprimer:\n",
    "                df = df_original.copy()\n",
    "                logger.debug(f\"\\n filtre_mensuel:\\n{filtre_mensuel} \")\n",
    "                logger.debug(f\"\\n filtre_horaire:\\n{filtre_horaire} \")\n",
    "                logger.debug(f\"\\n colonnes_to_drop:\\n{colonnes_to_drop} \")\n",
    "                logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "                df_prepared = preparer_donnees(df, filtre_mensuel, filtre_horaire, colonnes_to_drop)\n",
    "                logger.debug(f\"\\n liste colonnes:\\n{df_prepared.columns.tolist()} \")\n",
    "                # Appliquer split_time_series, entraîner le modèle, calculer et enregistrer le RMSE\n",
    "                # Préparation des données\n",
    "                # df_prepared = preparer_donnees(df, params)\n",
    "                df_prepared.index = pd.to_datetime(df_prepared.index)\n",
    "                logger.debug(f\"\\n liste colonnes:\\n{df_prepared.columns.tolist()} \")\n",
    "                # Division des données (assurez-vous que la colonne 'vitesse_vent_(km/h)' est bien dans votre df_prepared)\n",
    "                train_data, val_data, test_data = split_time_series(df_prepared, train_size=0.7, val_size=0.25, date_column='index')\n",
    "                logger.debug(f\"\\n train_data:\\n{train_data.head(2)} \")\n",
    "                logger.debug(f\"\\n test_data:\\n{test_data.head(2)} \")\n",
    "\n",
    "                # Modèle de régression linéaire\n",
    "                X_train = train_data.drop([colonne_to_predict], axis=1)\n",
    "                y_train = train_data[colonne_to_predict]\n",
    "\n",
    "                model = LinearRegression()\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Prédiction\n",
    "                X_test = test_data.drop([colonne_to_predict], axis=1)\n",
    "                y_test = test_data[colonne_to_predict]\n",
    "                predictions = model.predict(X_test)\n",
    "\n",
    "                # Calcul du MSE\n",
    "                mse = mean_squared_error(y_test, predictions)\n",
    "                logger.info(f\"MSE sur l'ensemble de test: {round(mse, 2)}(km/h)²\")\n",
    "                # Calcul du RMSE\n",
    "                # Le MSE est exprimé dans le carré de l'unité de la variable cible, on calcul sa racine carré\n",
    "                rmse = np.sqrt(mse)\n",
    "                # Enregistrement des résultats avec détails des filtres\n",
    "                df_name = params['df_name']\n",
    "                details_filtres = f\"test:{i}, {df_name}, Unité:{unite_colonne_to_predict}  Mois: {filtre_mensuel}, Heures: {filtre_horaire}, Colonnes supprimées: {colonnes_to_drop}\"\n",
    "                resultat = ((round(rmse, 3), details_filtres))\n",
    "                resultats.append(resultat)\n",
    "                i+=1\n",
    "                # Affichage des résultats\n",
    "                print(f\"RMSE: {round(rmse,3)}_{unite_colonne_to_predict} - Filtres: {details_filtres}\")\n",
    "\n",
    "                # Tracé\n",
    "                # plt.figure(figsize=(10, 6))\n",
    "                # plt.plot(y_test.values, label='Valeurs réelles')\n",
    "                # plt.plot(predictions, color='red', label='Prédictions')\n",
    "                # plt.title(f'Prédictions vs Valeurs Réelles de la {colonne_to_predict}')\n",
    "                # plt.legend()\n",
    "                # plt.show()\n",
    "    meilleur_rmse = min(resultats, key=lambda x: x[0])\n",
    "    print(f\"Meilleur RMSE: {meilleur_rmse[0]} - Filtres: {meilleur_rmse[1]}\")\n",
    "    return resultats\n",
    "    # Trouver et afficher la configuration avec le RMSE le plus bas\n",
    "    # meilleur_resultat = min(resultats, key=lambda x: x[0])\n",
    "    # logger.info(f\"Meilleur RMSE: {meilleur_resultat[0]} avec la configuration: {meilleur_resultat[1]}\")\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# Assurez-vous que df est défini et que les fonctions nécessaires sont bien implémentées\n",
    "# prediction_vent_avec_filtre(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879647aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframes_meteo_france_6_min_clean_sans_outlier[\"df_station_13005003\"]\n",
    "df = df.copy()\n",
    "logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "params = {\n",
    "    \"colonne_to_predict\":'vitesse_vent_(km/h)',\n",
    "    \"df_name\":df_name,\n",
    "    \"plages_mensuelles\": [[6, 7, 8, 9], [7, 8], [5, 6, 7, 8, 9, 10], [4, 5, 6, 7, 8, 9, 10]],\n",
    "    \"plages_horaires\": [[8, 18], [7, 19], [9, 17], [10, 16], [6, 20], [0,23]],\n",
    "    \"colonnes_possibles_a_supprimer\": [\n",
    "        ['précipitations_(mm)'],\n",
    "        ['direction_(°)'],\n",
    "        ['humidity_(%)'],\n",
    "        ['temperature_(°C)'],\n",
    "        ['précipitations_(mm)', 'direction_(°)'],\n",
    "        ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'],\n",
    "        []  # Aucune colonne supprimée\n",
    "    ],\n",
    "}\n",
    "# resultats=prediction_vent_avec_filtre(df, params)\n",
    "\n",
    "# score_regression_lineaire_df_station_13005003=resultats\n",
    "# RMSE: 3.76_(km/h) - Filtres: test:0, df_station_13111002, Unité:(km/h)  Mois: [6, 7, 8, 9], Heures: [8, 18], Colonnes supprimées: ['précipitations_(mm)']\n",
    "# RMSE: 3.736_(km/h) - Filtres: test:1, df_station_13111002, Unité:(km/h)  Mois: [6, 7, 8, 9], Heures: [8, 18], Colonnes supprimées: ['direction_(°)']\n",
    "# RMSE: 3.707_(km/h) - Filtres: test:2, df_station_13111002, Unité:(km/h)  Mois: [6, 7, 8, 9], Heures: [8, 18], Colonnes supprimées: ['humidity_(%)']\n",
    "# RMSE: 4.023_(km/h) - Filtres: test:3, df_station_13111002, Unité:(km/h)  Mois: [6, 7, 8, 9], Heures: [8, 18], Colonnes supprimées: ['temperature_(°C)']\n",
    "# RMSE: 3.741_(km/h) - Filtres: test:4, df_station_13111002, Unité:(km/h)  Mois: [6, 7, 8, 9], Heures: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)']\n",
    "# RMSE: 3.837_(km/h) - Filtres: test:5, df_station_13111002, Unité:(km/h)  Mois: [6, 7, 8, 9], Heures: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)']\n",
    "# RMSE: 3.756_(km/h) - Filtres: test:6, df_station_13111002, Unité:(km/h)  Mois: [6, 7, 8, 9], Heures: [8, 18], Colonnes supprimées: []\n",
    "# RMSE: 3.817_(km/h) - Filtres: test:7, df_station_13111002, Unité:(km/h)  Mois: [6, 7, 8, 9], Heures: [7, 19], Colonnes supprimées: ['précipitations_(mm)']\n",
    "# RMSE: 3.79_(km/h) - Filtres: test:8, df_station_13111002, Unité:(km/h)  Mois: [6, 7, 8, 9], Heures: [7, 19], Colonnes supprimées: ['direction_(°)']\n",
    "# RMSE: 3.748_(km/h) - Filtres: test:9, df_station_13111002, Unité:(km/h)  Mois: [6, 7, 8, 9], Heures: [7, 19], Colonnes supprimées: ['humidity_(%)']\n",
    "# RMSE: 4.036_(km/h) - Filtres: test:10, df_station_13111002, Unité:(km/h)  Mois: [6, 7, 8, 9], Heures: [7, 19], Colonnes supprimées: ['temperature_(°C)']\n",
    "# RMSE: 3.793_(km/h) - Filtres: test:11, df_station_13111002, Unité:(km/h)  Mois: [6, 7, 8, 9], Heures: [7, 19], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)']\n",
    "# RMSE: 3.879_(km/h) - Filtres: test:12, df_station_13111002, Unité:(km/h)  Mois: [6, 7, 8, 9], Heures: [7, 19], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)']\n",
    "# RMSE: 3.814_(km/h) - Filtres: test:13, df_station_13111002, Unité:(km/h)  Mois: [6, 7, 8, 9], Heures: [7, 19], Colonnes supprimées: []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894afb21",
   "metadata": {},
   "source": [
    "### boucle sur tous les datasets meteo-france au pas de  6 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4ffe75",
   "metadata": {},
   "source": [
    "##### aprroche mois heure minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# import logging\n",
    "\n",
    "\n",
    "# def preparer_donnees(df, filtre_mensuel, filtre_horaire, colonnes_to_drop):\n",
    "#     \"\"\"\n",
    "#     Prépare les données pour la modélisation en filtrant les dates, extrayant les composants temporels,\n",
    "#     et en appliquant un encodage one-hot sur les données temporelles.\n",
    "\n",
    "#     Args:\n",
    "#         df: DataFrame contenant les données à préparer.\n",
    "#         filtre_mensuel: Liste des mois à inclure dans l'analyse.\n",
    "#         filtre_horaire: Tuple contenant les heures de début et de fin à inclure dans l'analyse.\n",
    "#         colonnes_to_drop: Liste des noms des colonnes à supprimer du DataFrame.\n",
    "\n",
    "#     Returns:\n",
    "#         DataFrame préparé avec les dummies des composants temporels.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         df['date'] = pd.to_datetime(df['date'])\n",
    "#         df = df[df['date'].dt.month.isin(filtre_mensuel)]\n",
    "#         df = df[(df['date'].dt.hour >= filtre_horaire[0]) & (df['date'].dt.hour <= filtre_horaire[1])]\n",
    "\n",
    "#         df['mois'] = df['date'].dt.month\n",
    "#         df['heure'] = df['date'].dt.hour\n",
    "#         df['minute'] = df['date'].dt.minute\n",
    "\n",
    "#         # Suppression des colonnes inutiles pour la modélisation\n",
    "#         colonnes_a_supprimer = ['station', 'date'] + colonnes_to_drop\n",
    "#         for col in colonnes_a_supprimer:\n",
    "#             if col in df.columns:\n",
    "#                 df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "#         df = pd.get_dummies(df, columns=['mois', 'heure', 'minute'])\n",
    "#         # #suppression des colonnes, en plus de la colonne date\n",
    "#         # df = df.drop( colonnes_to_drop, axis=1, errors='ignore')\n",
    "#         # # df = df.drop( colonnes_to_drop, axis=1)\n",
    "#         # df = df.drop(['station', 'date'], axis=1)\n",
    "#         return df\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Erreur lors de la préparation des données: {e}\")\n",
    "#         return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b833fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# import logging\n",
    "\n",
    "# def prediction_vent_avec_filtre(df_original, params):\n",
    "#     resultats = []\n",
    "#     colonne_to_predict = params['colonne_to_predict']\n",
    "#     plages_mensuelles = params[\"plages_mensuelles\"]\n",
    "#     plages_horaires = params[\"plages_horaires\"]\n",
    "#     colonnes_possibles_a_supprimer = params[\"colonnes_possibles_a_supprimer\"]\n",
    "#     df_name = params['df_name']\n",
    "#     i=0\n",
    "#     if colonne_to_predict not in df_original.columns:\n",
    "#         logger.error(f\"La colonne à prédire '{colonne_to_predict}' n'est pas présente dans le DataFrame.\")\n",
    "#         return []\n",
    "#     unite_colonne_to_predict = colonne_to_predict[colonne_to_predict.find('('):] if '(' in colonne_to_predict else ''\n",
    "\n",
    "#     for filtre_mensuel in plages_mensuelles:\n",
    "#         for filtre_horaire in plages_horaires:\n",
    "#             for colonnes_to_drop in colonnes_possibles_a_supprimer:\n",
    "#                 df = df_original.copy()\n",
    "#                 df_prepared = preparer_donnees(df, filtre_mensuel, filtre_horaire, colonnes_to_drop)\n",
    "#                 df_prepared.index = pd.to_datetime(df_prepared.index)\n",
    "\n",
    "#                 # Supposons que split_time_series est correctement implémenté\n",
    "#                 train_data, val_data, test_data = split_time_series(df_prepared, train_size=0.7, val_size=0.25, date_column='index')\n",
    "\n",
    "#                 X_train = train_data.drop([colonne_to_predict], axis=1)\n",
    "#                 y_train = train_data[colonne_to_predict]\n",
    "#                 X_test = test_data.drop([colonne_to_predict], axis=1)\n",
    "#                 y_test = test_data[colonne_to_predict]\n",
    "\n",
    "#                 model = LinearRegression()\n",
    "#                 model.fit(X_train, y_train)\n",
    "#                 predictions = model.predict(X_test)\n",
    "\n",
    "#                 mse = mean_squared_error(y_test, predictions)\n",
    "#                 rmse = np.sqrt(mse)\n",
    "#                 mae = mean_absolute_error(y_test, predictions)\n",
    "#                 r2 = r2_score(y_test, predictions)\n",
    "\n",
    "#                 details_filtres = f\"test:{i}, {df_name}, Unité:{unite_colonne_to_predict} - Mois: {filtre_mensuel}, Heures: {filtre_horaire}, Colonnes supprimées: {colonnes_to_drop}\"\n",
    "#                 resultats.append((rmse, mae, r2, details_filtres))\n",
    "#                 i+=1\n",
    "#                 logger.debug(f\"RMSE: {rmse}{unite_colonne_to_predict}, MAE: {mae}, R2: {r2} - Filtres: {details_filtres}\")\n",
    "\n",
    "#     meilleur_rmse = min(resultats, key=lambda x: x[0])\n",
    "#     logger.info(f\"Meilleur RMSE: {meilleur_rmse[0]} - Filtres: {meilleur_rmse[3]}\")\n",
    "#     return resultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cc9c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture\n",
    "# # df = dataframes_meteo_france_6_min_clean_sans_outlier[\"df_station_13005003\"]\n",
    "# liste_resultats=[]\n",
    "\n",
    "# for df_name, df in dataframes_meteo_france_6_min_clean_sans_outlier.items():\n",
    "#     df=dataframes_meteo_france_6_min_clean_sans_outlier[df_name]\n",
    "#     df = df.copy()\n",
    "#     logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "#     params = {\n",
    "#         \"heure_de_reference\": 9,  # 9 heure de la journée pour la prédiction\n",
    "#         \"decalages_en_minutes\": [40, 120, 180],  # 40 min, 2h, 3h\n",
    "#         \"colonne_to_predict\": 'vitesse_vent_(km/h)',\n",
    "#         \"df_name\": df_name,\n",
    "#         \"plages_mensuelles\": [[6, 7, 8, 9], [7, 8], [5, 6, 7, 8, 9, 10], [4, 5, 6, 7, 8, 9, 10]],\n",
    "#         \"plages_horaires\": [[8, 18], [7, 19], [9, 17], [10, 16], [6, 20], [0, 23]],\n",
    "#         \"colonnes_possibles_a_supprimer\": [\n",
    "#             ['précipitations_(mm)'],\n",
    "#             ['direction_(°)'],\n",
    "#             ['humidity_(%)'],\n",
    "#             ['temperature_(°C)'],\n",
    "#             ['précipitations_(mm)', 'direction_(°)'],\n",
    "#             ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'],\n",
    "#             []  # Aucune colonne supprimée\n",
    "#         ],\n",
    "#     }\n",
    "#     name_tableau_resultat=f'score_RMSE_regression_lineaire_{df_name}'\n",
    "#     resultats = prediction_vent_avec_filtre(df, params)\n",
    "#     name_tableau_resultat = resultats\n",
    "#     liste_resultats.append(name_tableau_resultat)\n",
    "# liste_resultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5187a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = dataframes_meteo_france_6_min_clean_sans_outlier[\"df_station_13005003\"]\n",
    "# df = df.copy()\n",
    "\n",
    "# #  paramètres\n",
    "# params = {\n",
    "#     \"filtre_mensuel\": [6, 7, 8, 9],\n",
    "#     \"filtre_horaire\": lambda df: (df['date'].dt.hour >= 8) & (df['date'].dt.hour <= 18),\n",
    "#     \"colonnes_to_drop\": ['station', 'date', 'précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)']\n",
    "# }\n",
    "\n",
    "\n",
    "# resultats=prediction_vent_avec_filtre(df, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf24df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # resultats\n",
    "# score_regression_lineaire_df_station_13005003 = resultats\n",
    "\n",
    "# meilleur_rmse = min(score_regression_lineaire_df_station_13005003, key=lambda x: x[0])\n",
    "# print(f\"Meilleur RMSE: {meilleur_rmse[0]} - Filtres: {meilleur_rmse[1]}\")\n",
    "# # Meilleur RMSE: 3.3128742573271173 - Filtres: Mois: [6, 7, 8, 9], Heures: [10, 16], Colonnes supprimées: []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f8ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste_resultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c939353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplatir la liste de listes en une seule liste de tuples, en ignorant les éléments None\n",
    "liste_resultats_aplatie = [resultat for sous_liste in liste_resultats if sous_liste is not None for resultat in sous_liste]\n",
    "\n",
    "meilleur_score = float('inf')\n",
    "meilleure_description = \"\"\n",
    "\n",
    "# Parcourir la liste aplaties de résultats pour trouver le meilleur score\n",
    "for score, description in liste_resultats_aplatie:\n",
    "    if score < meilleur_score:\n",
    "        meilleur_score = score\n",
    "        meilleure_description = description\n",
    "\n",
    "# Afficher le meilleur score et la description\n",
    "print(f\"meilleur score: {meilleur_score}, avec {meilleure_description}\")\n",
    "# meilleur score: 3.313, avec test:27, df_station_13005003, Unité:(km/h)  Mois: [6, 7, 8, 9], Heures: [10, 16], Colonnes supprimées: []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f85f2b",
   "metadata": {},
   "source": [
    "#### prevision avec décalage horaire,  obtenir  les previsions à 40 min 2h, 3h à partir de 9h le matin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e813c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "def preparer_donnees_pour_prediction_horaires(df, plage_mensuelle, plage_horaire, colonnes_to_drop, heure_de_reference, decalages_en_minutes,\n",
    "                                              colonne_to_predict):\n",
    "    \"\"\"\n",
    "    Ajoute des colonnes décalées pour les prédictions futures spécifiques.\n",
    "    \"\"\"\n",
    "    # colonne_to_predict = params['colonne_to_predict']\n",
    "    # heure_de_reference = params['heure_de_reference']\n",
    "    # decalages_en_minutes = params['decalages_en_minutes']\n",
    "    df = df.copy()\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "    df = df[df['date'].dt.month.isin(plage_mensuelle)]\n",
    "    df = df[(df['date'].dt.hour >= plage_horaire[0]) & (df['date'].dt.hour <= plage_horaire[1])]\n",
    "\n",
    "    df['mois'] = df['date'].dt.month\n",
    "    df['heure'] = df['date'].dt.hour\n",
    "    df['minute'] = df['date'].dt.minute\n",
    "\n",
    "\n",
    "    # df['timestamp'] = pd.to_datetime(df['date']) + pd.to_timedelta(df['heure'], unit='h') + pd.to_timedelta(df['minute'], unit='m')\n",
    "    # logger.debug(f\"\\n df['timestamp']:\\n{df['timestamp']} \")\n",
    "\n",
    "    for decalage in decalages_en_minutes:\n",
    "        # nom_colonne = f'vent_plus_{decalage}min'\n",
    "        nom_colonne =f'{colonne_to_predict}_plus_{decalage}min'\n",
    "        logger.debug(f\"\\n nom_colonne:\\n{nom_colonne} \")\n",
    "        df[nom_colonne] = df.groupby(df['date'].dt.date)[colonne_to_predict].shift(-decalage // 6)\n",
    "        logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "\n",
    "    # Filtrer pour une heure de référence spécifique pour la prédiction\n",
    "    df = df[df['date'].dt.hour == heure_de_reference]\n",
    "    logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "    logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "\n",
    "    # Suppression des colonnes inutiles pour la modélisation\n",
    "    colonnes_a_supprimer = ['station', 'date'] + colonnes_to_drop\n",
    "    for col in colonnes_a_supprimer:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "    df=df.dropna()\n",
    "    logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e8ed91",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from unittest import result\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import logging\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def prediction_vent_avec_filtre_et_horaires(df_original, params):\n",
    "\n",
    "\n",
    "    resultats = []\n",
    "    colonne_to_predict = params['colonne_to_predict']\n",
    "    plages_mensuelles = params[\"plages_mensuelles\"]\n",
    "    plages_horaires = params[\"plages_horaires\"]\n",
    "    colonnes_possibles_a_supprimer = params[\"colonnes_possibles_a_supprimer\"]\n",
    "    heure_de_reference = params[\"heure_de_reference\"]\n",
    "    decalages_en_minutes = params[\"decalages_en_minutes\"]\n",
    "    df_name = params.get(\"df_name\", \"DataFrame\")\n",
    "\n",
    "    # Extraction de l'unité de la colonne à prédire pour les étiquettes des graphiques et le RMSE\n",
    "    if colonne_to_predict not in df_original.columns:\n",
    "        logger.error(f\"La colonne à prédire '{colonne_to_predict}' n'est pas présente dans le DataFrame.\")\n",
    "        return []\n",
    "    # unite_colonne_to_predict = colonne_to_predict[colonne_to_predict.find('('):] if '(' in colonne_to_predict else ''\n",
    "    unite_colonne_to_predict = colonne_to_predict[colonne_to_predict.find('('):] if '(' in colonne_to_predict else ''\n",
    "\n",
    "\n",
    "\n",
    "    logger.debug(f\"\\n unite_colonne_to_predict:\\n{unite_colonne_to_predict} \")\n",
    "    logger.debug(f\"\\n liste colonnes:\\n{df_original.columns.tolist()} \")\n",
    "    for plage_mensuelle in plages_mensuelles:\n",
    "        logger.debug(f\"\\n plage_mensuelle:\\n{plage_mensuelle} \")\n",
    "        for plage_horaire in plages_horaires:\n",
    "            logger.debug(f\"\\nplage_horaire :\\n{plage_horaire} \")\n",
    "            for colonnes_to_drop in colonnes_possibles_a_supprimer:\n",
    "                logger.debug(f\"\\n colonnes_to_drop:\\n{colonnes_to_drop} \")\n",
    "                df = df_original.copy()\n",
    "                df_prepared = preparer_donnees_pour_prediction_horaires(df, plage_mensuelle, plage_horaire, colonnes_to_drop, heure_de_reference,\n",
    "                                                                        decalages_en_minutes, colonne_to_predict)\n",
    "                df_prepared.index = pd.to_datetime(df_prepared.index)\n",
    "                logger.debug(f\"\\n liste colonnes:\\n{df_prepared.columns.tolist()} \")\n",
    "                if df_prepared.empty:\n",
    "                    continue  # Skip si le df préparé est vide\n",
    "\n",
    "                # Supposons que split_time_series est correctement implémenté\n",
    "                train_data, val_data, test_data = split_time_series(df_prepared, train_size=0.7, val_size=0.25, date_column='index')\n",
    "\n",
    "                for decalage in decalages_en_minutes:\n",
    "                    logger.debug(f\"\\n decalage:\\n{decalage} \")\n",
    "                    nom_colonne_cible = f'{colonne_to_predict}_plus_{decalage}min'\n",
    "                    # df[nom_colonne_cible] train_data[nom_colonne_cible] = train_data[colonne_to_predict].shift(-decalage)\n",
    "                    X_train = train_data.drop([nom_colonne_cible], axis=1, errors='ignore')\n",
    "                    y_train = train_data[nom_colonne_cible]\n",
    "                    X_test = test_data.drop([nom_colonne_cible], axis=1, errors='ignore')\n",
    "                    y_test = test_data[nom_colonne_cible]\n",
    "                    logger.debug(f\"\\n X_train:\\n{X_train} \")\n",
    "                    logger.debug(f\"\\n y_train:\\n{y_train} \")\n",
    "                    logger.debug(f\"\\n X_test:\\n{X_test} \")\n",
    "                    logger.debug(f\"\\n y_test:\\n{y_test} \")\n",
    "                    model = LinearRegression()\n",
    "                    model.fit(X_train, y_train)\n",
    "                    predictions = model.predict(X_test)\n",
    "\n",
    "                    mse = mean_squared_error(y_test, predictions)\n",
    "                    rmse = np.sqrt(mse)\n",
    "\n",
    "                    # Enregistrement des résultats avec détails des filtres\n",
    "                    details_filtres = f\"{df_name} - Mois: {plage_mensuelle}, Heures: {plage_horaire}, Colonnes supprimées: {colonnes_to_drop}, Prédiction pour: +{decalage}min, Unité: {unite_colonne_to_predict}\"\n",
    "                    resultats.append((rmse, details_filtres))\n",
    "\n",
    "                    # Log des résultats\n",
    "                    print(f\"RMSE: {rmse}{unite_colonne_to_predict} - Filtres: {details_filtres}\")\n",
    "    return resultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d7925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture\n",
    "# df = dataframes_meteo_france_6_min_clean_sans_outlier[\"df_station_13005003\"]\n",
    "liste_resultats = []\n",
    "\n",
    "for df_name, df in dataframes_meteo_france_6_min_clean_sans_outlier.items():\n",
    "    df = dataframes_meteo_france_6_min_clean_sans_outlier[df_name]\n",
    "    df = df.copy()\n",
    "    logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "    params = {\n",
    "        \"heure_de_reference\":\n",
    "        9,  # 9 heure de la journée pour la prédiction\n",
    "        \"decalages_en_minutes\": [40, 120, 180],  # 40 min, 2h, 3h\n",
    "        \"colonne_to_predict\":        'vitesse_vent_(km/h)',\n",
    "        # \"colonne_to_predict\":        'direction_(°)',\n",
    "        \"df_name\":\n",
    "        df_name,\n",
    "        \"plages_mensuelles\": [[6, 7, 8, 9], [7, 8], [5, 6, 7, 8, 9, 10], [4, 5, 6, 7, 8, 9, 10]],\n",
    "        \"plages_horaires\": [[8, 18], [7, 19], [9, 17], [10, 16], [6, 20], [0, 23]],\n",
    "        \"colonnes_possibles_a_supprimer\": [\n",
    "            ['précipitations_(mm)'],\n",
    "            ['direction_(°)'],\n",
    "            ['humidity_(%)'],\n",
    "            ['temperature_(°C)'],\n",
    "            ['précipitations_(mm)', 'direction_(°)'],\n",
    "            ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'],\n",
    "            []  # Aucune colonne supprimée\n",
    "        ],\n",
    "    }\n",
    "    name_tableau_resultat = f'score_RMSE_regression_lineaire_{df_name}'\n",
    "    resultats = prediction_vent_avec_filtre_et_horaires(df, params)\n",
    "    name_tableau_resultat = resultats\n",
    "    liste_resultats.append(name_tableau_resultat)\n",
    "\n",
    "# resultats_meteo_france_6min_all_stations_vent = liste_resultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste_resultats2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9fc705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "resultats_meteo_france_6min_all_stations_direction = liste_resultats2\n",
    "\n",
    "liste_resultats = resultats_meteo_france_6min_all_stations_direction\n",
    "# Initialisation d'un dictionnaire pour regrouper les RMSE par ensemble de paramètres\n",
    "rmse_par_ensemble = defaultdict(list)\n",
    "\n",
    "# Extraction et regroupement des RMSE par ensemble de paramètres\n",
    "for triplet_rmse in liste_resultats:\n",
    "    for rmse, details in triplet_rmse:\n",
    "        # Clé pour regrouper: retirer la partie variable des détails ('Prédiction pour')\n",
    "        cle_ensemble = details.split(\", Prédiction pour:\")[0]\n",
    "        rmse_par_ensemble[cle_ensemble].append(rmse)\n",
    "\n",
    "# Calculer un score agrégé (somme des RMSE) pour chaque ensemble\n",
    "scores_ensembles = {cle: sum(rmses) for cle, rmses in rmse_par_ensemble.items()}\n",
    "\n",
    "# Trouver l'ensemble de paramètres avec le score le plus bas\n",
    "meilleur_ensemble = min(scores_ensembles, key=scores_ensembles.get)\n",
    "meilleur_score = scores_ensembles[meilleur_ensemble]\n",
    "\n",
    "print(f\"# Meilleur ensemble de paramètres: {meilleur_ensemble}\")\n",
    "print(f\"# Score agrégé (somme des RMSE): {meilleur_score}\")\n",
    "\n",
    "\n",
    "# df_station_13022003\n",
    "# Meilleur ensemble de paramètres: df_station_13022003 - Mois: [5, 6, 7, 8, 9, 10], Heures: [6, 20], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)']\n",
    "# Score agrégé (somme des RMSE): 8.011629065679076\n",
    "# (RMSE:2.3156712228633487,\"df_station_13022003 - Mois: [5, 6, 7, 8, 9, 10], Heures: [6, 20], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Prédiction pour: +40min, Unité: (km/h)\"),\n",
    "# (RMSE: 2.869402454031047,\"df_station_13022003 - Mois: [5, 6, 7, 8, 9, 10], Heures: [6, 20], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Prédiction pour: +120min, Unité: (km/h)\"),\n",
    "# (RMSE:2.8265553887846804,\"df_station_13022003 - Mois: [5, 6, 7, 8, 9, 10], Heures: [6, 20], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Prédiction pour: +180min, Unité: (km/h)\"),\n",
    "# Meilleur ensemble de paramètres: df_station_13111002 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['précipitations_(mm)']\n",
    "# Score agrégé (somme des RMSE): 6.967227896155137\n",
    "\n",
    "# Meilleur ensemble de paramètres: df_station_13056002 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['temperature_(°C)']\n",
    "# Score agrégé (somme des RMSE): 57.83439305689105\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9516d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "\n",
    "resultats_meteo_france_6min_all_stations_direction = liste_resultats2\n",
    "\n",
    "liste_resultats = resultats_meteo_france_6min_all_stations_direction\n",
    "liste_resultats = resultats_meteo_france_6min_all_stations_vent\n",
    "# Initialisation d'un dictionnaire pour regrouper les RMSE par ensemble de paramètres\n",
    "rmse_par_ensemble = defaultdict(list)\n",
    "\n",
    "# Dictionnaire pour conserver les détails de chaque RMSE\n",
    "details_par_ensemble = defaultdict(list)\n",
    "\n",
    "for triplet_rmse in liste_resultats:\n",
    "    for rmse, details in triplet_rmse:\n",
    "        # Clé pour regrouper: retirer la partie variable des détails ('Prédiction pour')\n",
    "        cle_ensemble = details.split(\", Prédiction pour:\")[0]\n",
    "        rmse_par_ensemble[cle_ensemble].append(rmse)\n",
    "        details_par_ensemble[cle_ensemble].append((rmse, details))\n",
    "\n",
    "# Calculer un score agrégé (somme des RMSE) pour chaque ensemble\n",
    "scores_ensembles = {cle: sum(rmses) for cle, rmses in rmse_par_ensemble.items()}\n",
    "\n",
    "# Trouver les 3 ensembles de paramètres avec les scores les plus bas\n",
    "trois_meilleurs_ensembles = heapq.nsmallest(3, scores_ensembles, key=scores_ensembles.get)\n",
    "\n",
    "# Afficher les détails pour les trois meilleurs ensembles\n",
    "for ensemble in trois_meilleurs_ensembles:\n",
    "    score = scores_ensembles[ensemble]\n",
    "    details = details_par_ensemble[ensemble]\n",
    "    print(f\"# Meilleur ensemble de paramètres: {ensemble}\")\n",
    "    print(f\"# Score agrégé (somme des RMSE): {score}\")\n",
    "    print(\"# Détails:\")\n",
    "    for rmse, detail in sorted(details, key=lambda x: x[0]):  # Trier par RMSE si nécessaire\n",
    "        print(f\"# (RMSE: {rmse}, \\\"{detail}\\\")\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# Meilleur ensemble de paramètres: df_station_13111002 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['précipitations_(mm)']\n",
    "# Score agrégé (somme des RMSE): 6.967227896155137\n",
    "# Détails:\n",
    "# (RMSE: 2.13942914431285, \"df_station_13111002 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['précipitations_(mm)'], Prédiction pour: +120min, Unité: (km/h)\")\n",
    "# (RMSE: 2.372933328710459, \"df_station_13111002 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['précipitations_(mm)'], Prédiction pour: +180min, Unité: (km/h)\")\n",
    "# (RMSE: 2.454865423131827, \"df_station_13111002 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['précipitations_(mm)'], Prédiction pour: +40min, Unité: (km/h)\")\n",
    "\n",
    "\n",
    "\n",
    "# Meilleur ensemble de paramètres: df_station_13056002 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['temperature_(°C)']\n",
    "# Score agrégé (somme des RMSE): 57.83439305689105\n",
    "# Détails:\n",
    "# (RMSE: 14.887744648698703, \"df_station_13056002 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Prédiction pour: +120min, Unité: (°)\")\n",
    "# (RMSE: 16.999117295010173, \"df_station_13056002 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Prédiction pour: +180min, Unité: (°)\")\n",
    "# (RMSE: 25.94753111318218, \"df_station_13056002 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Prédiction pour: +40min, Unité: (°)\")\n",
    "\n",
    "# Meilleur ensemble de paramètres: df_station_13022003 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['humidity_(%)']\n",
    "# Score agrégé (somme des RMSE): 58.71523744842912\n",
    "# Détails:\n",
    "# (RMSE: 17.790243566286055, \"df_station_13022003 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['humidity_(%)'], Prédiction pour: +40min, Unité: (°)\")\n",
    "# (RMSE: 19.248990507055584, \"df_station_13022003 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['humidity_(%)'], Prédiction pour: +120min, Unité: (°)\")\n",
    "# (RMSE: 21.676003375087486, \"df_station_13022003 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['humidity_(%)'], Prédiction pour: +180min, Unité: (°)\")\n",
    "\n",
    "# Meilleur ensemble de paramètres: df_station_13022003 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['temperature_(°C)']\n",
    "# Score agrégé (somme des RMSE): 59.108745180276884\n",
    "# Détails:\n",
    "# (RMSE: 18.299432848133478, \"df_station_13022003 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Prédiction pour: +40min, Unité: (°)\")\n",
    "# (RMSE: 19.518263313564685, \"df_station_13022003 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Prédiction pour: +120min, Unité: (°)\")\n",
    "# (RMSE: 21.291049018578715, \"df_station_13022003 - Mois: [7, 8], Heures: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Prédiction pour: +180min, Unité: (°)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b73e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{datasets_dir}/SYNOP/{schema_filename}', 'r') as json_file:\n",
    "    schema = json.load(json_file)\n",
    "\n",
    "synop_codes = list(schema['definitions']['donnees-synop-essentielles-omm_records']['properties']['fields']['properties'].keys())\n",
    "# ---- Get the columns name as descriptions\n",
    "#\n",
    "synop_desc = list(df.columns)\n",
    "\n",
    "# ---- Set Codes as columns name\n",
    "#\n",
    "df.columns = synop_codes\n",
    "code2desc = dict(zip(synop_codes, synop_desc))\n",
    "\n",
    "# ---- Count the na values by columns\n",
    "#\n",
    "columns_na = df.isna().sum().tolist()\n",
    "\n",
    "# ---- Show all of that\n",
    "#\n",
    "df_desc = pd.DataFrame({'Code': synop_codes, 'Description': synop_desc, 'Na': columns_na})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0daa75",
   "metadata": {},
   "source": [
    "### Version intégrant d'autres modeles: RandomForestRegressor et XGboost, KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077c2f90",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import logging\n",
    "\n",
    "# def preparer_donnees_pour_prediction_horaires(df, plage_mensuelle, plage_horaire, colonnes_to_drop, heure_de_reference, decalages_en_minutes,\n",
    "#                                               colonne_to_predict):\n",
    "#     \"\"\"\n",
    "#     Ajoute des colonnes décalées pour les prédictions futures spécifiques.\n",
    "#     \"\"\"\n",
    "\n",
    "#     df = df.copy()\n",
    "\n",
    "#     df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "#     df = df[df['date'].dt.month.isin(plage_mensuelle)]\n",
    "#     df = df[(df['date'].dt.hour >= plage_horaire[0]) & (df['date'].dt.hour <= plage_horaire[1])]\n",
    "\n",
    "#     df['mois'] = df['date'].dt.month\n",
    "#     df['heure'] = df['date'].dt.hour\n",
    "#     df['minute'] = df['date'].dt.minute\n",
    "\n",
    "\n",
    "#     for decalage in decalages_en_minutes:\n",
    "#         # nom_colonne = f'vent_plus_{decalage}min'\n",
    "#         nom_colonne_decaled = f'{colonne_to_predict}_plus_{decalage}min'\n",
    "#         logger.debug(f\"\\n nom_colonne:\\n{nom_colonne_decaled} \")\n",
    "#         df[nom_colonne_decaled] = df.groupby(df['date'].dt.date)[colonne_to_predict].shift(-decalage // 6)\n",
    "#         logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "\n",
    "#     # Filtrer pour une heure de référence spécifique pour la prédiction\n",
    "#     df = df[df['date'].dt.hour == heure_de_reference]\n",
    "#     logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "#     logger.debug(f\"\\n heure_de_reference:\\n{heure_de_reference} \")\n",
    "\n",
    "#     # Suppression des colonnes inutiles pour la modélisation\n",
    "#     colonnes_a_supprimer = ['station', 'date'] + colonnes_to_drop\n",
    "#     for col in colonnes_a_supprimer:\n",
    "#         if col in df.columns:\n",
    "#             df.drop(col, axis=1, inplace=True)\n",
    "#     df = df.dropna()\n",
    "#     logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "#     return df, nom_colonne_decaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392612b1",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import logging\n",
    "\n",
    "# # Configuration du logger pour l'exemple\n",
    "# # logging.basicConfig(level=logging.DEBUG)\n",
    "# # logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# def preparer_donnees_pour_prediction_horaires(df, plage_mensuelle, plage_horaire, colonnes_to_drop, heure_de_reference, decalages_en_minutes,\n",
    "#                                               colonne_to_predict):\n",
    "#     \"\"\"\n",
    "#     Ajoute des colonnes décalées pour les prédictions futures spécifiques.\n",
    "#     \"\"\"\n",
    "#     df = df.copy()\n",
    "\n",
    "#     # df.index = pd.to_datetime(df.index, unit='s')  #unité en secondes\n",
    "#     # df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "#     # Vérifier si la colonne 'date' est de type datetime\n",
    "#     logger.debug(f\"\\n plage_mensuelle:\\n{plage_mensuelle} \")\n",
    "#     logger.debug(f\"\\n plage_horaire:\\n{plage_horaire} \")\n",
    "#     logger.debug(f\"\\n heure_de_reference:\\n{heure_de_reference} \")\n",
    "#     logger.debug(f\"\\n colonnes_to_drop:\\n{colonnes_to_drop} \")\n",
    "#     logger.debug(f\"\\n decalages_en_minutes:\\n{decalages_en_minutes} \")\n",
    "#     logger.debug(f\"\\n colonne_to_predict:\\n{colonne_to_predict} \")\n",
    "#     if not np.issubdtype(df['date'].dtype, np.datetime64):\n",
    "#         logger.debug(\"Conversion de la colonne 'date' en datetime.\")\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "#     else:\n",
    "#         logger.debug(\"La colonne 'date' est déjà de type datetime.\")\n",
    "\n",
    "#     df = df[df['date'].dt.month.isin(plage_mensuelle)]\n",
    "#     df = df[(df['date'].dt.hour >= plage_horaire[0]) & (df['date'].dt.hour <= plage_horaire[1])]\n",
    "#     logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "\n",
    "\n",
    "#     # noms_colonnes_decalees = []\n",
    "#     # Calculer le pas temporel moyen en minutes\n",
    "#     df.sort_values('date', inplace=True)\n",
    "#     pas_temporel_moyen = (df['date'].diff().dt.total_seconds().dropna().mean() / 60).round()\n",
    "#     logger.debug(f\"Pas temporel moyen : {pas_temporel_moyen} minutes.\")\n",
    "\n",
    "#     df['mois'] = df['date'].dt.month\n",
    "#     df['heure'] = df['date'].dt.hour\n",
    "#     df['minute'] = df['date'].dt.minute\n",
    "\n",
    "#     # for decalage in decalages_en_minutes:\n",
    "#     #     nom_colonne_decaled = f'{colonne_to_predict}_plus_{decalage}min'\n",
    "#     #     df[nom_colonne_decaled] = df.groupby(df['date'].dt.date)[colonne_to_predict].shift(-decalage // 6) # 6 pour un pas de 6 min\n",
    "#     # noms_colonnes_decalees.append(nom_colonne_decaled)\n",
    "#     for decalage in decalages_en_minutes:\n",
    "#         # Ajuster le décalage en fonction du pas temporel moyen\n",
    "#         decalage_en_pas = int(np.round(decalage / pas_temporel_moyen))\n",
    "#         nom_colonne_decaled = f'{colonne_to_predict}_plus_{decalage}min'\n",
    "#         df[nom_colonne_decaled] = df.groupby(df['date'].dt.date)[colonne_to_predict].shift(-decalage_en_pas)  # pour un pas pas_temporel_moyen\n",
    "\n",
    "\n",
    "#     df = df[df['date'].dt.hour == heure_de_reference]\n",
    "#     logger.debug(f\"\\n liste colonnes du df avant suppression des colonnes:\\n{df.columns.tolist()} \")\n",
    "#     logger.debug(f\"\\n df aprés decalage mais  avant suppression des colonnes:\\n{df.head(2)} \")\n",
    "\n",
    "#     colonnes_a_supprimer = ['station', 'date'] + colonnes_to_drop\n",
    "#     df.drop(columns=colonnes_a_supprimer, errors='ignore', inplace=True)\n",
    "#     df = df.dropna()\n",
    "#     logger.debug(f\"\\n liste colonnes du df aprés decalage aprés suppression des colonnes mais avant le split_times_series:\\n{df.columns.tolist()} \")\n",
    "#     logger.debug(f\"\\n df aprés decalage aprés suppression des colonnes mais avant le split_times_series:\\n{df.head(2)} \")\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0073c0",
   "metadata": {},
   "source": [
    "### fonction de preparation des df avec creation de nouvelles colonnes et plage horaire, utilisée dans (la ou les) fonction prediction prediction_vent_avec_filtre_et_horaires()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58612999",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import logging\n",
    "\n",
    "# # # Configuration du logger pour l'exemple\n",
    "# # logging.basicConfig(level=logging.DEBUG)\n",
    "# # logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# def preparer_donnees_pour_prediction_horaires(df, plage_mensuelle, plage_horaire, colonnes_to_drop, heure_de_reference, decalages_en_minutes,\n",
    "#                                               colonne_to_predict):\n",
    "#     \"\"\"\n",
    "#     Ajoute des colonnes décalées pour les prédictions futures spécifiques tout en conservant l'information temporelle.\n",
    "#     \"\"\"\n",
    "\n",
    "#     df = df.copy()\n",
    "#     logger.debug(f\"\\n liste colonnes du df avant suppression des colonnes:\\n{df.columns.tolist()} \")\n",
    "#     logger.debug(f\"\\n df avant decalage :\\n{df.head(2)} \")\n",
    "\n",
    "#     # Assurer que 'date' est au format datetime\n",
    "#     if not np.issubdtype(df['date'].dtype, np.datetime64):\n",
    "#         logger.debug(\"Conversion de la colonne 'date' en datetime.\")\n",
    "#         df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "#     else:\n",
    "#         logger.debug(\"La colonne 'date' est déjà de type datetime.\")\n",
    "\n",
    "#     # Filtrage selon les plages mensuelles et horaires\n",
    "#     df = df[df['date'].dt.month.isin(plage_mensuelle)]\n",
    "#     df = df[(df['date'].dt.hour >= plage_horaire[0]) & (df['date'].dt.hour <= plage_horaire[1])]\n",
    "\n",
    "#     df['mois'] = df['date'].dt.month\n",
    "#     df['heure'] = df['date'].dt.hour\n",
    "#     df['minute'] = df['date'].dt.minute\n",
    "#     logger.debug(f\"\\n liste colonnes du df avant suppression des colonnes:\\n{df.columns.tolist()} \")\n",
    "#     logger.debug(f\"\\n df avant decalage et avant suppression des colonnes:\\n{df.head(2)} \")\n",
    "#     # Définir 'date' comme index\n",
    "#     df.set_index('date', inplace=True)\n",
    "\n",
    "#     # Calcul du pas temporel moyen pour ajuster les décalages fonctionnne sur meteo france\n",
    "#     pas_temporel_moyen = (df.index.to_series().diff().dt.total_seconds().dropna().mean() / 60).round()\n",
    "#     logger.debug(f\"Pas temporel moyen : {pas_temporel_moyen} minutes.\")\n",
    "\n",
    "\n",
    "\n",
    "#     for decalage in decalages_en_minutes:\n",
    "#         decalage_en_pas = int(np.round(decalage / pas_temporel_moyen))\n",
    "#         nom_colonne_decaled = f'{colonne_to_predict}_plus_{decalage}min'\n",
    "#         df[nom_colonne_decaled] = df[colonne_to_predict].shift(-decalage_en_pas)\n",
    "\n",
    "#     # Filtrage selon l'heure de référence\n",
    "#     df = df[df.index.hour == heure_de_reference]\n",
    "\n",
    "#     logger.debug(f\"\\n liste colonnes du df avant suppression des colonnes:\\n{df.columns.tolist()} \")\n",
    "#     logger.debug(f\"\\n df aprés decalage mais avant suppression des colonnes:\\n{df.head(2)} \")\n",
    "#     # Suppression des colonnes non nécessaires, en conservant 'date' sous forme d'index\n",
    "#     colonnes_a_supprimer = set(['station'] + colonnes_to_drop)\n",
    "#     df.drop(columns=list(colonnes_a_supprimer.intersection(df.columns)), errors='ignore', inplace=True)\n",
    "\n",
    "#     df = df.dropna()\n",
    "#     logger.debug(f\"\\n liste colonnes du df aprés suppression des colonnes:\\n{df.columns.tolist()} \")\n",
    "#     logger.debug(f\"\\n df aprés decalage mais  aprés suppression des colonnes:\\n{df.head(2)} \")\n",
    "\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca3b718",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# # Configuration du logger pour l'exemple\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def preparer_donnees_pour_prediction_horaires(df, plage_mensuelle, plage_horaire, colonnes_to_drop, heure_de_reference, decalages_en_minutes,\n",
    "                                              colonne_to_predict):\n",
    "    \"\"\"\n",
    "    Ajoute des colonnes décalées pour les prédictions futures spécifiques tout en conservant l'information temporelle.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        logger.error(\"Le DataFrame est vide.\")\n",
    "        return df\n",
    "\n",
    "    df = df.copy()\n",
    "    logger.debug(f\"\\n liste colonnes du df avant suppression des colonnes:\\n{df.columns.tolist()} \")\n",
    "    logger.debug(f\"\\n df avant decalage :\\n{df.head(2)} \")\n",
    "\n",
    "\n",
    "\n",
    "    # Assurer que 'date' est au format datetime\n",
    "    if not np.issubdtype(df['date'].dtype, np.datetime64):\n",
    "        logger.debug(\"Conversion de la colonne 'date' en datetime.\")\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        logger.debug(\"La colonne 'date' est déjà de type datetime.\")\n",
    "\n",
    "    # Filtrage selon les plages mensuelles et horaires\n",
    "    df = df[df['date'].dt.month.isin(plage_mensuelle)]\n",
    "    df = df[(df['date'].dt.hour >= plage_horaire[0]) & (df['date'].dt.hour <= plage_horaire[1])]\n",
    "\n",
    "    df['mois'] = df['date'].dt.month\n",
    "    df['heure'] = df['date'].dt.hour\n",
    "    df['minute'] = df['date'].dt.minute\n",
    "    logger.debug(f\"\\n liste colonnes du df avant suppression des colonnes:\\n{df.columns.tolist()} \")\n",
    "    logger.debug(f\"\\n df avant decalage et avant suppression des colonnes:\\n{df.head(2)} \")\n",
    "    # Définir 'date' comme index\n",
    "    df.set_index('date', inplace=True)\n",
    "\n",
    "    # Calcul du pas temporel moyen pour ajuster les décalages fonctionnne sur meteo france\n",
    "    # pas_temporel_moyen = (df.index.to_series().diff().dt.total_seconds().dropna().mean() / 60).round()\n",
    "    # logger.debug(f\"Pas temporel moyen : {pas_temporel_moyen} minutes.\")\n",
    "\n",
    "\n",
    "    # Calcul du pas temporel moyen pour ajuster les décalages nouvelle version pour tenir compte de mobiiis\n",
    "    # Imputation par interpolation\n",
    "    df.interpolate(method='time', inplace=True)  # Utilise l'interpolation temporelle si approprié\n",
    "\n",
    "    pas_temporel_moyen = df.index.to_series().diff().dt.total_seconds().dropna().mean() / 60\n",
    "\n",
    "    if np.isnan(pas_temporel_moyen):  # Gère le cas où le pas temporel moyen n'est pas calculable\n",
    "        logger.warning(\"Pas temporel moyen calculé est NaN. Utilisation d'une valeur par défaut.\")\n",
    "        pas_temporel_moyen = 10 if 'mobilis' in df.index else 12  # Choisis une valeur par défaut basée sur la source\n",
    "\n",
    "    else:\n",
    "        pas_temporel_moyen = round(pas_temporel_moyen)\n",
    "\n",
    "    logger.debug(f\"Pas temporel moyen : {pas_temporel_moyen} minutes.\")\n",
    "    for decalage in decalages_en_minutes:\n",
    "        decalage_en_pas = int(np.round(decalage / pas_temporel_moyen))\n",
    "        nom_colonne_decaled = f'{colonne_to_predict}_plus_{decalage}min'\n",
    "        df[nom_colonne_decaled] = df[colonne_to_predict].shift(-decalage_en_pas)\n",
    "\n",
    "    # Filtrage selon l'heure de référence\n",
    "    df = df[df.index.hour == heure_de_reference]\n",
    "\n",
    "    logger.debug(f\"\\n liste colonnes du df avant suppression des colonnes:\\n{df.columns.tolist()} \")\n",
    "    logger.debug(f\"\\n df aprés decalage mais avant suppression des colonnes:\\n{df.head(2)} \")\n",
    "    # Suppression des colonnes non nécessaires, en conservant 'date' sous forme d'index\n",
    "    colonnes_a_supprimer = set(['station'] + colonnes_to_drop)\n",
    "    df.drop(columns=list(colonnes_a_supprimer.intersection(df.columns)), errors='ignore', inplace=True)\n",
    "\n",
    "    df = df.dropna()\n",
    "    logger.debug(f\"\\n liste colonnes du df aprés suppression des colonnes:\\n{df.columns.tolist()} \")\n",
    "    logger.debug(f\"\\n df aprés decalage mais  aprés suppression des colonnes:\\n{df.head(2)} \")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Configuration du logger pour le débogage\n",
    "# # logger = logging.getLogger(__name__)\n",
    "# # logging.basicConfig(level=logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea406ac8",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from torch import logdet\n",
    "# from xgboost import XGBRegressor\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import logging\n",
    "\n",
    "\n",
    "# def prediction_vent_avec_filtre_et_horaires(df_original, params):\n",
    "#     \"\"\"\n",
    "#     Effectuer des prédictions de vitesse et de direction du vent à plusieurs horizons temporels.\n",
    "\n",
    "#     :param df_original: DataFrame contenant les données météorologiques historiques.\n",
    "#     :type df_original: pd.DataFrame\n",
    "#     :param params: Dictionnaire contenant les paramètres de configuration pour le modèle et la prédiction.\n",
    "#     :type params: dict\n",
    "#     :return: Liste des résultats de prédiction, chaque élément est un dictionnaire avec les détails et le score RMSE.\n",
    "#     :rtype: list\n",
    "\n",
    "#     >>> df_test = pd.DataFrame(...)  # DataFrame exemple\n",
    "#     >>> params_test = {...}  # Paramètres exemple\n",
    "#     >>> resultats = prediction_vent_avec_filtre_et_horaires(df_test, params_test)\n",
    "#     \"\"\"\n",
    "#     resultats = []\n",
    "#     colonne_to_predict = params['colonne_to_predict']\n",
    "#     plages_mensuelles = params[\"plages_mensuelles\"]\n",
    "#     plages_horaires = params[\"plages_horaires\"]\n",
    "#     colonnes_possibles_a_supprimer = params[\"colonnes_possibles_a_supprimer\"]\n",
    "#     heure_de_reference = params[\"heure_de_reference\"]\n",
    "#     decalages_en_minutes = params[\"decalages_en_minutes\"]\n",
    "#     df_name = params.get(\"df_name\", \"DataFrame\")\n",
    "#     models_types = params.get('model_type', [LinearRegression])  # par defaut utilise la regression linéaire\n",
    "\n",
    "#     #Controle de la presence de la colonne à predire dans le dataset\n",
    "#     # Extraction de l'unité de la colonne à prédire pour les étiquettes des graphiques et le RMSE\n",
    "#     if colonne_to_predict not in df_original.columns:\n",
    "#         logger.error(f\"La colonne à prédire '{colonne_to_predict}' n'est pas présente dans le DataFrame.\")\n",
    "#         return []\n",
    "#     # unite_colonne_to_predict = colonne_to_predict[colonne_to_predict.find('('):] if '(' in colonne_to_predict else ''\n",
    "#     unite_colonne_to_predict = colonne_to_predict[colonne_to_predict.find('('):] if '(' in colonne_to_predict else ''\n",
    "#     # Logger les informations importantes\n",
    "#     logger.debug(f\"Début de la fonction de prédiction pour la colonne: {colonne_to_predict}\")\n",
    "#     i = 1\n",
    "#     logger.debug(f\"\\n test n°:\\n{i} \")\n",
    "#     for model_class in models_types:\n",
    "#         model_name = model_class.__name__\n",
    "#         logger.debug(f\"Évaluation du modèle : {model_name}\")\n",
    "\n",
    "#         # Sélection et configuration initiale du modèle\n",
    "#         # if model_class in [RandomForestRegressor, XGBRegressor, LinearRegression]:\n",
    "#         #     model = model_class(random_state=params.get(\"random_state\", 42))\n",
    "#         #     logger.debug(f\"\\n model_class:\\n{model_class} \")\n",
    "#         #########\n",
    "#         if model_class in [RandomForestRegressor, XGBRegressor]:\n",
    "#             model = model_class(random_state=params.get(\"random_state\", 42))\n",
    "#             logger.debug(f\"\\n model_class:\\n{model_class} \")\n",
    "#         elif model_class == LinearRegression:\n",
    "#             model = model_class()  # Pas de random_state pour LinearRegression\n",
    "#             logger.debug(f\"\\n model_class:\\n{model_class} \")\n",
    "#         else:\n",
    "#             logger.error(f\"Modèle {model_class} non géré par la configuration.\")\n",
    "#             return []\n",
    "\n",
    "#         ######### PREPARATION GRID_SEARCH ##############\n",
    "#         # Préparation du dictionnaire de paramètres pour GridSearchCV\n",
    "#         param_grid = {}\n",
    "#         hyperparametres = {}\n",
    "#         # if 'model_params' in params and model_name in params['model_params']:\n",
    "#         # if 'model_params' in params:\n",
    "#         #     for key, value in params['model_params'].items():\n",
    "#         #         logger.debug(f\"\\n key:\\n{key}  ------- value {value}\")\n",
    "#         #         if key.startswith(model_name.lower()):  # Adapter pour le modèle courant\n",
    "#         #             # Extraction du nom du paramètre sans le préfixe du modèle\n",
    "#         #             new_key = key.split('__')[1]\n",
    "#         #             logger.debug(f\"\\n new_key:\\n{new_key} \")\n",
    "#         #             param_grid[f'regressor__{new_key}'] = value\n",
    "#         #             hyperparametres[key] = value\n",
    "#         #             logger.debug(f\"\\n hyperparametres:\\n{hyperparametres} \")\n",
    "#         # else:\n",
    "#         #     logger.warning(f\"Aucun hyperparamètre spécifié pour {model_name}. Utilisation des paramètres par défaut.\")\n",
    "#         #     param_grid = {}\n",
    "\n",
    "\n",
    "#         ################\n",
    "#         if 'model_params' in params:\n",
    "#             model_params = params['model_params']\n",
    "#             for key, value in model_params.items():\n",
    "#                 # S'assurer que le modèle actuel a des paramètres spécifiés\n",
    "#                 if key.startswith(model_name):\n",
    "#                     # Adaptation du nom du paramètre pour GridSearchCV\n",
    "#                     param_name = key.replace(model_name + '__', '')\n",
    "#                     param_grid[f'model__{param_name}'] = value\n",
    "#                     hyperparametres[param_name] = value  # Stocker sans le préfixe 'model__'\n",
    "#         else:\n",
    "#             logger.warning(f\"Aucun hyperparamètre spécifié pour {model_name}. Utilisation des paramètres par défaut.\")\n",
    "#             param_grid = {}\n",
    "# ################\n",
    "# # Initialisation et configuration du modèle\n",
    "# # model = model_class(random_state=params.get(\"random_state\", 42))\n",
    "#         grid_search = GridSearchCV(Pipeline([('model', model)]),\n",
    "#                                    param_grid,\n",
    "#                                    cv=params.get('cv', 3),\n",
    "#                                    scoring=params.get('scoring', 'neg_mean_squared_error'),\n",
    "#                                    n_jobs=-1)\n",
    "#         logger.debug(f\"\\n model:\\n{model} \")\n",
    "#         logger.debug(f\"\\n grid_search:\\n{grid_search} \")\n",
    "\n",
    "#         ######### ITERATION SUR LES PLAGES MENSUELLES ############\n",
    "#         for plage_mensuelle in plages_mensuelles:\n",
    "#             logger.debug(f\"\\n plage_mensuelle:\\n{plage_mensuelle} \")\n",
    "#             for plage_horaire in plages_horaires:\n",
    "#                 logger.debug(f\"\\nplage_horaire :\\n{plage_horaire} \")\n",
    "#                 for colonnes_to_drop in colonnes_possibles_a_supprimer:\n",
    "#                     logger.debug(f\"\\n colonnes_to_drop:\\n{colonnes_to_drop} \")\n",
    "#                     df = df_original.copy()\n",
    "#                     ######### PREPARATION df ############\n",
    "#                     df_prepared = preparer_donnees_pour_prediction_horaires(df, plage_mensuelle, plage_horaire, colonnes_to_drop, heure_de_reference,\n",
    "#                                                                             decalages_en_minutes, colonne_to_predict)\n",
    "#                     logger.debug(f\"\\n df_prepared:\\n{df_prepared.head(2)} \")\n",
    "#                     logger.debug(f\"\\n liste colonnes:\\n{df_prepared.columns.tolist()} \")\n",
    "\n",
    "#                     ######### TRAIN TEST SPLIT ############\n",
    "#                     # df_prepared.index = pd.to_datetime(df_prepared.index, unit='us') # 'us' pour microsecondes\n",
    "#                     # df_prepared.set_index(nom_colonne_decaled, inplace=True)\n",
    "#                     # df_prepared.index = pd.to_datetime(df_prepared.index)\n",
    "\n",
    "#                     # logger.debug(f\"\\n nom_colonne_decaled :\\n{nom_colonne_decaled} \")\n",
    "#                     if df_prepared.empty:\n",
    "#                         continue  # Skip si le df préparé est vide\n",
    "\n",
    "#                     # Séparation des datasets avec la fonction split_time_series\n",
    "#                     train_data, val_data, test_data = split_time_series(df_prepared, train_size=0.7, val_size=0.25, date_column='index')\n",
    "#                     # train_data, val_data, test_data = split_time_series(df_prepared, train_size=0.7, val_size=0.25, date_column='date')\n",
    "\n",
    "#                     ######### ITERATION SUR LES COLONNES DECALLEES ############\n",
    "#                     for decalage in decalages_en_minutes:\n",
    "#                         logger.debug(f\"\\n decalage:\\n{decalage} \")\n",
    "#                         nom_colonne_cible = f'{colonne_to_predict}_plus_{decalage}min'\n",
    "#                         # nom_colonne_cible = nom_colonne_decaled\n",
    "#                         logger.debug(f\"\\n nom_colonne_cible:\\n{nom_colonne_cible} \")\n",
    "#                         # df[nom_colonne_cible] train_data[nom_colonne_cible] = train_data[colonne_to_predict].shift(-decalage)\n",
    "#                         X_train = train_data.drop([nom_colonne_cible], axis=1, errors='ignore')\n",
    "#                         y_train = train_data[nom_colonne_cible]\n",
    "#                         X_test = test_data.drop([nom_colonne_cible], axis=1, errors='ignore')\n",
    "#                         y_test = test_data[nom_colonne_cible]\n",
    "#                         logger.debug(f\"\\n X_train:\\n{X_train} \")\n",
    "#                         logger.debug(f\"\\n y_train:\\n{y_train} \")\n",
    "#                         logger.debug(f\"\\n X_test:\\n{X_test} \")\n",
    "#                         logger.debug(f\"\\n y_test:\\n{y_test} \")\n",
    "#                         model = model\n",
    "#                         # model.fit(X_train, y_train)\n",
    "#                         # predictions = model.predict(X_test)\n",
    "#                         ###### ENTRAINNEMENT #######\n",
    "#                         # Entraînement et optimisation du modèle avec GridSearchCV\n",
    "#                         grid_search.fit(X_train, y_train)\n",
    "\n",
    "#                         # Meilleur modèle après GridSearch\n",
    "#                         best_model = grid_search.best_estimator_\n",
    "#                         logger.debug(f\"\\n best_model:\\n{best_model} \")\n",
    "\n",
    "#                         ###### PREDICTIONS #######\n",
    "#                         # Prédiction sur l'ensemble de test avec le meilleur modèle\n",
    "#                         predictions = best_model.predict(X_test)\n",
    "#                         logger.debug(f\"\\n predictions:\\n{predictions} \")\n",
    "#                         ######\n",
    "\n",
    "#                         # # Enregistrement des résultats avec détails des filtres\n",
    "#                         # details_filtres = f\"{df_name} - Mois: {plage_mensuelle}, Heures: {plage_horaire}, Colonnes supprimées: {colonnes_to_drop}, Prédiction pour: +{decalage}min, Unité: {unite_colonne_to_predict}\"\n",
    "#                         # resultats.append((rmse, details_filtres))\n",
    "#                         # Calcul des scores\n",
    "#                         mse = mean_squared_error(y_test, predictions)\n",
    "#                         rmse = np.sqrt(mse)\n",
    "#                         mae = mean_absolute_error(y_test, predictions)\n",
    "#                         r2 = r2_score(y_test, predictions)\n",
    "\n",
    "#                         # Stockage des résultats\n",
    "#                         resultats.append({\n",
    "#                             'df_name': df_name,\n",
    "#                             'model': model_name,\n",
    "#                             'rmse': rmse,\n",
    "#                             'unité': unite_colonne_to_predict,\n",
    "#                             'mae': mae,\n",
    "#                             'r2': r2,\n",
    "#                             'plage_mensuelle': plage_mensuelle,\n",
    "#                             'plage_horaire': plage_horaire,\n",
    "#                             'colonnes_supprimees': colonnes_to_drop,\n",
    "#                             'meilleurs_parametres': grid_search.best_params_,\n",
    "#                             'hyperparametres': hyperparametres\n",
    "#                         })\n",
    "\n",
    "#                         logger.debug(\n",
    "#                             f\"test:{i} - {df_name}-->  RMSE: {round(rmse,3)}{unite_colonne_to_predict}, MAE: {round(mae,3)}{unite_colonne_to_predict}, R2: {round(r2*100,3)}%, Meilleurs paramètres: {grid_search.best_params_}, hyperparametre: {hyperparametres} \"\n",
    "#                         )\n",
    "#                         i += 1\n",
    "\n",
    "#                         # Log des résultats\n",
    "#                         # print(f\"RMSE: {rmse}{unite_colonne_to_predict} - Filtres: {details_filtres}\")\n",
    "#     return resultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded7cac",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# import logging\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.ensemble import GradientBoostingRegressor as GBR\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "# from sklearn.linear_model import ElasticNet as ElasticNet\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# def prepare_modele_et_grid_search(model_class, params):\n",
    "#     \"\"\"\n",
    "#     Configure et exécute GridSearchCV pour un modèle donné.\n",
    "\n",
    "#     :param model_class: Classe du modèle à utiliser.\n",
    "#     :param params: Dictionnaire de paramètres pour la configuration et GridSearch.\n",
    "#     :return: Instance du model et la preparation du GridSearchCV .\n",
    "#     \"\"\"\n",
    "#     model_name = model_class.__name__\n",
    "#     logger.debug(f\"Évaluation du modèle : {model_name}\")\n",
    "\n",
    "#     # if model_class in [RandomForestRegressor, XGBRegressor]:\n",
    "#     #     model = model_class(random_state=params.get(\"random_state\", 42))\n",
    "#     #     logger.debug(f\"\\n model_class:\\n{model_class} \")\n",
    "#     # elif model_class == LinearRegression:\n",
    "#     #     model = model_class()  # Pas de random_state pour LinearRegression\n",
    "#     #     logger.debug(f\"\\n model_class:\\n{model_class} \")\n",
    "#     # else:\n",
    "#     #     logger.error(f\"Modèle {model_class} non géré par la configuration.\")\n",
    "#     #     return []\n",
    "#     param_grid = {}\n",
    "#     hyperparametres = {}\n",
    "#     for key, value in params['model_params'].items():\n",
    "#         logger.debug(f\"\\n key:\\n{key}  ------- value {value}\")\n",
    "#         if key.startswith(model_name):\n",
    "#             logger.debug(f\"\\n key.startswith(model_name):\\n{key} \")\n",
    "#             param_name = key.replace(model_name + '__', '')\n",
    "#             param_grid[f'model__{param_name}'] = value\n",
    "#             hyperparametres[param_name] = value\n",
    "\n",
    "#     if not param_grid:\n",
    "#         logger.warning(f\"Aucun hyperparamètre spécifié pour {model_name}. Utilisation des paramètres par défaut.\")\n",
    "\n",
    "#     # Condition spéciale pour les modèles qui ne prennent pas `random_state`\n",
    "#     if model_class in [LinearRegression, KNeighborsRegressor]:\n",
    "#         model = model_class()\n",
    "#     else:\n",
    "#         model = model_class(random_state=params.get(\"random_state\"))\n",
    "\n",
    "#     grid_search = GridSearchCV(Pipeline([('model', model)]),\n",
    "#                                param_grid,\n",
    "#                                cv=params.get('cv', 3),\n",
    "#                                scoring=params.get('scoring', 'neg_mean_squared_error'),\n",
    "#                                n_jobs=-1)\n",
    "\n",
    "#     return model, grid_search, hyperparametres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563838ef",
   "metadata": {},
   "source": [
    "### fonction de preparation des df, utilisée dans (la ou les) fonction prediction prediction_vent_avec_filtre_et_horaires()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9011ad8b",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "from sklearn.linear_model import ElasticNet,  Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import logging\n",
    "\n",
    "\n",
    "def prepare_modele_et_grid_search(model_class, params):\n",
    "    \"\"\"\n",
    "    Configure et exécute GridSearchCV pour un modèle donné.\n",
    "    :param model_class: Classe du modèle à utiliser.\n",
    "    :param params: Dictionnaire de paramètres pour la configuration et GridSearch.\n",
    "    :return: Instance du model et la preparation du GridSearchCV .\n",
    "    \"\"\"\n",
    "    model_name = model_class.__name__\n",
    "    logger.debug(f\"Évaluation du modèle : {model_name}\")\n",
    "\n",
    "    param_grid = {}\n",
    "    hyperparametres = {}\n",
    "    for key, value in params['model_params'].items():\n",
    "        logger.debug(f\"\\n key:\\n{key}  ------- value {value}\")\n",
    "        if key.startswith(model_name):\n",
    "            logger.debug(f\"\\n key.startswith(model_name):\\n{key} \")\n",
    "            param_name = key.replace(model_name + '__', '')\n",
    "            logger.debug(f\"\\n param_name:\\n{param_name} \")\n",
    "            param_grid[f'model__{param_name}'] = value\n",
    "            logger.debug(f\"\\n param_grid ########  ={param_grid} \")\n",
    "            hyperparametres[param_name] = value\n",
    "\n",
    "    if not param_grid:\n",
    "        logger.warning(f\"Aucun hyperparamètre spécifié pour {model_name}. Utilisation des paramètres par défaut.\")\n",
    "\n",
    "    # Condition spéciale pour les modèles qui ne prennent pas `random_state`\n",
    "    if model_class in [LinearRegression, KNeighborsRegressor, SVR]:\n",
    "        model = model_class()\n",
    "    else:\n",
    "        model = model_class(random_state=params.get(\"random_state\"))\n",
    "\n",
    "    grid_search = GridSearchCV(Pipeline([('model', model)]),\n",
    "                               param_grid,\n",
    "                               cv=params.get('cv', 3),\n",
    "                               scoring=params.get('scoring', 'neg_mean_squared_error'),\n",
    "                               n_jobs=-1)\n",
    "\n",
    "    return model, grid_search, hyperparametres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d51ac5",
   "metadata": {},
   "source": [
    "### fonction prediction_vent_avec_filtre_et_horaires(df_original, params) utilisant 1 seule  colonnes à prédire: vent ou direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c161d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from torch import logdet\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "\n",
    "def prediction_vent_avec_filtre_et_horaires(df_original, params):\n",
    "    \"\"\"\n",
    "    Effectuer des prédictions de vitesse et de direction du vent à plusieurs horizons temporels.\n",
    "\n",
    "    :param df_original: DataFrame contenant les données météorologiques historiques.\n",
    "    :type df_original: pd.DataFrame\n",
    "    :param params: Dictionnaire contenant les paramètres de configuration pour le modèle et la prédiction.\n",
    "    :type params: dict\n",
    "    :return: Liste des résultats de prédiction, chaque élément est un dictionnaire avec les détails et le score RMSE.\n",
    "    :rtype: list\n",
    "\n",
    "    >>> df_test = pd.DataFrame(...)  # DataFrame exemple\n",
    "    >>> params_test = {...}  # Paramètres exemple\n",
    "    >>> resultats = prediction_vent_avec_filtre_et_horaires(df_test, params_test)\n",
    "    \"\"\"\n",
    "    resultats = []\n",
    "    colonne_to_predict = params['colonne_to_predict']\n",
    "    plages_mensuelles = params[\"plages_mensuelles\"]\n",
    "    plages_horaires = params[\"plages_horaires\"]\n",
    "    colonnes_possibles_a_supprimer = params[\"colonnes_possibles_a_supprimer\"]\n",
    "    heure_de_reference = params[\"heure_de_reference\"]\n",
    "    decalages_en_minutes = params[\"decalages_en_minutes\"]\n",
    "    df_name = params.get(\"df_name\", \"DataFrame\")\n",
    "    models_types = params.get('model_type', [LinearRegression])  # par defaut utilise la regression linéaire\n",
    "\n",
    "    #Controle de la presence de la colonne à predire dans le dataset\n",
    "    # Extraction de l'unité de la colonne à prédire pour les étiquettes des graphiques et le RMSE\n",
    "    if colonne_to_predict not in df_original.columns:\n",
    "        logger.error(f\"La colonne à prédire '{colonne_to_predict}' n'est pas présente dans le DataFrame {df_original.info()}.\")\n",
    "        return []\n",
    "    # unite_colonne_to_predict = colonne_to_predict[colonne_to_predict.find('('):] if '(' in colonne_to_predict else ''\n",
    "    unite_colonne_to_predict = colonne_to_predict[colonne_to_predict.find('('):] if '(' in colonne_to_predict else ''\n",
    "    # Logger les informations importantes\n",
    "    logger.debug(f\"Début de la fonction de prédiction pour la colonne: {colonne_to_predict}\")\n",
    "    i = 1\n",
    "    logger.debug(f\"\\n test n°:\\n{i} \")\n",
    "    for model_class in models_types:\n",
    "\n",
    "        ######################## appel fonction PREPARATION MODELES et GRID_SEARCH #####################\n",
    "        model, grid_search, hyperparametres = prepare_modele_et_grid_search(model_class, params)\n",
    "        model_name = model_class.__name__\n",
    "        logger.debug(f\"\\n model:\\n{model} \")\n",
    "        logger.debug(f\"\\n grid_search:\\n{grid_search} \")\n",
    "        logger.debug(f\"\\n hyperparametres:\\n{hyperparametres} \")\n",
    "        ######################## fin appel fonction PREPARATION MODELES et GRID_SEARCH #####################\n",
    "\n",
    "        ######### ITERATION SUR LES PLAGES MENSUELLES ############\n",
    "        for plage_mensuelle in plages_mensuelles:\n",
    "            logger.debug(f\"\\n plage_mensuelle:\\n{plage_mensuelle} \")\n",
    "            for plage_horaire in plages_horaires:\n",
    "                logger.debug(f\"\\nplage_horaire :\\n{plage_horaire} \")\n",
    "                for colonnes_to_drop in colonnes_possibles_a_supprimer:\n",
    "                    logger.debug(f\"\\n colonnes_to_drop:\\n{colonnes_to_drop} \")\n",
    "                    df = df_original.copy()\n",
    "                    ######### PREPARATION df ############\n",
    "                    df_prepared = preparer_donnees_pour_prediction_horaires(df, plage_mensuelle, plage_horaire, colonnes_to_drop, heure_de_reference,\n",
    "                                                                            decalages_en_minutes, colonne_to_predict)\n",
    "                    logger.debug(f\"\\n df_prepared:\\n{df_prepared.head(2)} \")\n",
    "                    logger.debug(f\"\\n liste colonnes:\\n{df_prepared.columns.tolist()} \")\n",
    "\n",
    "                    ######### TRAIN TEST SPLIT ############\n",
    "                    if df_prepared.empty:\n",
    "                        continue  # Skip si le df préparé est vide\n",
    "\n",
    "                    # Séparation des datasets avec la fonction split_time_series\n",
    "                    train_data, val_data, test_data = split_time_series(df_prepared, train_size=0.7, val_size=0.25, date_column='index')\n",
    "                    # train_data, val_data, test_data = split_time_series(df_prepared, train_size=0.7, val_size=0.25, date_column='date')\n",
    "\n",
    "                    ######### ITERATION SUR LES COLONNES DECALLEES ############\n",
    "                    for decalage in decalages_en_minutes:\n",
    "                        logger.debug(f\"\\n decalage:\\n{decalage} \")\n",
    "                        nom_colonne_cible = f'{colonne_to_predict}_plus_{decalage}min'\n",
    "                        # nom_colonne_cible = nom_colonne_decaled\n",
    "                        logger.debug(f\"\\n nom_colonne_cible:\\n{nom_colonne_cible} \")\n",
    "                        # df[nom_colonne_cible] train_data[nom_colonne_cible] = train_data[colonne_to_predict].shift(-decalage)\n",
    "                        X_train = train_data.drop([nom_colonne_cible], axis=1, errors='ignore')\n",
    "                        y_train = train_data[nom_colonne_cible]\n",
    "                        X_test = test_data.drop([nom_colonne_cible], axis=1, errors='ignore')\n",
    "                        y_test = test_data[nom_colonne_cible]\n",
    "                        logger.debug(f\"\\n X_train:\\n{X_train} \")\n",
    "                        logger.debug(f\"\\n y_train:\\n{y_train} \")\n",
    "                        logger.debug(f\"\\n X_test:\\n{X_test} \")\n",
    "                        logger.debug(f\"\\n y_test:\\n{y_test} \")\n",
    "                        # model = model\n",
    "                        # model.fit(X_train, y_train)\n",
    "                        # predictions = model.predict(X_test)\n",
    "                        ###### ENTRAINNEMENT #######\n",
    "                        # Entraînement et optimisation du modèle avec GridSearchCV\n",
    "                        grid_search.fit(X_train, y_train)\n",
    "\n",
    "                        # Meilleur modèle après GridSearch\n",
    "                        best_model = grid_search.best_estimator_\n",
    "                        logger.debug(f\"\\n best_model:\\n{best_model} \")\n",
    "\n",
    "                        ###### PREDICTIONS #######\n",
    "                        # Prédiction sur l'ensemble de test avec le meilleur modèle\n",
    "                        predictions = best_model.predict(X_test)\n",
    "                        logger.debug(f\"\\n predictions:\\n{predictions} \")\n",
    "                        ######\n",
    "\n",
    "                        # # Enregistrement des résultats avec détails des filtres\n",
    "                        # details_filtres = f\"{df_name} - Mois: {plage_mensuelle}, Heures: {plage_horaire}, Colonnes supprimées: {colonnes_to_drop}, Prédiction pour: +{decalage}min, Unité: {unite_colonne_to_predict}\"\n",
    "                        # resultats.append((rmse, details_filtres))\n",
    "                        # Calcul des scores\n",
    "                        mse = mean_squared_error(y_test, predictions)\n",
    "                        rmse = np.sqrt(mse)\n",
    "                        mae = mean_absolute_error(y_test, predictions)\n",
    "                        r2 = r2_score(y_test, predictions)\n",
    "\n",
    "                        # Stockage des résultats\n",
    "                        resultats.append({\n",
    "                            'df_name': df_name,\n",
    "                            'model': model_name,\n",
    "                            'rmse': rmse,\n",
    "                            'unité': unite_colonne_to_predict,\n",
    "                            'mae': mae,\n",
    "                            'r2': r2,\n",
    "                            'plage_mensuelle': plage_mensuelle,\n",
    "                            'plage_horaire': plage_horaire,\n",
    "                            'colonnes_supprimees': colonnes_to_drop,\n",
    "                            'meilleurs_parametres': grid_search.best_params_,\n",
    "                            'hyperparametres': hyperparametres\n",
    "                        })\n",
    "\n",
    "                        logger.debug(\n",
    "                            f\"\\ntest:{i} - {df_name}-->  RMSE: {round(rmse,3)}{unite_colonne_to_predict}, MAE: {round(mae,3)}{unite_colonne_to_predict}, R2: {round(r2*100,3)}%, Meilleurs paramètres: {grid_search.best_params_}, hyperparametre: {hyperparametres} \\n\"\n",
    "                        )\n",
    "                        i += 1\n",
    "\n",
    "                        # Log des résultats\n",
    "                        # print(f\"RMSE: {rmse}{unite_colonne_to_predict} - Filtres: {details_filtres}\")\n",
    "    return resultats, unite_colonne_to_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16194d9",
   "metadata": {},
   "source": [
    "### fonction prediction_vent_avec_filtre_et_horaires(df_original, params) integrant une iteration sur les 2 colonnes à prédire: vent et direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0087159",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from torch import logdet\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "\n",
    "def prediction_vent_direction_avec_filtre_et_horaires(df_original, params):\n",
    "    \"\"\"\n",
    "    Effectuer des prédictions de vitesse et de direction du vent à plusieurs horizons temporels.\n",
    "\n",
    "    :param df_original: DataFrame contenant les données météorologiques historiques.\n",
    "    :type df_original: pd.DataFrame\n",
    "    :param params: Dictionnaire contenant les paramètres de configuration pour le modèle et la prédiction.\n",
    "    :type params: dict\n",
    "    :return: Liste des résultats de prédiction, chaque élément est un dictionnaire avec les détails et le score RMSE.\n",
    "    :rtype: list\n",
    "\n",
    "    >>> df_test = pd.DataFrame(...)  # DataFrame exemple\n",
    "    >>> params_test = {...}  # Paramètres exemple\n",
    "    >>> resultats = prediction_vent_avec_filtre_et_horaires(df_test, params_test)\n",
    "    \"\"\"\n",
    "    resultats = []\n",
    "    colonnes_to_predict = params['colonnes_to_predict']\n",
    "    plages_mensuelles = params[\"plages_mensuelles\"]\n",
    "    plages_horaires = params[\"plages_horaires\"]\n",
    "    colonnes_possibles_a_supprimer = params[\"colonnes_possibles_a_supprimer\"]\n",
    "    heure_de_reference = params[\"heure_de_reference\"]\n",
    "    decalages_en_minutes = params[\"decalages_en_minutes\"]\n",
    "    df_name = params.get(\"df_name\", \"DataFrame\")\n",
    "    models_types = params.get('model_type', [LinearRegression])  # par defaut utilise la regression linéaire\n",
    "\n",
    "\n",
    "    for colonne_to_predict in colonnes_to_predict:\n",
    "        #Controle de la presence de la colonne à predire dans le dataset\n",
    "        # Extraction de l'unité de la colonne à prédire pour les étiquettes des graphiques et le RMSE\n",
    "        if colonne_to_predict not in df_original.columns:\n",
    "            logger.error(f\"La colonne à prédire '{colonne_to_predict}' n'est pas présente dans le DataFrame {df_original.info()}.\")\n",
    "            return []\n",
    "        # unite_colonne_to_predict = colonne_to_predict[colonne_to_predict.find('('):] if '(' in colonne_to_predict else ''\n",
    "        unite_colonne_to_predict = colonne_to_predict[colonne_to_predict.find('('):] if '(' in colonne_to_predict else ''\n",
    "        # Logger les informations importantes\n",
    "        logger.debug(f\"Début de la fonction de prédiction pour la colonne: {colonne_to_predict}\")\n",
    "        i = 1\n",
    "        logger.debug(f\"\\n test n°:\\n{i} \")\n",
    "        for model_class in models_types:\n",
    "\n",
    "            ######################## appel fonction PREPARATION MODELES et GRID_SEARCH #####################\n",
    "            model, grid_search, hyperparametres = prepare_modele_et_grid_search(model_class, params)\n",
    "            model_name = model_class.__name__\n",
    "            logger.debug(f\"\\n model:\\n{model} \")\n",
    "            logger.debug(f\"\\n grid_search:\\n{grid_search} \")\n",
    "            logger.debug(f\"\\n hyperparametres:\\n{hyperparametres} \")\n",
    "            ######################## fin appel fonction PREPARATION MODELES et GRID_SEARCH #####################\n",
    "\n",
    "            ######### ITERATION SUR LES PLAGES MENSUELLES ############\n",
    "            for plage_mensuelle in plages_mensuelles:\n",
    "                logger.debug(f\"\\n plage_mensuelle:\\n{plage_mensuelle} \")\n",
    "                for plage_horaire in plages_horaires:\n",
    "                    logger.debug(f\"\\nplage_horaire :\\n{plage_horaire} \")\n",
    "                    for colonnes_to_drop in colonnes_possibles_a_supprimer:\n",
    "                        logger.debug(f\"\\n colonnes_to_drop:\\n{colonnes_to_drop} \")\n",
    "                        df = df_original.copy()\n",
    "                        ######### PREPARATION df ############\n",
    "                        df_prepared = preparer_donnees_pour_prediction_horaires(df, plage_mensuelle, plage_horaire, colonnes_to_drop, heure_de_reference,\n",
    "                                                                                decalages_en_minutes, colonne_to_predict)\n",
    "                        logger.debug(f\"\\n df_prepared:\\n{df_prepared.head(2)} \")\n",
    "                        logger.debug(f\"\\n liste colonnes:\\n{df_prepared.columns.tolist()} \")\n",
    "\n",
    "                        ######### TRAIN TEST SPLIT ############\n",
    "                        if df_prepared.empty:\n",
    "                            continue  # Skip si le df préparé est vide\n",
    "\n",
    "                        # Séparation des datasets avec la fonction split_time_series\n",
    "                        train_data, val_data, test_data = split_time_series(df_prepared, train_size=0.7, val_size=0.25, date_column='index')\n",
    "                        # train_data, val_data, test_data = split_time_series(df_prepared, train_size=0.7, val_size=0.25, date_column='date')\n",
    "\n",
    "                        ######### ITERATION SUR LES COLONNES DECALLEES ############\n",
    "                        for decalage in decalages_en_minutes:\n",
    "                            logger.debug(f\"\\n decalage:\\n{decalage} \")\n",
    "                            nom_colonne_cible = f'{colonne_to_predict}_plus_{decalage}min'\n",
    "                            # nom_colonne_cible = nom_colonne_decaled\n",
    "                            logger.debug(f\"\\n nom_colonne_cible:\\n{nom_colonne_cible} \")\n",
    "                            # df[nom_colonne_cible] train_data[nom_colonne_cible] = train_data[colonne_to_predict].shift(-decalage)\n",
    "                            X_train = train_data.drop([nom_colonne_cible], axis=1, errors='ignore')\n",
    "                            y_train = train_data[nom_colonne_cible]\n",
    "                            X_test = test_data.drop([nom_colonne_cible], axis=1, errors='ignore')\n",
    "                            y_test = test_data[nom_colonne_cible]\n",
    "                            logger.debug(f\"\\n X_train:\\n{X_train} \")\n",
    "                            logger.debug(f\"\\n y_train:\\n{y_train} \")\n",
    "                            logger.debug(f\"\\n X_test:\\n{X_test} \")\n",
    "                            logger.debug(f\"\\n y_test:\\n{y_test} \")\n",
    "                            # model = model\n",
    "                            # model.fit(X_train, y_train)\n",
    "                            # predictions = model.predict(X_test)\n",
    "                            ###### ENTRAINNEMENT #######\n",
    "                            # Entraînement et optimisation du modèle avec GridSearchCV\n",
    "                            grid_search.fit(X_train, y_train)\n",
    "\n",
    "                            # Meilleur modèle après GridSearch\n",
    "                            best_model = grid_search.best_estimator_\n",
    "                            logger.debug(f\"\\n best_model:\\n{best_model} \")\n",
    "\n",
    "                            ###### PREDICTIONS #######\n",
    "                            # Prédiction sur l'ensemble de test avec le meilleur modèle\n",
    "                            predictions = best_model.predict(X_test)\n",
    "                            logger.debug(f\"\\n predictions:\\n{predictions} \")\n",
    "                            ######\n",
    "\n",
    "                            # # Enregistrement des résultats avec détails des filtres\n",
    "                            # details_filtres = f\"{df_name} - Mois: {plage_mensuelle}, Heures: {plage_horaire}, Colonnes supprimées: {colonnes_to_drop}, Prédiction pour: +{decalage}min, Unité: {unite_colonne_to_predict}\"\n",
    "                            # resultats.append((rmse, details_filtres))\n",
    "                            # Calcul des scores\n",
    "                            mse = mean_squared_error(y_test, predictions)\n",
    "                            rmse = np.sqrt(mse)\n",
    "                            mae = mean_absolute_error(y_test, predictions)\n",
    "                            r2 = r2_score(y_test, predictions)\n",
    "\n",
    "                            # Stockage des résultats\n",
    "                            resultats.append({\n",
    "                                'df_name': df_name,\n",
    "                                'model': model_name,\n",
    "                                'rmse': rmse,\n",
    "                                'unité': unite_colonne_to_predict,\n",
    "                                'mae': mae,\n",
    "                                'r2': r2,\n",
    "                                'plage_mensuelle': plage_mensuelle,\n",
    "                                'plage_horaire': plage_horaire,\n",
    "                                'colonnes_supprimees': colonnes_to_drop,\n",
    "                                'meilleurs_parametres': grid_search.best_params_,\n",
    "                                'hyperparametres': hyperparametres\n",
    "                            })\n",
    "\n",
    "                            logger.debug(\n",
    "                                f\"\\ntest:{i} - {df_name}-->  RMSE: {round(rmse,3)}{unite_colonne_to_predict}, MAE: {round(mae,3)}{unite_colonne_to_predict}, R2: {round(r2*100,3)}%, Meilleurs paramètres: {grid_search.best_params_}, hyperparametre: {hyperparametres} \\n\"\n",
    "                            )\n",
    "                            i += 1\n",
    "\n",
    "                            # Log des résultats\n",
    "                            # print(f\"RMSE: {rmse}{unite_colonne_to_predict} - Filtres: {details_filtres}\")\n",
    "    return resultats, unite_colonne_to_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes_meteo_france_6_min_clean_sans_outlier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb263da",
   "metadata": {},
   "source": [
    "### Extraction du top 3 de la liste des résultats, utilisé dans la fonction de prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fd41c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def top3_resultats_modeles_par_modele(resultats, unite_colonne_to_predict):\n",
    "    \"\"\"\n",
    "    Affiche et retourne les trois meilleurs ensembles de configurations pour chaque modèle utilisé,\n",
    "    basés sur le score RMSE moyen le plus bas, incluant également les scores MAE et R² ainsi que\n",
    "    les paramètres et hyperparamètres pour chaque ensemble, avec le nom de la station météo.\n",
    "    \"\"\"\n",
    "    # Regroupement des résultats par modèle\n",
    "    resultats_par_modele = defaultdict(list)\n",
    "    for groupe_resultat in resultats:\n",
    "        for resultat in groupe_resultat:\n",
    "            modele = resultat['model']\n",
    "            # Filtrer les résultats pour satisfaire les conditions RMSE et R² car un RMSE de 0 avec un score de 100% est signe d'un problemé d'apprentissage\n",
    "            if resultat['rmse'] >= 0.1 and resultat['r2'] <= 0.999:\n",
    "                resultats_par_modele[modele].append(resultat)\n",
    "            # resultats_par_modele[modele].append(resultat)\n",
    "    top3_global = []\n",
    "    # Pour chaque modèle, sélectionner les 3 meilleures configurations basées sur le RMSE,\n",
    "    # le MAE, et favoriser le plus grand R² en cas d'égalité\n",
    "    for modele, res_modele in resultats_par_modele.items():\n",
    "        # Trier les configurations d'abord par RMSE croissant, puis par MAE croissant, et enfin par R² décroissant\n",
    "        top3 = sorted(res_modele, key=lambda x: (x['rmse'], x['mae'], -x['r2']))[:3]\n",
    "        modele_top3 = {\n",
    "        'modele': modele,\n",
    "        'configurations': top3\n",
    "        }\n",
    "\n",
    "        top3_global.append(modele_top3)\n",
    "\n",
    "        print(f\"# Meilleurs ensembles de paramètres pour le modèle: {modele}\")\n",
    "        for config in top3:\n",
    "            # Inclure le nom de la station météo dans les détails\n",
    "            print(f\"## RMSE: {config['rmse']:.3f}{unite_colonne_to_predict}, MAE: {config['mae']:.3f}, R2: {config['r2']*100:.3f}%, \"\n",
    "                  f\"Détails: Station: {config.get('station_name', 'N/A')}, Plage mensuelle: {config.get('plage_mensuelle', 'N/A')}, \"\n",
    "                  f\"Plage horaire: {config.get('plage_horaire', 'N/A')}, Colonnes supprimées: {config.get('colonnes_supprimees', 'N/A')}, \"\n",
    "                  f\"Paramètres: {config.get('meilleurs_parametres', 'N/A')}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return top3_global\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46cee89",
   "metadata": {},
   "source": [
    "### lancement des predictions avec définition des paramétres et hyper parametres pour le gridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc028792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # %%capture   captureSVR\n",
    "\n",
    "# import sys\n",
    "# from contextlib import redirect_stdout\n",
    "\n",
    "\n",
    "\n",
    "# # from line_profiler import LineProfiler\n",
    "# %load_ext memory_profiler\n",
    "# %load_ext line_profiler\n",
    "\n",
    "# # df = dataframes_meteo_france_6_min_clean_sans_outlier[\"df_station_13005003\"]\n",
    "\n",
    "\n",
    "# liste_resultats = []\n",
    "# # for df_name, df in dataframes.items():\n",
    "# for df_name, df in dataframes_meteo_france_6_min_clean_sans_outlier.items():\n",
    "\n",
    "#     df = dataframes_meteo_france_6_min_clean_sans_outlier[df_name]\n",
    "#     # df = dataframes[df_name]\n",
    "#     df = df.copy()\n",
    "#     logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "#     logger.debug(f\"\\n df:\\n{df.head(2)} \")\n",
    "#     params = {\n",
    "#         \"heure_de_reference\":\n",
    "#         9,\n",
    "#         \"decalages_en_minutes\": [40, 120, 180],\n",
    "#         # \"colonnes_to_predict\":['vitesse_vent_(km/h)'],\n",
    "#         # \"colonnes_to_predict\":['vitesse_vent_(km/h)','direction_(°)'],\n",
    "#         # \"colonnes_to_predict\":['vitesse_vent_(km/h)','direction_(°)'],\n",
    "#         # \"colonne_to_predict\": 'direction_(°)',\n",
    "#         \"colonne_to_predict\": 'vitesse_vent_(km/h)',\n",
    "#         \"df_name\": df_name,\n",
    "#         \"plages_mensuelles\": [[6, 7, 8, 9], [7, 8], [5, 6, 7, 8, 9, 10], [4, 5, 6, 7, 8, 9, 10]],\n",
    "#         \"plages_horaires\": [[8, 18], [7, 19], [9, 17], [10, 16], [6, 20], [0, 23]],\n",
    "#         \"colonnes_possibles_a_supprimer\": [\n",
    "#             ['précipitations_(mm)'],\n",
    "#             ['direction_(°)'],\n",
    "#             ['humidity_(%)'],\n",
    "#             ['temperature_(°C)'],\n",
    "#             ['précipitations_(mm)', 'direction_(°)'],\n",
    "#             ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'],\n",
    "#             []  # Aucune colonne supprimée\n",
    "#         ],\n",
    "\n",
    "#         # liste des modeles machine learning\n",
    "#         # \"model_type\": [RandomForestRegressor, LinearRegression, XGBRegressor, KNeighborsRegressor, SVR, GBR, DTR, Ridge, Lasso, ElasticNet],\n",
    "#         \"model_type\": [ElasticNet],\n",
    "#         # liste des hyperparamétres\n",
    "#         \"model_params\": {\n",
    "#             # MODELE D'ENSEMBLE\n",
    "#             # RandomForestRegressor modele d'ensemble d'arbres de décision, combine plusieurs arbres de décision\n",
    "#             'RandomForestRegressor__n_estimators': [100, 200],\n",
    "#             'RandomForestRegressor__max_depth': [None, 10, 20],\n",
    "#             # XGBRegressor modele ( Xgboost eXtreme Gradient Boosting) d'ensemble type gradient boosting,modele de régression non linéaire,\n",
    "#             # ajoute séquentiellement des modèles faibles, utilise une fonction de perte spécifique et une régularisation pour optimiser les performances du modèle.\n",
    "#             'XGBRegressor__learning_rate': [0.01, 0.1],\n",
    "#             'XGBRegressor__max_depth': [3, 5, 7],\n",
    "#             # modèle d'ensemble qui combine plusieurs arbres de régression\n",
    "#             'GBR__n_estimators': [50, 100, 200],\n",
    "#             'GBR__learning_rate': [0.01, 0.1, 0.2],\n",
    "#             'GBR__max_depth': [3, 5, 7],\n",
    "#             # CLUSTERING modele de régression non linéaire\n",
    "#             # KNeighborsRegressor Clustering modèle des k plus proches voisins (KNeighbors) prédiction non paramétrique\n",
    "#             'KNeighborsRegressor__n_neighbors': [5, 10],\n",
    "#             'KNeighborsRegressor__weights': ['uniform', 'distance'],\n",
    "#             # SVM apprentissage automatique, modele de régression non linéaire\n",
    "#             # Support Vector Regressor méthode de régression basée sur les machines à vecteurs de support (SVM)\n",
    "#             # 'SVR__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#             # 'SVR__C': [0.1, 1, 10],\n",
    "#             # 'SVR__epsilon': [0.1, 0.2, 0.5],\n",
    "#             'SVR__kernel': ['linear'],\n",
    "#             'SVR__C': [0.1],\n",
    "#             'SVR__epsilon': [ 0.2],\n",
    "#             #REGRESSION\n",
    "#             # Decision Tree Regressor modèle d'apprentissage automatique Modèle d'arbres de décision\n",
    "#             'DTR__max_depth': [None, 5, 10],\n",
    "#             'DTR__min_samples_split': [2, 5, 10],\n",
    "#             'DTR__min_samples_leaf': [1, 2, 4],\n",
    "#             #REGRESSION LINEAIRE\n",
    "#             # modele utilisant la regression linaire simple\n",
    "#             'LinearRegression__fit_intercept': [True, False],\n",
    "#             # modele utilisant la regression linaire regularisé avec penalité L2\n",
    "#             # 'Ridge__alpha': [0.1, 0.5, 1.0],\n",
    "#             'Ridge__alpha': [0.05, 0.075, 0.1, 0.125, 0.15],\n",
    "#             'Ridge__fit_intercept': [True, False],\n",
    "#             'Ridge__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "#             # modele utilisant la regression linaire regularisé avec penalité L1\n",
    "#             'Lasso__alpha': [0.1, 0.5, 1.0],\n",
    "#             'Lasso__fit_intercept': [True, False],\n",
    "#             'Lasso__precompute': [True],\n",
    "#             # modele utilisant la regression linaire regularisé avec penalité L1(lasso) et L2 (ridge)\n",
    "#             'ElasticNet__alpha': [ 0.05],\n",
    "#             'ElasticNet__l1_ratio': [0.05],\n",
    "#             'ElasticNet__fit_intercept': [True],\n",
    "#             # 'ElasticNet__alpha': [0.05, 0.075, 0.1, 0.125, 0.15],\n",
    "#             # 'ElasticNet__l1_ratio': [0.05, 0.1, 0.2],\n",
    "#             # 'ElasticNet__fit_intercept': [True,False],\n",
    "#             # 'ElasticNet__alpha': [0.1, 0.5, 1.0],\n",
    "#             # 'ElasticNet__l1_ratio': [0.1, 0.5, 0.9],\n",
    "#             # 'ElasticNet__fit_intercept': [True, False],\n",
    "#             'cv': 3,\n",
    "#             'scoring': 'neg_mean_squared_error',\n",
    "#         },\n",
    "#         \"random_state\":42,\n",
    "#     }\n",
    "\n",
    "#     #  fonction  appelée  avec le profiler\n",
    "#     with open('profilage_resultats.txt', 'w') as f:\n",
    "#         with redirect_stdout(f):\n",
    "#             %lprun -f prediction_vent_avec_filtre_et_horaires prediction_vent_avec_filtre_et_horaires(df, params)\n",
    "#     # %lprun -f prediction_vent_avec_filtre_et_horaires prediction_vent_avec_filtre_et_horaires(df, params)\n",
    "#     #Suivi consommatio nmemeoire\n",
    "#     %mprun -T mprun_output.txt -f prediction_vent_avec_filtre_et_horaires prediction_vent_avec_filtre_et_horaires(df, params)\n",
    "\n",
    "#     # Extract column names to predict\n",
    "#     if 'colonnes_to_predict' in params:\n",
    "#         liste_colonnes_to_predict = '_'.join(params['colonnes_to_predict'])\n",
    "#     else:\n",
    "#         liste_colonnes_to_predict = '_'.join(params['colonne_to_predict'])\n",
    "\n",
    "\n",
    "#     # Extract model names used\n",
    "#     model_names = '_'.join(model.__name__ for model in params['model_type'])\n",
    "\n",
    "#     # Construct the filename incorporating both column and model names\n",
    "#     name_tableau_resultat = f\"scores_des_différents_modèles_{df_name}_{liste_colonnes_to_predict}_{model_names}\"\n",
    "\n",
    "#     # # name_tableau_resultat = f'scores_des_différents_modéles_{df_name}_{params['colonnes_to_predict'][0]}_{params['colonnes_to_predict'][1]}'\n",
    "#     # # Determine the number of columns to predict and construct the file name accordingly\n",
    "#     # if len(params['colonnes_to_predict']) > 1:\n",
    "#     #     name_tableau_resultat = f\"scores_des_différents_modèles_{df_name}_{params['colonnes_to_predict'][0]}_{params['colonnes_to_predict'][1]}\"\n",
    "#     # else:\n",
    "#     #     name_tableau_resultat = f\"scores_des_différents_modèles_{df_name}_{params['colonnes_to_predict'][0]}\"\n",
    "\n",
    "#     # lancement fonction prediction sur une seule colonne en mêm temps\n",
    "#     resultats = prediction_vent_avec_filtre_et_horaires(df, params)\n",
    "#     # lancement fonction prediction sur les 2 colonnes à predire, vent et direction\n",
    "#     # resultats = prediction_vent_direction_avec_filtre_et_horaires(df, params)\n",
    "#     name_tableau_resultat = resultats\n",
    "#     print(f\"\\n name_tableau_resultat:\\n{name_tableau_resultat} \\n\")\n",
    "#     liste_resultats.append(resultats)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, value in globals().items():\n",
    "    if type(value) is pd.DataFrame and value is df_clean_12col:\n",
    "       print(f\"\\n :\\n{df_name} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ddf76",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "# # %%capture   captureSVR\n",
    "\n",
    "import sys\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "\n",
    "\n",
    "# from line_profiler import LineProfiler\n",
    "%load_ext memory_profiler\n",
    "%load_ext line_profiler\n",
    "\n",
    "# df = dataframes_meteo_france_6_min_clean_sans_outlier[\"df_station_13005003\"]\n",
    "\n",
    "\n",
    "\n",
    "def lancement_prediction(dataframes, path_sauvegarde):\n",
    "    \"\"\"\n",
    "    Lance la prédiction sur un ensemble de DataFrames spécifiés avec des paramètres prédéfinis.\n",
    "\n",
    "    Args:\n",
    "        dataframes (dict): Dictionnaire des DataFrames où chaque clé est le nom du DataFrame\n",
    "                           et chaque valeur est le DataFrame lui-même.\n",
    "\n",
    "    Returns:\n",
    "        list: Liste contenant les résultats des prédictions pour chaque DataFrame.\n",
    "    \"\"\"\n",
    "    liste_resultats = []\n",
    "    resultats = []\n",
    "    # Determine s'il y a plusieurs df dans dataframes\n",
    "    multiple_dfs = len(dataframes)\n",
    "    logger.debug(f\"\\n multiple_dfs:\\n{multiple_dfs} \")\n",
    "    # # si le dico dataframes fourni n'est pas un dico mais  un df il faut le transformer en dico\n",
    "    # # if multiple_dfs==1 and isinstance(dataframes, pd.DataFrame):\n",
    "    # if multiple_dfs==1 :\n",
    "    #     for df_name, value in globals().items():\n",
    "    #         if type(value) is pd.DataFrame and value is dataframes:\n",
    "    #             logger.critical(f\"\\n :\\n{df_name} \\n\")\n",
    "    #     dataframes = {df_name: dataframes}\n",
    "    # else:\n",
    "    #     dataframes = df_example\n",
    "\n",
    "    for df_name, df in dataframes.items():\n",
    "    # for df_name, df in dataframes_meteo_france_6_min_clean_sans_outlier.items():\n",
    "        # try:\n",
    "        # df = dataframes_meteo_france_6_min_clean_sans_outlier[df_name]\n",
    "        df = dataframes[df_name]\n",
    "        df = df.copy()\n",
    "        logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "        logger.debug(f\"\\n df:\\n{df.head(2)} \")\n",
    "        params = {\n",
    "            \"heure_de_reference\": 9,\n",
    "            \"decalages_en_minutes\": [40, 120, 180],\n",
    "            # \"colonnes_to_predict\":['vitesse_vent_(km/h)'],\n",
    "            # \"colonnes_to_predict\":['vitesse_vent_(km/h)','direction_(°)'],\n",
    "            # \"colonnes_to_predict\":['vitesse_vent_(km/h)','direction_(°)'],\n",
    "            \"colonne_to_predict\": 'direction_(°)',\n",
    "            # \"colonne_to_predict\": 'vitesse_vent_(km/h)',\n",
    "            # \"colonne_to_predict\": 'vitesse_(km/h)',\n",
    "            \"df_name\": df_name,\n",
    "            \"plages_mensuelles\": [[6, 7, 8, 9], [7, 8], [5, 6, 7, 8, 9, 10], [4, 5, 6, 7, 8, 9, 10]],\n",
    "            \"plages_horaires\": [[8, 18], [7, 19], [9, 17], [10, 16], [6, 20], [0, 23]],\n",
    "            \"colonnes_possibles_a_supprimer\": [\n",
    "                ['précipitations_(mm)'],\n",
    "                ['direction_(°)'],\n",
    "                ['humidity_(%)'],\n",
    "                ['temperature_(°C)'],\n",
    "                ['précipitations_(mm)', 'direction_(°)'],\n",
    "                ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'],\n",
    "                []  # Aucune colonne supprimée\n",
    "            ],\n",
    "\n",
    "            # liste des modeles machine learning\n",
    "            # \"model_type\": [RandomForestRegressor, LinearRegression, XGBRegressor, KNeighborsRegressor, SVR, GBR, DTR, Ridge, Lasso, ElasticNet],\n",
    "            \"model_type\": [RandomForestRegressor, LinearRegression, XGBRegressor, KNeighborsRegressor, GBR, DTR, Ridge, Lasso, ElasticNet],\n",
    "            # \"model_type\": [ LinearRegression, Ridge, Lasso, ElasticNet],\n",
    "            # \"model_type\": [ElasticNet],\n",
    "            # liste des hyperparamétres\n",
    "            \"model_params\": {\n",
    "                # MODELE D'ENSEMBLE\n",
    "                # RandomForestRegressor modele d'ensemble d'arbres de décision, combine plusieurs arbres de décision\n",
    "                'RandomForestRegressor__n_estimators': [100, 200],\n",
    "                'RandomForestRegressor__max_depth': [None, 10, 20],\n",
    "                # XGBRegressor modele ( Xgboost eXtreme Gradient Boosting) d'ensemble type gradient boosting,modele de régression non linéaire,\n",
    "                # ajoute séquentiellement des modèles faibles, utilise une fonction de perte spécifique et une régularisation pour optimiser les performances du modèle.\n",
    "                'XGBRegressor__learning_rate': [0.01, 0.1],\n",
    "                'XGBRegressor__max_depth': [3, 5, 7],\n",
    "                # modèle d'ensemble qui combine plusieurs arbres de régression\n",
    "                'GBR__n_estimators': [50, 100, 200],\n",
    "                'GBR__learning_rate': [0.01, 0.1, 0.2],\n",
    "                'GBR__max_depth': [3, 5, 7],\n",
    "                # CLUSTERING modele de régression non linéaire\n",
    "                # KNeighborsRegressor Clustering modèle des k plus proches voisins (KNeighbors) prédiction non paramétrique\n",
    "                'KNeighborsRegressor__n_neighbors': [5, 10],\n",
    "                'KNeighborsRegressor__weights': ['uniform', 'distance'],\n",
    "                # SVM apprentissage automatique, modele de régression non linéaire\n",
    "                # Support Vector Regressor méthode de régression basée sur les machines à vecteurs de support (SVM)\n",
    "                # 'SVR__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                # 'SVR__C': [0.1, 1, 10],\n",
    "                # 'SVR__epsilon': [0.1, 0.2, 0.5],\n",
    "                'SVR__kernel': ['linear'],\n",
    "                'SVR__C': [0.1],\n",
    "                'SVR__epsilon': [ 0.2],\n",
    "                #REGRESSION\n",
    "                # Decision Tree Regressor modèle d'apprentissage automatique Modèle d'arbres de décision\n",
    "                'DTR__max_depth': [None, 5, 10],\n",
    "                'DTR__min_samples_split': [2, 5, 10],\n",
    "                'DTR__min_samples_leaf': [1, 2, 4],\n",
    "                #REGRESSION LINEAIRE\n",
    "                # modele utilisant la regression linaire simple\n",
    "                'LinearRegression__fit_intercept': [True, False],\n",
    "                # modele utilisant la regression linaire regularisé avec penalité L2\n",
    "                # 'Ridge__alpha': [0.1, 0.5, 1.0],\n",
    "                'Ridge__alpha': [0.05, 0.075, 0.1, 0.125, 0.15],\n",
    "                'Ridge__fit_intercept': [True, False],\n",
    "                'Ridge__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "                # modele utilisant la regression linaire regularisé avec penalité L1\n",
    "                'Lasso__alpha': [0.1, 0.5, 1.0],\n",
    "                'Lasso__fit_intercept': [True, False],\n",
    "                'Lasso__precompute': [True],\n",
    "                # modele utilisant la regression linaire regularisé avec penalité L1(lasso) et L2 (ridge)\n",
    "                # 'ElasticNet__alpha': [ 0.01],\n",
    "                # 'ElasticNet__l1_ratio': [0.01],\n",
    "                # 'ElasticNet__fit_intercept': [True],\n",
    "                # 'ElasticNet__alpha': [0.05, 0.075, 0.1, 0.125, 0.15],\n",
    "                # 'ElasticNet__l1_ratio': [0.05, 0.1, 0.2],\n",
    "                # 'ElasticNet__fit_intercept': [True,False],\n",
    "                'ElasticNet__alpha': [0.01,0.1, 0.5, 1.0],\n",
    "                'ElasticNet__l1_ratio': [0.01,0.1, 0.5, 0.9],\n",
    "                'ElasticNet__fit_intercept': [True, False],\n",
    "                'ElasticNet__max_iter': [100000],\n",
    "                'cv': 3,\n",
    "                'scoring': 'neg_mean_squared_error',\n",
    "            },\n",
    "            \"random_state\":42,\n",
    "        }\n",
    "\n",
    "        #  fonction  appelée  avec le profiler\n",
    "        # with open('profilage_resultats.txt', 'w') as f:\n",
    "        #     with redirect_stdout(f):\n",
    "        #         %lprun -f prediction_vent_avec_filtre_et_horaires prediction_vent_avec_filtre_et_horaires(df, params)\n",
    "\n",
    "        # %lprun -f prediction_vent_avec_filtre_et_horaires prediction_vent_avec_filtre_et_horaires(df, params)\n",
    "        #Suivi consommatio nmemeoire\n",
    "        # %mprun -T mprun_output.txt -f prediction_vent_avec_filtre_et_horaires prediction_vent_avec_filtre_et_horaires(df, params)\n",
    "\n",
    "        # # Extract column names to predict\n",
    "        # liste_colonnes_to_predict = '_'.join(params['colonnes_to_predict'])\n",
    "        # Extract column names to predict\n",
    "        if 'colonnes_to_predict' in params:\n",
    "            # liste_colonnes_to_predict = '_'.join(params['colonnes_to_predict'])\n",
    "            liste_colonnes_to_predict = params['colonnes_to_predict']\n",
    "        else:\n",
    "            # liste_colonnes_to_predict = '_'.join(params['colonne_to_predict'])\n",
    "            liste_colonnes_to_predict = params['colonne_to_predict']\n",
    "\n",
    "        # unite_colonne_to_predict = colonne_to_predict[colonne_to_predict.find('('):] if '(' in colonne_to_predict else ''\n",
    "\n",
    "        # Extract model names used\n",
    "        model_names = '_'.join(model.__name__ for model in params['model_type'])\n",
    "        logger.debug(f\"\\n model_names:\\n{model_names} \")\n",
    "        logger.debug(f\"\\n liste_colonnes_to_predict:\\n{liste_colonnes_to_predict} \")\n",
    "\n",
    "        if multiple_dfs==1:\n",
    "            station_name=df_name\n",
    "\n",
    "        else:\n",
    "            station_name=\"all_stations\"\n",
    "        # Construct the filename incorporating both column and model names\n",
    "        name_tableau_resultat = f\"scores_modeles_{model_names}_de_{station_name}_{liste_colonnes_to_predict.replace('(','').replace(')','').replace('/','_')}\"\n",
    "        logger.debug(f\"\\n name_tableau_resultat:\\n{name_tableau_resultat} \")\n",
    "\n",
    "        # # name_tableau_resultat = f'scores_des_différents_modéles_{df_name}_{params['colonnes_to_predict'][0]}_{params['colonnes_to_predict'][1]}'\n",
    "        # # Determine the number of columns to predict and construct the file name accordingly\n",
    "        # if len(params['colonnes_to_predict']) > 1:\n",
    "        #     name_tableau_resultat = f\"scores_des_différents_modèles_{df_name}_{params['colonnes_to_predict'][0]}_{params['colonnes_to_predict'][1]}\"\n",
    "        # else:\n",
    "        #     name_tableau_resultat = f\"scores_des_différents_modèles_{df_name}_{params['colonnes_to_predict'][0]}\"\n",
    "\n",
    "        # lancement fonction prediction sur une seule colonne en mêm temps\n",
    "        resultats,unite_colonne_to_predict = prediction_vent_avec_filtre_et_horaires(df, params)\n",
    "        # lancement fonction prediction sur les 2 colonnes à predire, vent et direction\n",
    "        # resultats = prediction_vent_direction_avec_filtre_et_horaires(df, params)\n",
    "        # name_tableau_resultat = resultats\n",
    "        liste_resultats.append(resultats)\n",
    "\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     logger.error(f\"Erreur lors du traitement de {df_name}: {e}\")\n",
    "        #     continue\n",
    "\n",
    "    # top 3 des résultats\n",
    "    top3=top3_resultats_modeles_par_modele(liste_resultats,unite_colonne_to_predict)\n",
    "\n",
    "    # sauvegarde les résultats du modele au format pkl\n",
    "    model_name = name_tableau_resultat\n",
    "    path = os.path.join(path_sauvegarde, f\"{model_name}.pkl\").replace('\\\\', '/')\n",
    "    logger.debug(f\"\\n path:\\n{path} \")\n",
    "    name_file = name_tableau_resultat\n",
    "\n",
    "    sauve_charge_pickle(path, name_file=name_file, true_for_load=False, is_file=True)\n",
    "\n",
    "    return liste_resultats , top3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942daf31",
   "metadata": {},
   "source": [
    "### prediction pour meteo_france au pas de 6 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c78856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauve_charge_pickle(path, name_file=name_file, true_for_load=False, is_file=True)\n",
    "# Meilleurs ensembles de paramètres pour le modèle: ElasticNet\n",
    "## RMSE: 0.806, MAE: 0.626, R2: 98.078%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__alpha': 0.05, 'model__fit_intercept': True, 'model__l1_ratio': 0.05}\n",
    "## RMSE: 0.807, MAE: 0.627, R2: 98.072%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__alpha': 0.05, 'model__fit_intercept': True, 'model__l1_ratio': 0.05}\n",
    "## RMSE: 0.807, MAE: 0.627, R2: 98.070%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {'model__alpha': 0.05, 'model__fit_intercept': True, 'model__l1_ratio': 0.05}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2436cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_meteo_france_6_min_clean_sans_outlier\n",
    "# df_clean_12col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79f7ae",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# dataframes_meteo_france_6_min_clean_sans_outlier\n",
    "dataframes = dataframes_meteo_france_6_min_clean_sans_outlier\n",
    "path_sauvegarde = path_sauve_modeles_meteo_france_6min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbfc40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### modif niveau de log\n",
    "\n",
    "# params = {\"niveau_log\": 'DEBUG'}\n",
    "# params = {\"niveau_log\": 'INFO'}\n",
    "# params = {\"niveau_log\": 'WARNING'}\n",
    "params = {\"niveau_log\": 'ERROR'}\n",
    "# params = {\"niveau_log\": 'CRITICAL'}\n",
    "reconfigurer_logging(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8a5f5",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "liste_resultats,top3=lancement_prediction(dataframes, path_sauvegarde)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf300dd",
   "metadata": {},
   "source": [
    "### prediction pour mobilils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08601178",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du logging\n",
    "# 'DEBUG' 'INFO' 'WARNING' 'ERROR'  'CRITICAL'\n",
    "\n",
    "# params = {\"niveau_log\": 'DEBUG'}\n",
    "# params = {\"niveau_log\": 'INFO'}\n",
    "# params = {\"niveau_log\": 'WARNING'}\n",
    "params = {\"niveau_log\": 'ERROR'}\n",
    "# params = {\"niveau_log\": 'CRITICAL'}\n",
    "reconfigurer_logging(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf31ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_clean_7col.copy()\n",
    "# Résumé des informations du dataframe\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d11634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPARATION DU DF\n",
    "df_clean_7col = df_clean_7col.copy()\n",
    "# renommage de la colonne avec ajout de 'vent'\n",
    "df_clean_7col.rename(columns={'vitesse_(km/h)': 'vitesse_vent_(km/h)'}, inplace=True)\n",
    "#formatage de la date pour être compatible\n",
    "df_clean_7col['date'] = pd.to_datetime(df_clean_7col['date'])\n",
    "# df_clean_12col['date'] = df_clean_12col['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "# transformation du df en dico afin d'être compatible dan sla fonction de prediction\n",
    "dataframes = {'df_clean_7col':df_clean_7col}\n",
    "\n",
    "# LANCEMENT FONCTION\n",
    "path_sauvegarde = path_sauve_modeles_mobilis\n",
    "\n",
    "liste_resultats,top3=lancement_prediction(dataframes, path_sauvegarde)\n",
    "# Meilleurs ensembles de paramètres pour le modèle: ElasticNet\n",
    "## RMSE: 1.252(km/h), MAE: 0.851, R2: 99.226%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 0.01, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "## RMSE: 1.252(km/h), MAE: 0.851, R2: 99.226%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 0.01, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "## RMSE: 1.252(km/h), MAE: 0.855, R2: 99.226%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__alpha': 0.01, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "\n",
    "# test:193 - df_clean_7col-->  RMSE: 1.327(km/h), MAE: 1.028(km/h), R2: 98.679%,\n",
    "# Meilleurs paramètres: {'model__alpha': 0.1, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "# hyperparametre: {'alpha': [0.01, 0.1, 0.5, 1.0], 'l1_ratio': [0.01, 0.1, 0.5, 0.9], 'fit_intercept': [True, False], 'max_iter': [100000]}\n",
    "# hyperparametre: {'alpha': [0.01, 0.1, 0.5, 1.0], 'l1_ratio': [0.01, 0.1, 0.5, 0.9], 'fit_intercept': [True, False], 'max_iter': [100000]}\n",
    "# test:199 - df_clean_7col-->  RMSE: 1.338(km/h), MAE: 1.044(km/h), R2: 98.658%,\n",
    "# Meilleurs paramètres: {'model__alpha': 0.1, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "# test:253 - df_clean_7col-->  RMSE: 1.302(km/h), MAE: 0.926(km/h), R2: 99.238%,\n",
    "# Meilleurs paramètres: {'model__alpha': 0.01, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "# hyperparametre: {'alpha': [0.01, 0.1, 0.5, 1.0], 'l1_ratio': [0.01, 0.1, 0.5, 0.9], 'fit_intercept': [True, False], 'max_iter': [100000]}\n",
    "# test:268 - df_clean_7col-->  RMSE: 1.303(km/h), MAE: 0.926(km/h), R2: 99.236%,\n",
    "# Meilleurs paramètres: {'model__alpha': 0.01, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "# hyperparametre: {'alpha': [0.01, 0.1, 0.5, 1.0], 'l1_ratio': [0.01, 0.1, 0.5, 0.9], 'fit_intercept': [True, False], 'max_iter': [100000]}\n",
    "# test:262 - df_clean_7col-->  RMSE: 1.302(km/h), MAE: 0.925(km/h), R2: 99.237%,\n",
    "# Meilleurs paramètres: {'model__alpha': 0.01, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "# hyperparametre: {'alpha': [0.01, 0.1, 0.5, 1.0], 'l1_ratio': [0.01, 0.1, 0.5, 0.9], 'fit_intercept': [True, False], 'max_iter': [100000]}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: LinearRegression\n",
    "## RMSE: 1.252(km/h), MAE: 0.852, R2: 99.226%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__fit_intercept': False}\n",
    "## RMSE: 1.252(km/h), MAE: 0.852, R2: 99.226%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__fit_intercept': False}\n",
    "## RMSE: 1.253(km/h), MAE: 0.856, R2: 99.226%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__fit_intercept': True}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: Ridge\n",
    "## RMSE: 1.252(km/h), MAE: 0.851, R2: 99.227%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 0.15, 'model__fit_intercept': True, 'model__solver': 'saga'}\n",
    "## RMSE: 1.252(km/h), MAE: 0.851, R2: 99.227%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 0.15, 'model__fit_intercept': True, 'model__solver': 'saga'}\n",
    "## RMSE: 1.252(km/h), MAE: 0.852, R2: 99.226%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__alpha': 0.05, 'model__fit_intercept': True, 'model__solver': 'saga'}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: Lasso\n",
    "## RMSE: 1.253(km/h), MAE: 0.853, R2: 99.225%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': True, 'model__precompute': True}\n",
    "## RMSE: 1.253(km/h), MAE: 0.853, R2: 99.225%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': True, 'model__precompute': True}\n",
    "## RMSE: 1.253(km/h), MAE: 0.853, R2: 99.225%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['direction_(°)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': True, 'model__precompute': True}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: ElasticNet\n",
    "## RMSE: 1.252(km/h), MAE: 0.851, R2: 99.226%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 0.01, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "## RMSE: 1.252(km/h), MAE: 0.851, R2: 99.226%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 0.01, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "## RMSE: 1.252(km/h), MAE: 0.855, R2: 99.226%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__alpha': 0.01, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: RandomForestRegressor\n",
    "## RMSE: 1.119(km/h), MAE: 0.668, R2: 99.059%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [6, 20], Colonnes supprimées: ['direction_(°)'], Paramètres: {'model__max_depth': 10, 'model__n_estimators': 100}\n",
    "## RMSE: 1.119(km/h), MAE: 0.668, R2: 99.059%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [6, 20], Colonnes supprimées: ['direction_(°)'], Paramètres: {'model__max_depth': 10, 'model__n_estimators': 100}\n",
    "## RMSE: 1.119(km/h), MAE: 0.668, R2: 99.059%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [6, 20], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)'], Paramètres: {'model__max_depth': 10, 'model__n_estimators': 100}\n",
    "\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: LinearRegression\n",
    "## RMSE: 1.252(km/h), MAE: 0.852, R2: 99.226%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__fit_intercept': False}\n",
    "## RMSE: 1.252(km/h), MAE: 0.852, R2: 99.226%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__fit_intercept': False}\n",
    "## RMSE: 1.253(km/h), MAE: 0.856, R2: 99.226%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__fit_intercept': True}\n",
    "\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: XGBRegressor\n",
    "## RMSE: 0.842(km/h), MAE: 0.531, R2: 99.479%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__learning_rate': 0.1, 'model__max_depth': 3}\n",
    "## RMSE: 0.842(km/h), MAE: 0.531, R2: 99.479%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [7, 19], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__learning_rate': 0.1, 'model__max_depth': 3}\n",
    "## RMSE: 0.845(km/h), MAE: 0.492, R2: 99.463%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [6, 20], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__learning_rate': 0.1, 'model__max_depth': 3}\n",
    "\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: KNeighborsRegressor\n",
    "## RMSE: 1.602(km/h), MAE: 1.168, R2: 98.733%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [7, 19], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__n_neighbors': 5, 'model__weights': 'distance'}\n",
    "## RMSE: 1.602(km/h), MAE: 1.200, R2: 98.733%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__n_neighbors': 5, 'model__weights': 'distance'}\n",
    "## RMSE: 1.602(km/h), MAE: 1.200, R2: 98.733%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [9, 17], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__n_neighbors': 5, 'model__weights': 'distance'}\n",
    "\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: GradientBoostingRegressor\n",
    "## RMSE: 0.648(km/h), MAE: 0.303, R2: 99.684%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [6, 20], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {}\n",
    "## RMSE: 0.648(km/h), MAE: 0.303, R2: 99.684%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [6, 20], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {}\n",
    "## RMSE: 0.685(km/h), MAE: 0.346, R2: 99.647%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [6, 20], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {}\n",
    "\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: DecisionTreeRegressor\n",
    "## RMSE: 0.778(km/h), MAE: 0.503, R2: 99.545%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [6, 20], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {}\n",
    "## RMSE: 0.778(km/h), MAE: 0.503, R2: 99.545%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [6, 20], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {}\n",
    "## RMSE: 0.799(km/h), MAE: 0.430, R2: 99.521%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['direction_(°)'], Paramètres: {}\n",
    "\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: Ridge\n",
    "## RMSE: 1.252(km/h), MAE: 0.851, R2: 99.227%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 0.15, 'model__fit_intercept': True, 'model__solver': 'saga'}\n",
    "## RMSE: 1.252(km/h), MAE: 0.851, R2: 99.227%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 0.15, 'model__fit_intercept': True, 'model__solver': 'saga'}\n",
    "## RMSE: 1.252(km/h), MAE: 0.852, R2: 99.226%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__alpha': 0.05, 'model__fit_intercept': True, 'model__solver': 'saga'}\n",
    "\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: Lasso\n",
    "## RMSE: 1.253(km/h), MAE: 0.853, R2: 99.225%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': True, 'model__precompute': True}\n",
    "## RMSE: 1.253(km/h), MAE: 0.853, R2: 99.225%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': True, 'model__precompute': True}\n",
    "## RMSE: 1.253(km/h), MAE: 0.853, R2: 99.225%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['direction_(°)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': True, 'model__precompute': True}\n",
    "\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: ElasticNet\n",
    "## RMSE: 1.252(km/h), MAE: 0.851, R2: 99.226%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 0.01, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "## RMSE: 1.252(km/h), MAE: 0.851, R2: 99.226%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 0.01, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "## RMSE: 1.252(km/h), MAE: 0.855, R2: 99.226%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__alpha': 0.01, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "\n",
    "\n",
    "# DIRECTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186d7a1e",
   "metadata": {},
   "source": [
    "### Prédictions sur Windsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ba193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_windsup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d407af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPARATION DU DF\n",
    "df_to_predict = df_windsup.copy()\n",
    "\n",
    "# # renommage de la colonne avec ajout de 'vent'\n",
    "# df_to_predict.rename(columns={'vitesse_(km/h)': 'vitesse_vent_(km/h)'}, inplace=True)\n",
    "# #formatage de la date pour être compatible\n",
    "# df_to_predict['date'] = pd.to_datetime(df_to_predict['date'])\n",
    "# # df_clean_12col['date'] = df_clean_12col['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# transformation du df en dico afin d'être compatible dan sla fonction de prediction\n",
    "dataframes = {'df_windsup': df_to_predict}\n",
    "\n",
    "# LANCEMENT FONCTION\n",
    "path_sauvegarde = path_dossier_windsup_renamed\n",
    "\n",
    "liste_resultats, top3 = lancement_prediction(dataframes, path_sauvegarde)\n",
    "# Meilleurs ensembles de paramètres pour le modèle: RandomForestRegressor\n",
    "## RMSE: 2.843(km/h), MAE: 0.478, R2: 99.056%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['direction_(°)'], Paramètres: {'model__max_depth': 10, 'model__n_estimators': 100}\n",
    "## RMSE: 2.843(km/h), MAE: 0.478, R2: 99.056%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['direction_(°)'], Paramètres: {'model__max_depth': 10, 'model__n_estimators': 100}\n",
    "## RMSE: 2.843(km/h), MAE: 0.478, R2: 99.056%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)'], Paramètres: {'model__max_depth': 10, 'model__n_estimators': 100}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: LinearRegression\n",
    "## RMSE: 4.566(km/h), MAE: 3.043, R2: 97.565%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {'model__fit_intercept': False}\n",
    "## RMSE: 4.566(km/h), MAE: 3.043, R2: 97.565%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__fit_intercept': False}\n",
    "## RMSE: 4.566(km/h), MAE: 3.043, R2: 97.565%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__fit_intercept': False}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: XGBRegressor\n",
    "## RMSE: 2.486(km/h), MAE: 0.384, R2: 99.278%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {'model__learning_rate': 0.1, 'model__max_depth': 3}\n",
    "## RMSE: 2.486(km/h), MAE: 0.384, R2: 99.278%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {'model__learning_rate': 0.1, 'model__max_depth': 3}\n",
    "## RMSE: 2.486(km/h), MAE: 0.384, R2: 99.278%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['direction_(°)'], Paramètres: {'model__learning_rate': 0.1, 'model__max_depth': 3}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: KNeighborsRegressor\n",
    "## RMSE: 4.427(km/h), MAE: 1.936, R2: 97.711%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['direction_(°)'], Paramètres: {'model__n_neighbors': 5, 'model__weights': 'distance'}\n",
    "## RMSE: 4.427(km/h), MAE: 1.936, R2: 97.711%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['direction_(°)'], Paramètres: {'model__n_neighbors': 5, 'model__weights': 'distance'}\n",
    "## RMSE: 4.427(km/h), MAE: 1.936, R2: 97.711%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)'], Paramètres: {'model__n_neighbors': 5, 'model__weights': 'distance'}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: GradientBoostingRegressor\n",
    "## RMSE: 2.423(km/h), MAE: 0.356, R2: 99.314%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['direction_(°)'], Paramètres: {}\n",
    "## RMSE: 2.423(km/h), MAE: 0.356, R2: 99.314%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['direction_(°)'], Paramètres: {}\n",
    "## RMSE: 2.423(km/h), MAE: 0.356, R2: 99.314%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)'], Paramètres: {}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: DecisionTreeRegressor\n",
    "## RMSE: 2.409(km/h), MAE: 0.364, R2: 99.322%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['direction_(°)'], Paramètres: {}\n",
    "## RMSE: 2.409(km/h), MAE: 0.364, R2: 99.322%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['direction_(°)'], Paramètres: {}\n",
    "## RMSE: 2.409(km/h), MAE: 0.364, R2: 99.322%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)'], Paramètres: {}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: Ridge\n",
    "## RMSE: 4.566(km/h), MAE: 3.046, R2: 97.565%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {'model__alpha': 0.05, 'model__fit_intercept': True, 'model__solver': 'sparse_cg'}\n",
    "## RMSE: 4.566(km/h), MAE: 3.046, R2: 97.565%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__alpha': 0.05, 'model__fit_intercept': True, 'model__solver': 'sparse_cg'}\n",
    "## RMSE: 4.566(km/h), MAE: 3.046, R2: 97.565%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 0.05, 'model__fit_intercept': True, 'model__solver': 'sparse_cg'}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: Lasso\n",
    "## RMSE: 4.568(km/h), MAE: 3.043, R2: 97.563%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['direction_(°)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': False, 'model__precompute': True}\n",
    "## RMSE: 4.568(km/h), MAE: 3.043, R2: 97.563%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': False, 'model__precompute': True}\n",
    "## RMSE: 4.568(km/h), MAE: 3.043, R2: 97.563%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': False, 'model__precompute': True}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: ElasticNet\n",
    "## RMSE: 4.564(km/h), MAE: 3.040, R2: 97.567%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "## RMSE: 4.564(km/h), MAE: 3.040, R2: 97.567%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "## RMSE: 4.564(km/h), MAE: 3.040, R2: 97.567%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "\n",
    "#-----------------------------\n",
    "# DIRECTION\n",
    "#-----------------------------\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: RandomForestRegressor\n",
    "## RMSE: 0.421(°), MAE: 0.316, R2: 99.892%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {'model__max_depth': 10, 'model__n_estimators': 200}\n",
    "## RMSE: 0.421(°), MAE: 0.316, R2: 99.892%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: [], Paramètres: {'model__max_depth': 10, 'model__n_estimators': 200}\n",
    "## RMSE: 0.421(°), MAE: 0.316, R2: 99.892%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [7, 19], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {'model__max_depth': 10, 'model__n_estimators': 200}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: LinearRegression\n",
    "## RMSE: 3.201(°), MAE: 2.304, R2: 94.046%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__fit_intercept': True}\n",
    "## RMSE: 3.201(°), MAE: 2.304, R2: 94.046%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [7, 19], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__fit_intercept': True}\n",
    "## RMSE: 3.288(°), MAE: 2.429, R2: 93.719%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__fit_intercept': False}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: XGBRegressor\n",
    "## RMSE: 0.591(°), MAE: 0.459, R2: 99.788%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {'model__learning_rate': 0.1, 'model__max_depth': 7}\n",
    "## RMSE: 0.591(°), MAE: 0.459, R2: 99.788%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: [], Paramètres: {'model__learning_rate': 0.1, 'model__max_depth': 7}\n",
    "## RMSE: 0.610(°), MAE: 0.461, R2: 99.774%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__learning_rate': 0.1, 'model__max_depth': 7}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: KNeighborsRegressor\n",
    "## RMSE: 4.866(°), MAE: 4.177, R2: 89.508%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [0, 23], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__n_neighbors': 10, 'model__weights': 'distance'}\n",
    "## RMSE: 5.200(°), MAE: 4.271, R2: 88.019%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [0, 23], Colonnes supprimées: ['direction_(°)'], Paramètres: {'model__n_neighbors': 5, 'model__weights': 'distance'}\n",
    "## RMSE: 5.200(°), MAE: 4.271, R2: 88.019%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [0, 23], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)'], Paramètres: {'model__n_neighbors': 5, 'model__weights': 'distance'}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: GradientBoostingRegressor\n",
    "## RMSE: 0.578(°), MAE: 0.485, R2: 99.798%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {}\n",
    "## RMSE: 0.578(°), MAE: 0.485, R2: 99.798%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [7, 19], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {}\n",
    "## RMSE: 0.609(°), MAE: 0.492, R2: 99.775%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: DecisionTreeRegressor\n",
    "## RMSE: 0.461(°), MAE: 0.300, R2: 99.871%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {}\n",
    "## RMSE: 0.510(°), MAE: 0.418, R2: 99.861%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [6, 20], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {}\n",
    "## RMSE: 0.510(°), MAE: 0.418, R2: 99.861%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [6, 20], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: Ridge\n",
    "## RMSE: 2.649(°), MAE: 2.004, R2: 95.923%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__alpha': 0.15, 'model__fit_intercept': False, 'model__solver': 'sparse_cg'}\n",
    "## RMSE: 2.649(°), MAE: 2.004, R2: 95.923%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [7, 19], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__alpha': 0.15, 'model__fit_intercept': False, 'model__solver': 'sparse_cg'}\n",
    "## RMSE: 2.676(°), MAE: 2.027, R2: 95.841%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 0.15, 'model__fit_intercept': False, 'model__solver': 'sparse_cg'}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: Lasso\n",
    "## RMSE: 2.839(°), MAE: 2.133, R2: 95.319%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__alpha': 1.0, 'model__fit_intercept': True, 'model__precompute': True}\n",
    "## RMSE: 2.839(°), MAE: 2.133, R2: 95.319%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [7, 19], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__alpha': 1.0, 'model__fit_intercept': True, 'model__precompute': True}\n",
    "## RMSE: 2.894(°), MAE: 2.205, R2: 95.133%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__alpha': 1.0, 'model__fit_intercept': True, 'model__precompute': True}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: ElasticNet\n",
    "## RMSE: 2.846(°), MAE: 2.138, R2: 95.296%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__alpha': 1.0, 'model__fit_intercept': True, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "## RMSE: 2.846(°), MAE: 2.138, R2: 95.296%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [7, 19], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__alpha': 1.0, 'model__fit_intercept': True, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n",
    "## RMSE: 3.048(°), MAE: 2.238, R2: 95.885%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [0, 23], Colonnes supprimées: ['direction_(°)'], Paramètres: {'model__alpha': 1.0, 'model__fit_intercept': False, 'model__l1_ratio': 0.9, 'model__max_iter': 100000}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad71442",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# liste_resultats\n",
    "# resultats_meteo_france_6min_all_stations_vent_SVR = liste_resultats\n",
    "resultats_mobilis_all_modeles_vent_direction = liste_resultats\n",
    "# 1100 min  pour obtenir les resultats RandomForestRegressor, LinearRegression, XGBRegressor\n",
    "# 600 min pour le test2 RandomForestRegressor, LinearRegression, XGBRegressor\n",
    "# 266 min pour Ridge\n",
    "# 80 min pour Lasso\n",
    "# 113 min pour elasticnet\n",
    "# 74 min pour DTR\n",
    "# 201 min pour GBR\n",
    "# SVR arret forcé au bout de 2300 min !, les rmse en sont pas bon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616b6b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultats_meteo_france_6min_all_stations_vent_all_modeles=resultats_meteo_france_6min_all_stations_vent\n",
    "# resultats_meteo_france_6min_all_stations_vent_all_modeles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f7a93",
   "metadata": {},
   "source": [
    "### utilisation d'un profiler de charge CPU et GPU line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Installez d'abord le module line_profiler (si ce n'est pas déjà fait)\n",
    "# !pip install line_profiler\n",
    "\n",
    "# # Importez le décorateur\n",
    "# from line_profiler import LineProfiler\n",
    "\n",
    "# # Définissez votre fonction à profiler\n",
    "# def my_function(numbers):\n",
    "#     s = sum(numbers)\n",
    "#     l = [numbers[i] / 43 for i in range(len(numbers))]\n",
    "#     m = ['hello' + str(numbers[i]) for i in range(len(numbers))]\n",
    "\n",
    "# # Créez un profil line_profiler\n",
    "# lp = LineProfiler()\n",
    "# lp_wrapper = lp(my_function)\n",
    "\n",
    "# # Exécutez votre fonction avec les arguments souhaités\n",
    "# numbers = [random.randint(1, 100) for i in range(1000)]\n",
    "# lp_wrapper(numbers)\n",
    "\n",
    "# # Affichez les statistiques de profil\n",
    "# lp.print_stats()\n",
    "# #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d417e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# def top3_resultats_modeles_par_modele(resultats):\n",
    "#     \"\"\"\n",
    "#     Affiche les trois meilleurs ensembles de configurations pour chaque modèle utilisé,\n",
    "#     basés sur le score RMSE moyen le plus bas, incluant également les scores MAE et R² ainsi que\n",
    "#     les paramètres et hyperparamètres pour chaque ensemble, avec le nom de la station météo.\n",
    "#     \"\"\"\n",
    "#     # Regroupement des résultats par modèle\n",
    "#     resultats_par_modele = defaultdict(list)\n",
    "#     for resultat in resultats:\n",
    "#         for res in resultat:\n",
    "#             modele = res['model']\n",
    "#             resultats_par_modele[modele].append(res)\n",
    "\n",
    "#     # Pour chaque modèle, sélectionner les 3 meilleures configurations basées sur le RMSE,\n",
    "#     # et en cas d'égalité sur RMSE, utiliser MAE comme critère secondaire (favoriser le plus petit MAE)\n",
    "#     for modele, res_modele in resultats_par_modele.items():\n",
    "#         # Trier les configurations d'abord par RMSE croissant puis par MAE croissant\n",
    "#         top3 = sorted(res_modele, key=lambda x: (x['rmse'], x['mae'],-x['r2']))[:3]\n",
    "\n",
    "#         print(f\"# Meilleurs ensembles de paramètres pour le modèle: {modele}\")\n",
    "#         for config in top3:\n",
    "#             # Inclure le nom de la station météo dans les détails\n",
    "#             print(f\"## RMSE: {config['rmse']:.3f}, MAE: {config['mae']:.3f}, R2: {config['r2']*100:.3f}%, \"\n",
    "#                   f\"Détails: Station: {config['df_name']}, Plage mensuelle: {config['plage_mensuelle']}, \"\n",
    "#                   f\"Plage horaire: {config['plage_horaire']}, Colonnes supprimées: {config['colonnes_supprimees']}, \"\n",
    "#                   f\"Paramètres: {config['meilleurs_parametres']}\")\n",
    "#         print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84caa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultats_meteo_france_6min_all_stations_vent_all_modeles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55139df",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "# top3_resultats_modeles_par_modele(resultats)\n",
    "# liste_resultats\n",
    "liste_rmse=[resultat['rmse'] for resultat in resultats ]\n",
    "# print(f\"\\n liste_rmse:\\n{liste_rmse} \\n\")\n",
    "min(liste_rmse)\n",
    "top5=sorted(liste_rmse)\n",
    "# print(f\"\\n top5:\\n{top5} \\n\")\n",
    "top5=sorted(liste_rmse)[-5:]\n",
    "# print(f\"\\n top5:\\n{top5} \\n\")\n",
    "top3_rmse=heapq.nsmallest(5,liste_rmse)\n",
    "print(f\"\\n top3_rmse:\\n{top3_rmse} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c4e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_meteo_france_6min_all_stations_direction_Lasso=resultats_meteo_france_6min_all_stations_vent_and_direction_Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72738400",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_meteo_france_6min_all_stations_vent_ElasticNet=liste_resultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5619902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_resultats_modeles_par_modele(resultats_meteo_france_6min_all_stations_vent_ElasticNet)\n",
    "# Meilleurs ensembles de paramètres pour le modèle: ElasticNet\n",
    "## RMSE: 0.806, MAE: 0.626, R2: 98.078%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__alpha': 0.05, 'model__fit_intercept': True, 'model__l1_ratio': 0.05}\n",
    "## RMSE: 0.807, MAE: 0.627, R2: 98.072%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__alpha': 0.05, 'model__fit_intercept': True, 'model__l1_ratio': 0.05}\n",
    "## RMSE: 0.807, MAE: 0.627, R2: 98.070%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {'model__alpha': 0.05, 'model__fit_intercept': True, 'model__l1_ratio': 0.05}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: ElasticNet\n",
    "## RMSE: 0.806, MAE: 0.626, R2: 98.078%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__alpha': 0.05, 'model__fit_intercept': True, 'model__l1_ratio': 0.05}\n",
    "## RMSE: 0.807, MAE: 0.627, R2: 98.072%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__alpha': 0.05, 'model__fit_intercept': True, 'model__l1_ratio': 0.05}\n",
    "## RMSE: 0.807, MAE: 0.627, R2: 98.070%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {'model__alpha': 0.05, 'model__fit_intercept': True, 'model__l1_ratio': 0.05}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a446a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_resultats_modeles_par_modele(resultats_meteo_france_6min_all_stations_direction_Lasso)\n",
    "# Meilleurs ensembles de paramètres pour le modèle: Lasso\n",
    "## RMSE: 3.929, MAE: 2.822, R2: 97.847%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__alpha': 1.0, 'model__fit_intercept': False, 'model__precompute': True}\n",
    "## RMSE: 3.929, MAE: 2.823, R2: 97.847%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__alpha': 1.0, 'model__fit_intercept': False, 'model__precompute': True}\n",
    "## RMSE: 3.929, MAE: 2.823, R2: 97.847%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {'model__alpha': 1.0, 'model__fit_intercept': False, 'model__precompute': True}\n",
    "top3_resultats_modeles_par_modele(resultats_meteo_france_6min_all_stations_vent_Lasso)\n",
    "# Meilleurs ensembles de paramètres pour le modèle: Lasso\n",
    "## RMSE: 0.842, MAE: 0.651, R2: 97.900%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': False, 'model__precompute': True}\n",
    "## RMSE: 0.843, MAE: 0.652, R2: 97.896%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['direction_(°)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': False, 'model__precompute': True}\n",
    "## RMSE: 0.843, MAE: 0.652, R2: 97.896%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': False, 'model__precompute': True}\n",
    "\n",
    "top3_resultats_modeles_par_modele(resultats_meteo_france_6min_all_stations_vent_Dtr)\n",
    "# Meilleurs ensembles de paramètres pour le modèle: DecisionTreeRegressor\n",
    "## RMSE: 1.409, MAE: 1.130, R2: 83.485%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['humidity_(%)'], Paramètres: {}\n",
    "## RMSE: 1.433, MAE: 1.111, R2: 93.864%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['direction_(°)'], Paramètres: {}\n",
    "## RMSE: 1.463, MAE: 1.146, R2: 93.601%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)'], Paramètres: {}\n",
    "\n",
    "top3_resultats_modeles_par_modele(resultats_meteo_france_6min_all_stations_vent_Ridge)\n",
    "# Meilleurs ensembles de paramètres pour le modèle: Ridge\n",
    "## RMSE: 0.799, MAE: 0.620, R2: 98.109%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': True, 'model__solver': 'sparse_cg'}\n",
    "## RMSE: 0.799, MAE: 0.621, R2: 98.107%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': False, 'model__solver': 'auto'}\n",
    "## RMSE: 0.801, MAE: 0.621, R2: 98.102%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: [], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': True, 'model__solver': 'sparse_cg'}\n",
    "\n",
    "top3_resultats_modeles_par_modele(resultats_meteo_france_6min_all_stations_vent_ElasticNet)\n",
    "# Meilleurs ensembles de paramètres pour le modèle: ElasticNet\n",
    "## RMSE: 0.814, MAE: 0.631, R2: 98.036%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': False, 'model__l1_ratio': 0.1}\n",
    "## RMSE: 0.815, MAE: 0.632, R2: 98.034%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': False, 'model__l1_ratio': 0.1}\n",
    "## RMSE: 0.816, MAE: 0.633, R2: 98.026%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {'model__alpha': 0.1, 'model__fit_intercept': True, 'model__l1_ratio': 0.1}\n",
    "\n",
    "top3_resultats_modeles_par_modele(resultats_meteo_france_6min_all_stations_vent_GBR)\n",
    "# Meilleurs ensembles de paramètres pour le modèle: GradientBoostingRegressor\n",
    "## RMSE: 0.966, MAE: 0.740, R2: 90.739%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['humidity_(%)'], Paramètres: {}\n",
    "## RMSE: 0.971, MAE: 0.749, R2: 90.651%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {}\n",
    "## RMSE: 0.981, MAE: 0.754, R2: 90.460%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['direction_(°)'], Paramètres: {}\n",
    "\n",
    "top3_resultats_modeles_par_modele(resultats_meteo_france_6min_all_stations_vent_et_direction_SVR)\n",
    "# test:106 - df_station_13005003-->  RMSE: 9.775(°), MAE: 6.767(°), R2: 95.15%,\n",
    "# Meilleurs paramètres: {'model__C': 10, 'model__epsilon': 0.2, 'model__kernel': 'rbf'}\n",
    "# test:112 - df_station_13005003-->  RMSE: 9.795(°), MAE: 6.779(°), R2: 95.13%,\n",
    "# Meilleurs paramètres: {'model__C': 10, 'model__epsilon': 0.2, 'model__kernel': 'rbf'}\n",
    "# test:115 - df_station_13005003-->  RMSE: 9.791(°), MAE: 6.782(°), R2: 95.134%,\n",
    "# Meilleurs paramètres: {'model__C': 10, 'model__epsilon': 0.2, 'model__kernel': 'rbf'}\n",
    "# test:124 - df_station_13005003-->  RMSE: 9.803(°), MAE: 6.797(°), R2: 95.122%,\n",
    "# Meilleurs paramètres: {'model__C': 10, 'model__epsilon': 0.2, 'model__kernel': 'rbf'}\n",
    "\n",
    "top3_resultats_modeles_par_modele(resultats_meteo_france_6min_all_stations_vent_kneighbors)\n",
    "# Meilleurs ensembles de paramètres pour le modèle: resultats_meteo_france_6min_all_stations_vent_all_modeles2\n",
    "## RMSE: 1.078, MAE: 0.821, R2: 88.470%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__n_neighbors': 10, 'model__weights': 'distance'}\n",
    "## RMSE: 1.175, MAE: 0.939, R2: 95.685%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__n_neighbors': 5, 'model__weights': 'distance'}\n",
    "## RMSE: 1.198, MAE: 0.889, R2: 88.059%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__n_neighbors': 5, 'model__weights': 'distance'}\n",
    "\n",
    "top3_resultats_modeles_par_modele(resultats_meteo_france_6min_all_stations_vent_all_modeles)\n",
    "# Meilleurs ensembles de paramètres pour le modèle: RandomForestRegressor\n",
    "## RMSE: 1.109, MAE: 0.877, R2: 98.957%, Détails: Station: df_station_13054001, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [9, 17], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {}\n",
    "## RMSE: 1.124, MAE: 0.887, R2: 98.929%, Détails: Station: df_station_13054001, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [9, 17], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {}\n",
    "## RMSE: 1.126, MAE: 0.897, R2: 98.926%, Détails: Station: df_station_13054001, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [9, 17], Colonnes supprimées: [], Paramètres: {}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: LinearRegression\n",
    "## RMSE: 0.851, MAE: 0.660, R2: 98.435%, Détails: Station: df_station_13074003, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['humidity_(%)'], Paramètres: {}\n",
    "## RMSE: 0.851, MAE: 0.660, R2: 98.435%, Détails: Station: df_station_13074003, Plage mensuelle: [7, 8], Plage horaire: [9, 17], Colonnes supprimées: ['humidity_(%)'], Paramètres: {}\n",
    "## RMSE: 0.851, MAE: 0.654, R2: 98.435%, Détails: Station: df_station_13074003, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: XGBRegressor\n",
    "## RMSE: 1.180, MAE: 0.924, R2: 94.653%, Détails: Station: df_station_13091002, Plage mensuelle: [5, 6, 7, 8, 9, 10], Plage horaire: [9, 17], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {}\n",
    "## RMSE: 1.191, MAE: 0.929, R2: 94.552%, Détails: Station: df_station_13091002, Plage mensuelle: [5, 6, 7, 8, 9, 10], Plage horaire: [9, 17], Colonnes supprimées: ['direction_(°)'], Paramètres: {}\n",
    "## RMSE: 1.199, MAE: 0.916, R2: 98.781%, Détails: Station: df_station_13054001, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [9, 17], Colonnes supprimées: ['direction_(°)'], Paramètres: {}\n",
    "\n",
    "#test du 10 avril 17h\n",
    "# Meilleurs ensembles de paramètres pour le modèle: RandomForestRegressor\n",
    "## RMSE: 0.977, MAE: 0.754, R2: 90.538%, Détails: Station: df_station_13022003, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {'model__max_depth': 10, 'model__n_estimators': 200}\n",
    "## RMSE: 0.978, MAE: 0.752, R2: 90.513%, Détails: Station: df_station_13022003, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: [], Paramètres: {'model__max_depth': 10, 'model__n_estimators': 200}\n",
    "## RMSE: 0.983, MAE: 0.751, R2: 90.416%, Détails: Station: df_station_13022003, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__max_depth': 10, 'model__n_estimators': 200}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: LinearRegression\n",
    "## RMSE: 0.799, MAE: 0.620, R2: 98.110%, Détails: Station: df_station_13074003, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__fit_intercept': False}\n",
    "## RMSE: 0.799, MAE: 0.621, R2: 98.107%, Détails: Station: df_station_13074003, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__fit_intercept': True}\n",
    "## RMSE: 0.800, MAE: 0.621, R2: 98.105%, Détails: Station: df_station_13074003, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: [], Paramètres: {'model__fit_intercept': False}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: XGBRegressor\n",
    "## RMSE: 0.990, MAE: 0.769, R2: 90.282%, Détails: Station: df_station_13022003, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: [], Paramètres: {}\n",
    "## RMSE: 1.040, MAE: 0.798, R2: 89.281%, Détails: Station: df_station_13022003, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {}\n",
    "## RMSE: 1.046, MAE: 0.790, R2: 89.138%, Détails: Station: df_station_13022003, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {}\n",
    "\n",
    "# autre methode pour faire le trie ds résultats\n",
    "# Meilleurs ensembles de paramètres pour le modèle: RandomForestRegressor\n",
    "## RMSE: 0.977, MAE: 0.754, R2: 90.538%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {'model__max_depth': 10, 'model__n_estimators': 200}\n",
    "## RMSE: 0.978, MAE: 0.752, R2: 90.513%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: [], Paramètres: {'model__max_depth': 10, 'model__n_estimators': 200}\n",
    "## RMSE: 0.983, MAE: 0.751, R2: 90.416%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {'model__max_depth': 10, 'model__n_estimators': 200}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: LinearRegression\n",
    "## RMSE: 0.799, MAE: 0.620, R2: 98.110%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['humidity_(%)'], Paramètres: {'model__fit_intercept': False}\n",
    "## RMSE: 0.799, MAE: 0.621, R2: 98.107%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)', 'direction_(°)', 'humidity_(%)', 'temperature_(°C)'], Paramètres: {'model__fit_intercept': True}\n",
    "## RMSE: 0.800, MAE: 0.621, R2: 98.105%, Détails: Station: N/A, Plage mensuelle: [7, 8], Plage horaire: [8, 18], Colonnes supprimées: [], Paramètres: {'model__fit_intercept': False}\n",
    "\n",
    "# Meilleurs ensembles de paramètres pour le modèle: XGBRegressor\n",
    "## RMSE: 0.990, MAE: 0.769, R2: 90.282%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: [], Paramètres: {}\n",
    "## RMSE: 1.040, MAE: 0.798, R2: 89.281%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['temperature_(°C)'], Paramètres: {}\n",
    "## RMSE: 1.046, MAE: 0.790, R2: 89.138%, Détails: Station: N/A, Plage mensuelle: [6, 7, 8, 9], Plage horaire: [8, 18], Colonnes supprimées: ['précipitations_(mm)'], Paramètres: {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc1546",
   "metadata": {},
   "source": [
    "##### sauvegarde des resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path=path_sauve_modeles_meteo_france_6min\n",
    "# liste_resultats\n",
    "# liste_model = [\n",
    "#     liste_resultats2, resultats, resultats_meteo_france_6min_all_stations, resultats_meteo_france_6min_all_stations_vent,\n",
    "#     resultats_meteo_france_6min_all_stations_direction\n",
    "# ]\n",
    "# liste_name_model = [\n",
    "#     \"liste_resultats2\", \"resultats\", \"resultats_meteo_france_6min_all_stations\", \"resultats_meteo_france_6min_all_stations_vent\",\n",
    "#     \"resultats_meteo_france_6min_all_stations_direction\"\n",
    "# ]\n",
    "# for model in liste_model:\n",
    "#     # logger.debug(f\"\\n model:\\n{model} \")\n",
    "#     for model_name in liste_name_model:\n",
    "#         # logger.debug(f\"\\n model_name:\\n{model_name} \")\n",
    "#         model=pd.DataFrame(model)\n",
    "#         # Correction et construction des chemins de fichier\n",
    "#         file_basename = os.path.join(path, f\"{model_name}\").replace('\\\\', '/')\n",
    "#         # Créer le dossier si nécessaire\n",
    "#         # os.makedirs(os.path.dirname(file_basename), exist_ok=True)\n",
    "#         model.to_pickle(f\"{file_basename}.pkl\")\n",
    "#         # save_dataframe(model, model_name,path)\n",
    "#         logger.info(f\"\\n le model {model.info()} est sauvegardé sous le nom \\n{model_name} dans le repertoire \\n{path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8184c96f",
   "metadata": {},
   "source": [
    "##### charge les resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa61046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = path_sauve_modeles_meteo_france_6min\n",
    "\n",
    "\n",
    "# liste_name_model = [\n",
    "#     \"liste_resultats2\", \"resultats\", \"resultats_meteo_france_6min_all_stations\", \"resultats_meteo_france_6min_all_stations_vent\",\n",
    "#     \"resultats_meteo_france_6min_all_stations_direction\"\n",
    "# ]\n",
    "\n",
    "# for model_name in liste_name_model:\n",
    "#     # logger.debug(f\"\\n model_name:\\n{model_name} \")\n",
    "#     model = pd.DataFrame(model)\n",
    "#     # Correction et construction des chemins de fichier\n",
    "#     file_basename = os.path.join(path, f\"{model_name}.pkl\").replace('\\\\', '/')\n",
    "#     # logger.debug(f\"\\n file_basename:\\n{file_basename} \")\n",
    "#     model_name = pd.read_pickle(file_basename)\n",
    "#     logger.info(f\"\\n le model {model.info()} est chargé sous le nom \\n{model_name} depuis le repertoire \\n{path}\")\n",
    "# # projet_meteo\\Projet_Meteo\\Modeles\\sauvegarde_modeles\\model_meteo_france_6min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b3667",
   "metadata": {},
   "source": [
    "#####  sauve charge model pickle avec une seule fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1075d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_meteo_france_6min_all_stations_vent_Lasso=resultats_meteo_france_6min_all_stations_vent_and_direction_Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d9807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sauve_charge_df_csv_json_pkl import sauve_charge_pickle\n",
    "\n",
    "# sauve résultats modele\n",
    "path = path_sauve_modeles_meteo_france_6min\n",
    "model_name = \"resultats_meteo_france_6min_all_stations_vent_Lasso\"\n",
    "path = os.path.join(path, f\"{model_name}.pkl\").replace('\\\\', '/')\n",
    "name_file = resultats_meteo_france_6min_all_stations_vent_Lasso\n",
    "sauve_charge_pickle(path, name_file=name_file, true_for_load=False, is_file=True)\n",
    "\n",
    "#charge tous les modeles du repertoire\n",
    "path = path_sauve_modeles_meteo_france_6min\n",
    "# sauve_charge_pickle(path, name_file=None,true_for_load=True, is_file=False)\n",
    "# print(path)\n",
    "\n",
    "#charge un modele\n",
    "model_name = \"resultats_meteo_france_6min_all_stations_vent_and_direction_Lasso\"\n",
    "path = os.path.join(path, f\"{model_name}.pkl\").replace('\\\\', '/')\n",
    "\n",
    "resultats_meteo_france_6min_all_stations_vent_and_direction_Lasso=sauve_charge_pickle(path, name_file=None,true_for_load=True, is_file=True)\n",
    "\n",
    "# print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95298216",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_meteo_france_6min_all_stations_vent.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018575a8",
   "metadata": {},
   "source": [
    "## SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee7e4d",
   "metadata": {},
   "source": [
    "###  Prunage ACP des df\n",
    "\n",
    "Le prunage, est une technique pour optimiser les modèles d'arbres de décision. L'Analyse en Composantes Principales (ACP), en revanche, est une méthode statistique qui transforme les données en composantes orthogonales de manière à réduire le nombre de variables et à mettre en évidence les plus significatives, utilisée pour l'analyse de données et la visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844b038",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d442bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_france_13009 = load_dataframe(\"01-Jan-2004_at_00h00_to_31-Dec-2004_at_23h59_1\", path_data_meteo_france_upload_data_depuis_api, format_type='csv', sep=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_france_13009\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "\n",
    "def prunage_df(data, colonne='vitesse_(km/h)', acp_component=2, k=6):\n",
    "    \"\"\"\n",
    "    Applique PCA pour réduire la dimensionnalité des données et utilise SelectKBest pour identifier les caractéristiques les plus informatives.\n",
    "    Applique PCA et SelectKBest pour réduire la dimensionnalité des données\n",
    "    et comparer les méthodes basées sur la variance expliquée et les scores ANOVA F-value.\n",
    "\n",
    "\n",
    "    :param data: DataFrame contenant les données.\n",
    "    :param colonne: Nom de la colonne cible pour SelectKBest.\n",
    "    :k nombre de k paramétres pour selectkBest\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    selected_features = []  # Initialisez à une liste vide\n",
    "    scores = []  # Initialisez à une liste vide\n",
    "    # faire une copie du dataframe\n",
    "    data=data.copy()\n",
    "    try:\n",
    "        # Liste des colonnes à supprimer\n",
    "        cols_to_drop = ['station', 'POSTE', 'DATE']\n",
    "        # Supprimer les colonnes de la liste si elles existent dans le DataFrame\n",
    "        data.drop(columns=[col for col in cols_to_drop if col in data.columns], inplace=True)\n",
    "\n",
    "        # Vérifier si 'date' est dans les colonnes et la convertir en datetime\n",
    "        if 'date' in data.columns:\n",
    "            data['date'] = pd.to_datetime(data['date'])\n",
    "            data.set_index('date', inplace=True)\n",
    "        logger.debug(f\" colonnes aprés suppresiions {df.columns.tolist()}\")\n",
    "\n",
    "        # Ajuster n_components pour PCA et k pour SelectKBest selon le nombre de colonnes\n",
    "        n_cols = data.shape[1] - 1  # Exclure la colonne cible\n",
    "        n_component_max = min(n_cols, 10)  # PCA n_components ne doit pas dépasser le nombre de colonnes\n",
    "        n_component_min=2\n",
    "        n_component = max(n_component_min, min(acp_component, n_component_max))\n",
    "        logger.debug(f\"n_component :{n_component}\")\n",
    "\n",
    "\n",
    "        k_min = 2  # SelectKBest k  doit avoir une taille mini de 2\n",
    "        k_max = min(k, n_cols - 1)  # SelectKBest k ne doit pas dépasser le nombre de colonnes - 1\n",
    "        # k=k_min if k < k_min else k\n",
    "        # k=k_max if k> k_max else k\n",
    "        k = max(k_min, min(k, k_max))\n",
    "        logger.debug(f\"k :{k}\")\n",
    "        # ACP\n",
    "        # Suppression de la colonne cible des données pour PCA\n",
    "        data_for_pca = data.drop(columns=[colonne])\n",
    "\n",
    "        # Création et ajustement du modèle PCA\n",
    "        pca = PCA(n_components=n_component)\n",
    "        pca.fit(data_for_pca)\n",
    "        pca_variance_explained = pca.explained_variance_ratio_\n",
    "        # Affichage des résultats\n",
    "        variance_list_ACP = []\n",
    "        logger.info(\"PCA Variance Explained:\")\n",
    "        for i, variance in enumerate(pca_variance_explained, 1):\n",
    "            variance_list_ACP.append(variance)\n",
    "            logger.info(f\"PC{i}: {variance:.4f}\")\n",
    "\n",
    "        # SELECTKBEST\n",
    "        # Préparation des données pour SelectKBest\n",
    "        X = data.drop(columns=[colonne])\n",
    "        y = data[colonne]\n",
    "\n",
    "        # Calcul des scores ANOVA F-value avec SelectKBest\n",
    "        kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "        kbest.fit(X, y)\n",
    "\n",
    "        # Affichage des noms des colonnes des caractéristiques sélectionnées\n",
    "        selected_features = X.columns[kbest.get_support()]\n",
    "        scores = kbest.scores_[kbest.get_support()].tolist()\n",
    "\n",
    "        # logger.debug(\"Noms des features sélectionnées :\", selected_features.tolist())\n",
    "        logger.debug(f\"Noms des features sélectionnées :{selected_features}\" )\n",
    "\n",
    "\n",
    "        # Affichage des scores d'information pour les caractéristiques sélectionnées\n",
    "        logger.debug(f\"Scores d'information pour les features sélectionnées : {scores}\" )\n",
    "        # logger.debug(\"Scores d'information pour les features sélectionnées :\", kbest.scores_[kbest.get_support()].tolist())\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Une erreur est survenue : {e}\")\n",
    "        # Ici, retourner selected_features et scores même s'ils sont vides\n",
    "        # assure que la fonction se termine proprement sans erreurs de référence non définie\n",
    "        return selected_features, scores\n",
    "    # liste_col = selected_features.tolist()\n",
    "\n",
    "    return variance_list_ACP,selected_features, scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541348c3",
   "metadata": {},
   "source": [
    "### prunage de mobilis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a3f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test\n",
    "\n",
    "data=df_clean_7col\n",
    "# Noms des features sélectionnées : ['humidity_(%)', 'temperature_(°C)', 'direction_(°)', 'rafale_(km/h)']\n",
    "# Scores d'information pour les features sélectionnées : [2.3274957137516807, 1.7501298686799882, 2.052926112796495, 382.22239124091436]\n",
    "data=df_clean_12col\n",
    "# Noms des features sélectionnées : ['wave_amplitude_(m)', 'direction_(°)', 'rafale_(km/h)', 'vitesse_surface_(km/h)']\n",
    "# Scores d'information pour les features sélectionnées : [3.6220416671023536, 1.7338180629993725, 199.59388346579624, 1.7723010749662798]\n",
    "\n",
    "\n",
    "# data=\"C:\\programmation\\IA\\Projet_Meteo\\projet_meteo\\Projet_Meteo\\Datasets\\meteo_france\\upload_dataset_depuis_api\\13001009\\horaire\\01-Jan-2004_at_00h00_to_31-Dec-2004_at_23h59\\01-Jan-2004_at_00h00_to_31-Dec-2004_at_23h59_1.csv\"\n",
    "# data=df_clean_7col\n",
    "data=df\n",
    "prunage_df(data,colonne='vitesse_(km/h)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc9e32",
   "metadata": {},
   "source": [
    "### prunage meteo france"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae2c17",
   "metadata": {},
   "source": [
    "traite les colonnes du df de meteo france, en faisant une colonne moyenne pour la vitesse, direction, temeprature, humidité\n",
    "\n",
    "Supprime la colonne station, mais la date au format datetime et en index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f3d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_13005003_01jan2014_01mar2024.copy()\n",
    "# Convertir la colonne 'date' en datetime si ce n'est pas déjà fait\n",
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "# Définir la colonne 'date' comme nouvel index du DataFrame\n",
    "# df.set_index('date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35919aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df = df_13005003_01jan2014_01mar2024.copy()\n",
    "# Convertir la colonne 'date' en datetime si ce n'est pas déjà fait\n",
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "# Définir la colonne 'date' comme nouvel index du DataFrame\n",
    "# df.set_index('date', inplace=True)\n",
    "\n",
    "df.columns.tolist()\n",
    "logger.debug(f\"colonnes  {df.columns.tolist()}\")\n",
    "# df.head(2)\n",
    "# df.info()\n",
    "df = df.dropna()\n",
    "# df.info()\n",
    "\n",
    "cols_num = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# ______________________________\n",
    "#test avec les colonnes moyennées\n",
    "# ______________________________\n",
    "# # La colonne d'intérêt\n",
    "col_interet = 'vitesse_vent_moyen_at_1_m_(km/h)'\n",
    "#on ne garde qu'une seule colonne vitesse, celle à predire\n",
    "# retirer les autres colonnes de vitesse du vent\n",
    "cols_a_conserver = [col for col in cols_num if \"vitesse\" not in col or col == col_interet]\n",
    "logger.debug(f\"cols_a_conserver  {cols_a_conserver}\")\n",
    "# lancement fonction\n",
    "variance_list_ACP1, selected_features1, scores1 = prunage_df(df[cols_a_conserver], colonne=col_interet, acp_component=2, k=2)\n",
    "logger.warning(f\"\\n variance_list_ACP1:\\n{variance_list_ACP1} \\n\")\n",
    "logger.warning(f\"\\n liste_col1:\\n{selected_features1} \\n\")\n",
    "logger.warning(f\"\\n scores1:\\n{scores1} \\n\")\n",
    "\n",
    "# ______________________________\n",
    "#test avec les colonnes moyennées\n",
    "# ______________________________\n",
    "# Filtrer les colonnes: inclure toutes les colonnes numériques sauf celles contenant \"vitesse\", sauf la colonne d'intérêt\n",
    "cols_vitesse = [col for col in cols_num if \"vitesse\"  in col.lower() ]\n",
    "cols_humidity = [col for col in cols_num if \"humidi\" in col.lower()]\n",
    "cols_temperature = [col for col in cols_num if \"temp\" in col.lower()]\n",
    "cols_direction = [col for col in cols_num if \"direction\" in col.lower()]\n",
    "cols_heure = [col for col in cols_num if \"heure\" in col.lower()]\n",
    "\n",
    "logger.debug(f\"cols_humidity :\\n{cols_humidity} \\n\")\n",
    "logger.debug(f\" cols_temperature:\\n{cols_temperature} \\n\")\n",
    "logger.debug(f\" cols_vitesse:\\n{cols_vitesse} \\n\")\n",
    "logger.debug(f\"cols_direction:\\n{cols_direction} \\n\")\n",
    "\n",
    "# Création des colonnes moyennes pour chaque catégorie\n",
    "if cols_humidity:  # Vérification si la liste n'est pas vide\n",
    "    df['mean_humidity'] = df[cols_humidity].mean(axis=1)\n",
    "if cols_temperature:\n",
    "    df['mean_temperature'] = df[cols_temperature].mean(axis=1)\n",
    "if cols_direction:\n",
    "    df['mean_direction'] = df[cols_direction].mean(axis=1)\n",
    "if cols_vitesse:\n",
    "    df['mean_vitesse'] = df[cols_vitesse].mean(axis=1)\n",
    "\n",
    "# Suppression des colonnes qui ont servi à calculer les moyennes\n",
    "df.drop(columns=cols_humidity + cols_temperature + cols_direction + cols_vitesse + cols_heure, inplace=True)\n",
    "logger.debug(f\"colonnes  {df.columns.tolist()}\")\n",
    "# selection des colonnes\n",
    "cols_num_to_conserved = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "logger.debug(f\"cols_a_conserver  {cols_num_to_conserved}\")\n",
    "\n",
    "# La colonne d'intérêt\n",
    "col_interet = \"mean_vitesse\"\n",
    "# lancement fonction\n",
    "variance_list_ACP, selected_features, scores = prunage_df(df[cols_num_to_conserved], colonne=col_interet, acp_component=2, k=2)\n",
    "\n",
    "logger.warning(f\"\\n variance_list_ACP:\\n{variance_list_ACP} \\n\")\n",
    "logger.warning(f\"\\n liste_col:\\n{selected_features} \\n\")\n",
    "logger.warning(f\"\\n scores:\\n{scores} \\n\")\n",
    "# Vitesse du vent à 1m (km/h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36014b3",
   "metadata": {},
   "source": [
    "## Analyse preliminaire PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd764aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def analyse_preliminaire_series_temporelles(data, freq='M'):\n",
    "    \"\"\"\n",
    "    Effectue une analyse préliminaire d'une série temporelle.\n",
    "\n",
    "    :param data: Series pandas contenant les données de série temporelle.\n",
    "    :param freq: La fréquence de la série temporelle (ex. 'M' pour mensuelle, 'Q' pour trimestrielle).\n",
    "    \"\"\"\n",
    "    data=data.copy()\n",
    "\n",
    "\n",
    "    # # Assurer que l'index est de type datetime\n",
    "    # if not isinstance(data.index, pd.DatetimeIndex):\n",
    "    #     # Vérifier si 'date' est dans les colonnes et la convertir en datetime\n",
    "    #     if 'date' in data.columns:\n",
    "    #         data['date'] = pd.to_datetime(data['date'])\n",
    "    #         data.set_index('date', inplace=True)\n",
    "    #     else:\n",
    "    #         raise logger.error(\"L'index du DataFrame doit être de type pd.DatetimeIndex.\")\n",
    "\n",
    "    # # Liste des colonnes à supprimer\n",
    "    # cols_to_drop = ['station', 'POSTE', 'DATE']\n",
    "    # # Supprimer les colonnes de la liste si elles existent dans le DataFrame\n",
    "    # data.drop(columns=[col for col in cols_to_drop if col in data.columns], inplace=True)\n",
    "\n",
    "    # Tracé de la série temporelle\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(data)\n",
    "    plt.title('Série Temporelle')\n",
    "    plt.xlabel('Temps')\n",
    "    plt.ylabel('Valeurs')\n",
    "    plt.show()\n",
    "\n",
    "    # Décomposition saisonnière\n",
    "    decomposition = seasonal_decompose(data, model='additive', period=freq)\n",
    "    decomposition.plot()\n",
    "    plt.show()\n",
    "\n",
    "    # Fonction d'autocorrélation (ACF)\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plot_acf(data, lags=40)\n",
    "    plt.title('Autocorrélation (ACF)')\n",
    "    plt.show()\n",
    "\n",
    "    # Fonction d'autocorrélation partielle (PACF)\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plot_pacf(data, lags=40, method='ywm')\n",
    "    plt.title('Autocorrélation Partielle (PACF)')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# test\n",
    "# 'data' est une Series pandas avec un index DatetimeIndex\n",
    "# data=mon_df\n",
    "# analyse_preliminaire_series_temporelles(data, freq=12) # pour une série temporelle mensuelle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c59cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=mon_df.copy()\n",
    "df.columns.tolist()\n",
    "# ['station',\n",
    "#  'date',\n",
    "#  'précipitation_1_heures_(mm)',\n",
    "#  'température_(°C)',\n",
    "#  'température_point_de_rosée_(°C)',\n",
    "#  'température_mini_(°C)',\n",
    "#  'heure_tps_mini',\n",
    "#  'température_maxi_(°C)',\n",
    "#  'heure_tps_max',\n",
    "#  'durée_gel_(mn)',\n",
    "#  'température_mini_à_10cm_(°C)',\n",
    "#  'vent_moyen_à_1_m_(m/s)',\n",
    "#  'Direction',\n",
    "#  'vent_max_à_10_m_(m/s)',\n",
    "#  'direction_vent_max_à_10m',\n",
    "#  'heure_vent_max_à_10m',\n",
    "#  'vent_max(m/s)',\n",
    "#  'direction_vent_max',\n",
    "#  'heure_vent_max',\n",
    "#  'humidité_(%)',\n",
    "#  'humidité_mini(%)',\n",
    "#  'heure_humidité',\n",
    "#  'humidité_max_(%)',\n",
    "#  'heure_humidité_max_(%)',\n",
    "#  'humidité_absolue_2_m_(%)',\n",
    "#  'humidité_sup_a_40%_(%)',\n",
    "#  'humidité_sup_a_80%_(%)',\n",
    "#  'température_sol_(°C)',\n",
    "#  'Enthalpie_énergie_totale_système_atmosphérique']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0cd4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse_preliminaire_series_temporelles(df[\"vitesse_(km/h)\"], freq=1)\n",
    "# df = df_13001009_01jan2014_01mar2024.copy()\n",
    "col = df[\"vitesse_vent_moyen_at_1_m_(km/h)\"]\n",
    "\n",
    "# Forward Fill\n",
    "# col.fillna(method='ffill', inplace=True)\n",
    "col.ffill(inplace=True)\n",
    "\n",
    "# Backward Fill\n",
    "# col.fillna(method='bfill', inplace=True)\n",
    "col.bfill(inplace=True)\n",
    "# Interpolation\n",
    "col.interpolate(inplace=True)\n",
    "\n",
    "print(f\"\\n colonnes :{df.columns.tolist()}\")\n",
    "analyse_preliminaire_series_temporelles(col, freq=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eaee55",
   "metadata": {},
   "source": [
    "### test de stationnarité par Dickey-Fuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73562cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "\n",
    "def tester_stationnarite(serie):\n",
    "    \"\"\"\n",
    "    Teste la stationnarité d'une série temporelle.\n",
    "\n",
    "    :param serie: Pandas Series contenant les données temporelles.\n",
    "    :return: Résultats du test ADF, y compris la statistique de test, la valeur p, et les valeurs critiques.\n",
    "    \"\"\"\n",
    "    resultat_adf = adfuller(serie)\n",
    "    print('Statistique ADF : %f' % resultat_adf[0])\n",
    "    print('Valeur-p : %f' % resultat_adf[1])\n",
    "    print('Valeurs Critiques :')\n",
    "    for cle, valeur in resultat_adf[4].items():\n",
    "        print('\\t%s: %.3f' % (cle, valeur))\n",
    "\n",
    "    if resultat_adf[1] > 0.05:\n",
    "        print(f\"La série n'est pas stationnaire. car p= {resultat_adf[1]:.3%} est > à 0.05\")\n",
    "    else:\n",
    "        print(f\"La série est stationnaire, car p= {resultat_adf[1]:.3%} est < à 0.05\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23168692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serie_a_tester = df_sans_outlier['vitesse_(km/h)']\n",
    "# tester_stationnarite(serie_a_tester)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b006f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serie_a_tester = df_sans_outlier['rafale_(km/h)']\n",
    "# tester_stationnarite(serie_a_tester)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484070d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    print(f\"Testing stationarity for column: {col}\")\n",
    "    serie_a_tester = df[col]\n",
    "    tester_stationnarite(serie_a_tester)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3bf0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_13001009_01jan2014_01mar2024.copy()\n",
    "col = df[\"vitesse_vent_moyen_at_1_m_(km/h)\"]\n",
    "\n",
    "# Liste des colonnes à supprimer\n",
    "cols_to_drop = ['station', 'POSTE', 'series_name']\n",
    "\n",
    "# Supprimer les colonnes de la liste si elles existent dans le DataFrame\n",
    "df.drop(columns=[col for col in cols_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "#suppression des na\n",
    "df=df.dropna()\n",
    "\n",
    "# Vérifier si 'date' est dans les colonnes et la convertir en datetime\n",
    "if 'date' in data.columns:\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data.set_index('date', inplace=True)\n",
    "logger.debug(f\" colonnes aprés suppresiions {df.columns.tolist()}\")\n",
    "\n",
    "# Forward Fill Example:\n",
    "col.fillna(method='ffill', inplace=True)\n",
    "# Backward Fill Example:\n",
    "col.fillna(method='bfill', inplace=True)\n",
    "# Interpolation Example:\n",
    "col.interpolate(inplace=True)\n",
    "print(f\"\\n colonnes :{df.columns.tolist()}\")\n",
    "\n",
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    print(f\"Testing stationarity for column: {col}\")\n",
    "    serie_a_tester = df[col]\n",
    "    tester_stationnarite(serie_a_tester)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806d6a4",
   "metadata": {},
   "source": [
    "### Analyse graphique de la stationarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a4d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "# Créer les subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "# Tracer l'ACF\n",
    "plot_acf(df['vitesse_(km/h)'], lags=30, zero=True, ax=ax1)\n",
    "ax1.set_title('ACF - Vitesse du vent')\n",
    "ax1.set_xlabel('Lag')\n",
    "ax1.set_ylabel('Corrélation')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Ajuster les graduations sur l'axe x pour l'ACF\n",
    "ax1.set_xticks(np.arange(0, 31, 1))\n",
    "\n",
    "# Tracer le PACF\n",
    "plot_pacf(df['vitesse_(km/h)'], lags=30, zero=True, ax=ax2)\n",
    "ax2.set_title('PACF - Vitesse  du vent')\n",
    "ax2.set_xlabel('Lag')\n",
    "ax2.set_ylabel('Corrélation partielle')\n",
    "ax2.grid(True)\n",
    "\n",
    "# Ajuster les graduations sur l'axe x pour le PACF\n",
    "ax2.set_xticks(np.arange(0, 31, 1))\n",
    "\n",
    "# Ajuster les subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f304f2",
   "metadata": {},
   "source": [
    "### Rendre si necessaire la serie stationnaire par différenciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rendre_stationnaire(serie):\n",
    "    \"\"\"\n",
    "    Rend une série temporelle stationnaire par différenciation.\n",
    "\n",
    "    :param serie: Pandas Series contenant les données temporelles.\n",
    "    :return: Série temporelle différenciée.\n",
    "    \"\"\"\n",
    "    return serie.diff().dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25906699",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = rendre_stationnaire(df.select_dtypes(include=[np.number]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7d3c4",
   "metadata": {},
   "source": [
    "### Train test pour série temporelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044730a3",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def split_time_series(data, train_size=0.7, val_size=0.25, date_column='date'):\n",
    "    \"\"\"\n",
    "    Divise les données de série temporelle en ensembles d'entraînement, de validation et de test\n",
    "    basés sur des proportions spécifiques, après avoir converti la colonne de date spécifiée en datetime\n",
    "    et l'avoir définie comme index si nécessaire. Inclut la journalisation des tailles des ensembles.\n",
    "\n",
    "    :param data: DataFrame pandas contenant la série temporelle.\n",
    "    :param train_size: Proportion de l'ensemble d'entraînement.\n",
    "    :param val_size: Proportion de l'ensemble de validation.\n",
    "    :param date_column: Nom de la colonne de date à convertir et à utiliser comme index.\n",
    "    :return: Trois DataFrames pour l'entraînement, la validation et le test.\n",
    "    \"\"\"\n",
    "    # Vérification et ajustement de l'index\n",
    "    if not isinstance(data.index, pd.DatetimeIndex):\n",
    "        if date_column in data.columns:\n",
    "            data[date_column] = pd.to_datetime(data[date_column])\n",
    "            data.set_index(date_column, inplace=True)\n",
    "        else:\n",
    "            logger.error(f\"La colonne spécifiée '{date_column}' n'existe pas dans le DataFrame.\"    )\n",
    "            raise ValueError( f\"La colonne spécifiée '{date_column}' n'existe pas dans le DataFrame.\"  )\n",
    "\n",
    "    if train_size + val_size >= 1:\n",
    "        logger.error(\"La somme des tailles d'entraînement et de validation doit être inférieure à 1.\" )\n",
    "        raise ValueError(\"La somme des tailles d'entraînement et de validation doit être inférieure à 1.\")\n",
    "\n",
    "    # Calcul des indices de coupure\n",
    "    n = len(data)\n",
    "    train_end = int(n * train_size)\n",
    "    val_end = int(n * (train_size + val_size))\n",
    "\n",
    "    # Division des données\n",
    "    train_data = data.iloc[:train_end]\n",
    "    val_data = data.iloc[train_end:val_end]\n",
    "    test_data = data.iloc[val_end:]\n",
    "\n",
    "    # Journalisation des tailles des ensembles\n",
    "    logger.info(f\"Taille de l'ensemble d'entraînement : {len(train_data)}\")\n",
    "    logger.info(f\"Taille de l'ensemble de validation : {len(val_data)}\")\n",
    "    logger.info(f\"Taille de l'ensemble de test : {len(test_data)}\")\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "# Exemple d'utilisation:\n",
    "# Assurez-vous de remplacer 'votre_fichier.csv' et 'date' par vos propres valeurs\n",
    "# data = pd.read_csv('votre_fichier.csv')\n",
    "# train_data,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7eed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "# df.set_index('date', inplace=True)\n",
    "train_data, val_data, test_data = split_time_series(df,\n",
    "                                                    train_size=0.7,\n",
    "                                                    val_size=0.2,\n",
    "                                                    date_column='date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c2468",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245bfac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9468da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a32338",
   "metadata": {},
   "source": [
    "### Méthodologie Box-Jenkins pour ARIMA\n",
    "La méthodologie Box-Jenkins implique l'identification, l'estimation et la vérification des modèles ARIMA. \n",
    "\n",
    "Identification des ordres:\n",
    "- p (AR),\n",
    "- d (différenciation),\n",
    "- q (MA) à l'aide des graphiques ACF (AutoCorrelation Function) et PACF (Partial AutoCorrelation Function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d50347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "\n",
    "def identifier_parametres_arima(serie):\n",
    "    \"\"\"\n",
    "    Identifie visuellement les paramètres ARIMA d'une série temporelle.\n",
    "\n",
    "    :param serie: Pandas Series stationnaire.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(211)\n",
    "    plot_acf(serie, ax=plt.gca(), lags=40)\n",
    "    plt.subplot(212)\n",
    "    plot_pacf(serie, ax=plt.gca(), lags=40)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c97627",
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_parametres_arima(train_data[\"vitesse_(km/h)\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b18c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_parametres_arima(train_data[\"direction_(°)\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2172fba",
   "metadata": {},
   "source": [
    "### pipeline SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d158a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "\n",
    "def pipeline_sarimax(data, ordre, saisonnalite):\n",
    "    \"\"\"\n",
    "    Crée un pipeline pour trouver les hyperparamètres du modèle SARIMAX.\n",
    "\n",
    "    :param data: DataFrame ou Series contenant les données.\n",
    "    :param ordre: Tuple représentant l'ordre (p, d, q) du modèle ARIMA.\n",
    "    :param saisonnalite: Tuple représentant la saisonnalité (P, D, Q, S).\n",
    "    :return: Modèle SARIMAX ajusté.\n",
    "    \"\"\"\n",
    "    model = SARIMAX(data, order=ordre, seasonal_order=saisonnalite)\n",
    "    result = model.fit()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a5091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = pipeline_sarimax(\n",
    "#     df,\n",
    "#     (),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c86fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "\n",
    "\n",
    "def recherche_hyperparametres_sarimax(data, p_range, d_range, q_range,\n",
    "                                      s_range):\n",
    "    \"\"\"\n",
    "    Recherche les meilleurs hyperparamètres pour le modèle SARIMAX.\n",
    "    \"\"\"\n",
    "    meilleurs_resultats = float(\"inf\")\n",
    "    meilleurs_parametres = None  # Initialisez à None\n",
    "\n",
    "    pdq_combinations = list(itertools.product(p_range, d_range, q_range))\n",
    "    saisonnalite_combinations = [(x[0], x[1], x[2], s)\n",
    "                                 for x in pdq_combinations for s in s_range]\n",
    "\n",
    "    for parametres in pdq_combinations:\n",
    "        for parametres_saisonnalite in saisonnalite_combinations:\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\"ignore\")\n",
    "                    model = SARIMAX(data,\n",
    "                                    order=parametres,\n",
    "                                    seasonal_order=parametres_saisonnalite)\n",
    "                    result = model.fit()\n",
    "\n",
    "                if result.aic < meilleurs_resultats:\n",
    "                    meilleurs_resultats = result.aic\n",
    "                    meilleurs_parametres = (parametres,\n",
    "                                            parametres_saisonnalite)\n",
    "\n",
    "                logger.info(f'ARIMA{parametres}x{  parametres_saisonnalite} - AIC:{result.aic}'                )\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "    # Vérification si les meilleurs paramètres ont été trouvés\n",
    "    if meilleurs_parametres is not None:\n",
    "        logger.info(f'Meilleur modèle: ARIMA{meilleurs_parametres[0]}x{meilleurs_parametres[1]} - AIC: {meilleurs_resultats}' )\n",
    "        return SARIMAX(data,\n",
    "                       order=meilleurs_parametres[0],\n",
    "                       seasonal_order=meilleurs_parametres[1]).fit()\n",
    "    else:\n",
    "        print(\"Aucun modèle n'a pu être ajusté avec les paramètres fournis.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# # Exemple d'utilisation\n",
    "# data = train_data['vitesse_(km/h)']\n",
    "# meilleurs_modele = recherche_hyperparametres_sarimax(data, p_range=[0, 1, 2], d_range=[0, 1], q_range=[0, 1, 2], s_range=[0, 12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b2a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "data = train_data['vitesse_(km/h)']\n",
    "# meilleurs_modele = recherche_hyperparametres_sarimax(data,\n",
    "                                                    #  p_range=[0, 1, 2],\n",
    "                                                    #  d_range=[0, 1],\n",
    "                                                    #  q_range=[0, 1, 2],\n",
    "                                                    #  s_range=[0, 12])\n",
    "#  résultat en 98 min\n",
    "# Meilleur modèle: ARIMA(2, 1, 1)x(0, 0, 0, 0) - AIC: 42566.484576861214\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0709eda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_data['vitesse_(km/h)']\n",
    "# meilleurs_modele = recherche_hyperparametres_sarimax(data,\n",
    "                                                    #  p_range=[3, 4, 5],\n",
    "                                                    #  d_range=[0, 1],\n",
    "                                                    #  q_range=[3, 4, 5],\n",
    "                                                    #  s_range=[0, 6])\n",
    "\n",
    "# 688 min\n",
    "# ARIMA(5, 1, 5)x(5, 1, 5, 6) - AIC:42639.36229136215\n",
    "# Meilleur modèle: ARIMA(4, 0, 5)x(4, 1, 4, 6) - AIC: 42571.929467473594\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3529079",
   "metadata": {},
   "source": [
    "### fonction qui affiche les graphiques liés au modèle SARIMAX, \n",
    "dont:\n",
    " - les scores du modèle et \n",
    " - les graphiques ACF (Autocorrelation Function) \n",
    " - PACF (Partial Autocorrelation Function), \n",
    " \n",
    "Cette fonction suppose le modèle SARIMAX soit déja ajusté "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dcee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "\n",
    "def afficher_diagnostics_sarimax(modele, lags=40):\n",
    "    \"\"\"\n",
    "    Affiche les diagnostics d'un modèle SARIMAX, incluant le score AIC,\n",
    "    ainsi que les graphiques ACF et PACF des résidus.\n",
    "\n",
    "    :param modele: le modèle SARIMAX ajusté.\n",
    "    :param lags: le nombre de retards à inclure dans les graphiques ACF et PACF.\n",
    "    \"\"\"\n",
    "    # Affichage du score AIC\n",
    "    print(f\"Score AIC: {modele.aic}\")\n",
    "\n",
    "    # Extraction des résidus\n",
    "    residus = modele.resid\n",
    "\n",
    "    # Création de la figure pour les graphiques\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "    # Graphique ACF\n",
    "    plot_acf(residus, lags=lags, ax=axes[0])\n",
    "    axes[0].set_title(\"Autocorrelation Function\")\n",
    "\n",
    "    # Graphique PACF\n",
    "    plot_pacf(residus, lags=lags, ax=axes[1], method='ywm')\n",
    "    axes[1].set_title(\"Partial Autocorrelation Function\")\n",
    "\n",
    "    # Afficher les graphiques\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Exemple d'utilisation:\n",
    "# modele_sarimax = SARIMAX(...).fit()\n",
    "# afficher_diagnostics_sarimax(modele_sarimax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bbfc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "meilleur_modele = SARIMAX(df['vitesse_(km/h)'],\n",
    "                          order=(2, 1, 1),\n",
    "                          seasonal_order=(0, 0, 0, 0),\n",
    "                          enforce_stationarity=False,\n",
    "                          enforce_invertibility=False)\n",
    "\n",
    "resultats = meilleur_modele.fit()\n",
    "afficher_diagnostics_sarimax(resultats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b8b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "meilleur_modele = SARIMAX(df['direction_(°)'],\n",
    "                          order=(2, 1, 1),\n",
    "                          seasonal_order=(0, 0, 0, 0),\n",
    "                          enforce_stationarity=False,\n",
    "                          enforce_invertibility=False)\n",
    "\n",
    "resultats = meilleur_modele.fit()\n",
    "afficher_diagnostics_sarimax(resultats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2052b70",
   "metadata": {},
   "source": [
    "## Approche par ARIMA car il semble que nous n'ayons pas de saisonnalité dans le df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fab514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "\n",
    "\n",
    "def recherche_hyperparametres_arima(data, p_range, d_range, q_range):\n",
    "    \"\"\"\n",
    "    Recherche les meilleurs hyperparamètres pour le modèle ARIMA.\n",
    "    \"\"\"\n",
    "    meilleurs_resultats = float(\"inf\")\n",
    "    meilleurs_parametres = None  # Initialisez à None\n",
    "\n",
    "    pdq_combinations = list(itertools.product(p_range, d_range, q_range))\n",
    "\n",
    "    for parametres in pdq_combinations:\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"ignore\")\n",
    "                model = ARIMA(data, order=parametres)\n",
    "                result = model.fit()\n",
    "\n",
    "            if result.aic < meilleurs_resultats:\n",
    "                meilleurs_resultats = result.aic\n",
    "                meilleurs_parametres = parametres\n",
    "\n",
    "            print(f'ARIMA{parametres} - AIC:{result.aic}')\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # Vérification si les meilleurs paramètres ont été trouvés\n",
    "    if meilleurs_parametres is not None:\n",
    "        print(f'Meilleur modèle: ARIMA{meilleurs_parametres} - AIC: {meilleurs_resultats}' )\n",
    "        return ARIMA(data, order=meilleurs_parametres).fit()\n",
    "    else:\n",
    "        print(\"Aucun modèle n'a pu être ajusté avec les paramètres fournis.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# Assurez-vous que train_data est votre DataFrame et 'vitesse_(km/h)' la colonne cible\n",
    "# data = train_data['vitesse_(km/h)']\n",
    "# meilleurs_modele = recherche_hyperparametres_arima(data, p_range=[0, 1, 2], d_range=[0, 1], q_range=[0, 1, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assurez-vous que train_data est votre DataFrame et 'vitesse_(km/h)' la colonne cible\n",
    "data = train_data['vitesse_(km/h)']\n",
    "meilleurs_modele = recherche_hyperparametres_arima(data,\n",
    "                                                   p_range=[2, 1, 1],\n",
    "                                                   d_range=[0, 1],\n",
    "                                                   q_range=[0, 1, 2])\n",
    "# 1 min\n",
    "# ARIMA(1, 1, 2) - AIC:42588.17025183867\n",
    "# Meilleur modèle: ARIMA(1, 0, 2) - AIC: 42559.473341610465\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d725d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_data['direction_(°)']\n",
    "meilleurs_modele = recherche_hyperparametres_arima(data,\n",
    "                                                   p_range=[2, 1, 1],\n",
    "                                                   d_range=[0, 1],\n",
    "                                                   q_range=[0, 1, 2])\n",
    "# ARIMA(1, 1, 2) - AIC:-16613.022539697413\n",
    "# Meilleur modèle: ARIMA(2, 0, 2) - AIC: -16689.948372385094\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa2a165",
   "metadata": {},
   "source": [
    "### ARIMA avec la fonction pmdArima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72fe127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "\n",
    "\n",
    "def ajuster_auto_arima_diurne(train_data, target_column):\n",
    "    \"\"\"\n",
    "    Ajuste automatiquement un modèle ARIMA sur la colonne cible du dataset, en tenant compte\n",
    "    des variations diurnes pour des données au pas de 10 minutes.\n",
    "\n",
    "    :param train_data: DataFrame pandas contenant les données d'entraînement.\n",
    "    :param target_column: La colonne cible (variable à prédire) dans le DataFrame.\n",
    "    :return: Le modèle ARIMA ajusté.\n",
    "    \"\"\"\n",
    "    # Sélection de la colonne cible\n",
    "    y = train_data[target_column]\n",
    "\n",
    "    # Ajustement du modèle ARIMA avec une potentialité de variation diurne\n",
    "    modele_arima = pm.auto_arima(\n",
    "        y,\n",
    "        seasonal=True,  # Explorer la composante saisonnière\n",
    "        m=24,  # Nombre de périodes de 10 min dans une journée\n",
    "        d=None,  # Laisser pmdarima déterminer d\n",
    "        D=1,  # Tester avec D=1, vu la variation diurne supposée\n",
    "        max_p=2,\n",
    "        max_q=2,\n",
    "        max_P=2,\n",
    "        max_Q=2,\n",
    "        max_D=1,\n",
    "        trace=True,  # Afficher le processus de recherche\n",
    "        error_action='ignore',  # Ne pas arrêter sur des erreurs de convergence\n",
    "        suppress_warnings=True,  # Supprimer les avertissements\n",
    "        stepwise=True)  # Utiliser l'algorithme stepwise\n",
    "\n",
    "    logging.info(modele_arima.summary())\n",
    "    return modele_arima\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# Remplacer 'train_data' par votre DataFrame et 'vitesse_(km/h)' par la colonne cible de votre choix\n",
    "# modele_arima_ajuste = ajuster_auto_arima_diurne(train_data, 'vitesse_(km/h)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da0ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86408dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_data\n",
    "modele_arima_ajuste = ajuster_auto_arima_diurne(train_data, 'vitesse_(km/h)')\n",
    "modele_arima_ajuste\n",
    "#  ARIMA(2,0,2)(1,1,1)[144] intercept   : AIC=inf, Time=6430.11 sec\n",
    "#  ARIMA(0,0,0)(0,1,0)[144] intercept   : AIC=100765.465, Time=81.50 sec\n",
    "# Performing stepwise search to minimize aic\n",
    "#  ARIMA(2,0,2)(1,1,1)[24] intercept   : AIC=inf, Time=403.60 sec\n",
    "#  ARIMA(0,0,0)(0,1,0)[24] intercept   : AIC=-15560.423, Time=3.63 sec\n",
    "#  ARIMA(1,0,0)(1,1,0)[24] intercept   : AIC=-51507.835, Time=64.61 sec\n",
    "#  ARIMA(0,0,1)(0,1,1)[24] intercept   : AIC=-29560.577, Time=41.46 sec\n",
    "#  ARIMA(0,0,0)(0,1,0)[24]             : AIC=-15562.347, Time=2.29 sec\n",
    "#  ARIMA(1,0,0)(0,1,0)[24] intercept   : AIC=-47740.368, Time=6.65 sec\n",
    "#  ARIMA(1,0,0)(2,1,0)[24] intercept   : AIC=-52983.633, Time=350.20 sec\n",
    "#  ARIMA(1,0,0)(2,1,1)[24] intercept   : AIC=inf, Time=492.25 sec\n",
    "#  ARIMA(1,0,0)(1,1,1)[24] intercept   : AIC=inf, Time=100.12 sec\n",
    "#  ARIMA(0,0,0)(2,1,0)[24] intercept   : AIC=-15833.619, Time=75.39 sec\n",
    "#  ARIMA(2,0,0)(2,1,0)[24] intercept   : AIC=-53024.361, Time=476.26 sec\n",
    "#  ARIMA(2,0,0)(1,1,0)[24] intercept   : AIC=-51569.533, Time=62.95 sec\n",
    "#  ARIMA(2,0,0)(2,1,1)[24] intercept   : AIC=inf, Time=588.83 sec\n",
    "#  ARIMA(2,0,0)(1,1,1)[24] intercept   : AIC=inf, Time=98.57 sec\n",
    "#  ARIMA(2,0,1)(2,1,0)[24] intercept   : AIC=-53023.449, Time=601.38 sec\n",
    "#  ARIMA(1,0,1)(2,1,0)[24] intercept   : AIC=-53028.062, Time=614.76 sec\n",
    "#  ARIMA(1,0,1)(1,1,0)[24] intercept   : AIC=-51572.005, Time=77.84 sec\n",
    "#  ARIMA(1,0,1)(2,1,1)[24] intercept   : AIC=inf, Time=693.66 sec\n",
    "#  ARIMA(1,0,1)(1,1,1)[24] intercept   : AIC=inf, Time=117.39 sec\n",
    "#  ARIMA(0,0,1)(2,1,0)[24] intercept   : AIC=-29687.499, Time=230.13 sec\n",
    "#  ARIMA(1,0,2)(2,1,0)[24] intercept   : AIC=-53018.092, Time=572.23 sec\n",
    "#  ARIMA(0,0,2)(2,1,0)[24] intercept   : AIC=-37425.462, Time=476.40 sec\n",
    "#  ARIMA(2,0,2)(2,1,0)[24] intercept   : AIC=-52944.422, Time=655.27 sec\n",
    "#  ARIMA(1,0,1)(2,1,0)[24]             : AIC=-53030.050, Time=120.95 sec\n",
    "#  ARIMA(1,0,1)(1,1,0)[24]             : AIC=-51573.865, Time=20.13 sec\n",
    "#  ARIMA(1,0,1)(2,1,1)[24]             : AIC=inf, Time=291.86 sec\n",
    "#  ARIMA(1,0,1)(1,1,1)[24]             : AIC=inf, Time=74.22 sec\n",
    "#  ARIMA(0,0,1)(2,1,0)[24]             : AIC=-29689.407, Time=54.95 sec\n",
    "#  ARIMA(1,0,0)(2,1,0)[24]             : AIC=-52985.611, Time=103.84 sec\n",
    "#  ARIMA(2,0,1)(2,1,0)[24]             : AIC=-53025.439, Time=118.15 sec\n",
    "#  ARIMA(1,0,2)(2,1,0)[24]             : AIC=-53031.011, Time=185.04 sec\n",
    "#  ARIMA(1,0,2)(1,1,0)[24]             : AIC=-51574.954, Time=25.58 sec\n",
    "#  ARIMA(1,0,2)(2,1,1)[24]             : AIC=inf, Time=389.06 sec\n",
    "#  ARIMA(1,0,2)(1,1,1)[24]             : AIC=inf, Time=135.23 sec\n",
    "#  ARIMA(0,0,2)(2,1,0)[24]             : AIC=-37427.381, Time=141.00 sec\n",
    "#  ARIMA(2,0,2)(2,1,0)[24]             : AIC=-53034.431, Time=182.50 sec\n",
    "#  ARIMA(2,0,2)(1,1,0)[24]             : AIC=-51573.792, Time=39.23 sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99f26ff",
   "metadata": {},
   "source": [
    "### Modele VARMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed91bd",
   "metadata": {},
   "source": [
    "#### Fonction reduction ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def appliquer_acp(df, n_components=None):\n",
    "    \"\"\"\n",
    "    Applique l'ACP sur le DataFrame et retourne les composantes principales.\n",
    "\n",
    "    :param df: DataFrame pandas contenant les données.\n",
    "    :param n_components: Nombre de composantes principales à conserver.\n",
    "    :return: DataFrame des composantes principales.\n",
    "    \"\"\"\n",
    "    # Standardisation des données\n",
    "    df_std = StandardScaler().fit_transform(df)\n",
    "\n",
    "    # ACP\n",
    "    pca = PCA(n_components=n_components)\n",
    "    composantes_principales = pca.fit_transform(df_std)\n",
    "\n",
    "    # Création d'un DataFrame pour les composantes principales\n",
    "    cols = [f'PC{i+1}' for i in range(composantes_principales.shape[1])]\n",
    "    df_pca = pd.DataFrame(data=composantes_principales, columns=cols, index=df.index)\n",
    "\n",
    "    return df_pca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256afb8",
   "metadata": {},
   "source": [
    "#### Fonction recherche des meilleurs hyperparametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e6d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def recherche_hyperparametres_varmax(data, p_range, q_range, d=1, saison=None):\n",
    "    \"\"\"\n",
    "    Recherche les meilleurs hyperparamètres pour le modèle VARMAX.\n",
    "\n",
    "    :param data: DataFrame des données d'entraînement.\n",
    "    :param p_range: Intervalle des valeurs de p à tester.\n",
    "    :param q_range: Intervalle des valeurs de q à tester.\n",
    "    :param d: Ordre de différenciation (suppose une stationnarité après d différenciations).\n",
    "    :param saison: Paramètres saisonniers sous la forme (P,D,Q,s).\n",
    "    :return: Meilleurs paramètres et résultat.\n",
    "    \"\"\"\n",
    "    meilleurs_resultats = float(\"inf\")\n",
    "    meilleurs_parametres = None\n",
    "\n",
    "    for p in p_range:\n",
    "        for q in q_range:\n",
    "            try:\n",
    "                modele_varmax = VARMAX(data, order=(p,d,q), seasonal_order=saison, enforce_stationarity=False, enforce_invertibility=False)\n",
    "                resultats_varmax = modele_varmax.fit(disp=False)\n",
    "\n",
    "                # Utilisez votre critère de sélection ici, par exemple AIC\n",
    "                if resultats_varmax.aic < meilleurs_resultats:\n",
    "                    meilleurs_resultats = resultats_varmax.aic\n",
    "                    meilleurs_parametres = (p, d, q, saison)\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    print(f\"Meilleurs Paramètres: {meilleurs_parametres}, AIC: {meilleurs_resultats}\")\n",
    "    return meilleurs_parametres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7ec5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00686121",
   "metadata": {},
   "source": [
    "#### Fonction ajustement modele VARIMAX au pas horaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def ajuster_varmax_horaire(df):\n",
    "    \"\"\"\n",
    "    Ajuste un modèle VARMAX sur un dataset transformé au pas horaire avec ACP.\n",
    "\n",
    "    :param df: DataFrame pandas contenant les données d'entraînement.\n",
    "    :param n_components: Nombre de composantes principales à utiliser dans l'ACP.\n",
    "    :return: Le modèle VARMAX ajusté.\n",
    "    \"\"\"\n",
    "    # Application de l'ACP\n",
    "    df_acp = appliquer_acp(df, n_components=n_components)\n",
    "    df=df_acp.copy()\n",
    "    # Assurer que l'index est de type datetime et que les données sont au pas horaire\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = df.resample('h').mean()  # Aggrégation horaire par la moyenne\n",
    "\n",
    "    # Tester la stationnarité de chaque variable\n",
    "    def tester_stationnarite(serie):\n",
    "        resultat = adfuller(serie.dropna(), autolag='AIC')\n",
    "        return resultat[1]  # Retourne la p-value du test ADF\n",
    "\n",
    "    # Appliquer une différenciation si nécessaire pour atteindre la stationnarité\n",
    "    variables_a_differencier = [\n",
    "        col for col in df.columns if tester_stationnarite(df[col]) > 0.05\n",
    "    ]\n",
    "    for col in variables_a_differencier:\n",
    "        df[col] = df[col].diff().dropna()\n",
    "\n",
    "    df.dropna(inplace=True)  # Éliminer les possibles NaN après différenciation\n",
    "\n",
    "    # Sélection des variables pour le modèle VARMAX (peut nécessiter ajustement)\n",
    "    variables = df.columns\n",
    "\n",
    "    # Ajustement du modèle VARMAX\n",
    "    m = 144,  # Nombre de périodes de 10 min dans une journée\n",
    "    # Remplacer 'order=(2, 2)' par vos paramètres AR et MA, 'seasonal_order=(1, 1, 1, 144)' par vos paramètres saisonniers\n",
    "    modele_varmax = VARMAX(df[variables],\n",
    "                           order=(2, 2),\n",
    "                           seasonal_order=(1, 0, 2, m),\n",
    "                           trend='c')\n",
    "    resultats_varmax = modele_varmax.fit(disp=False)\n",
    "\n",
    "    print(resultats_varmax.summary())\n",
    "    return resultats_varmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ebd1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns.tolist()\n",
    "print(f\"\\n train_data.columns.tolist():\\n{train_data.columns.tolist()} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed67fb",
   "metadata": {},
   "source": [
    "#### Fonction globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda13f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "def appliquer_acp(df, variance_expliquee_seuil=0.95,n_components=5):\n",
    "    df_std = StandardScaler().fit_transform(df)\n",
    "    pca = PCA()\n",
    "    pca.fit(df_std)\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    n_components = np.argmax(cumsum >= variance_expliquee_seuil) + 1\n",
    "    pca = PCA(n_components=n_components)\n",
    "    composantes_principales = pca.fit_transform(df_std)\n",
    "    cols = [f'PC{i+1}' for i in range(composantes_principales.shape[1])]\n",
    "    df_pca = pd.DataFrame(data=composantes_principales, columns=cols, index=df.index)\n",
    "    return df_pca\n",
    "\n",
    "\n",
    "def ajuster_varmax_horaire(df, p_range, range_d, q_range, n_components=None):\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = df.resample('h').mean()\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    df_imputed = imputer.fit_transform(df)\n",
    "    df_imputed = pd.DataFrame(df_imputed, columns=df.columns, index=df.index)\n",
    "    df=df_imputed.copy()\n",
    "    df_pca = appliquer_acp(df, n_components=n_components)\n",
    "\n",
    "    def tester_stationnarite(serie):\n",
    "        resultat = adfuller(serie.dropna(), autolag='AIC')\n",
    "        return resultat[1]\n",
    "\n",
    "    variables_a_differencier = [col for col in df_pca.columns if tester_stationnarite(df_pca[col]) > 0.05]\n",
    "    for col in variables_a_differencier:\n",
    "        df_pca[col] = df_pca[col].diff().dropna()\n",
    "\n",
    "    df_pca.dropna(inplace=True)\n",
    "    variables = df_pca.columns\n",
    "\n",
    "    meilleurs_resultats = float(\"inf\")\n",
    "    meilleurs_parametres = None\n",
    "    for d in range_d:\n",
    "        for p in p_range:\n",
    "            for q in q_range:\n",
    "                try:\n",
    "                    modele_varmax = VARMAX(df_pca[variables], order=(p,0,q), trend='c')\n",
    "                    resultats_varmax = modele_varmax.fit(disp=False)\n",
    "\n",
    "                    if resultats_varmax.aic < meilleurs_resultats:\n",
    "                        meilleurs_resultats = resultats_varmax.aic\n",
    "                        meilleurs_parametres = (p, q)\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    print(f\"Meilleurs Paramètres: {meilleurs_parametres}, AIC: {meilleurs_resultats}\")\n",
    "    modele_varmax = VARMAX(df_pca[variables], order=meilleurs_parametres, trend='c')\n",
    "    resultats_varmax = modele_varmax.fit(disp=False)\n",
    "    print(resultats_varmax.summary())\n",
    "\n",
    "    return resultats_varmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a126e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exemple de plage de valeurs pour p et q\n",
    "p_range = [0,1, 2, 3,4]\n",
    "q_range = [0,1, 2, 3,4]\n",
    "range_d = [0,1, 2]\n",
    "\n",
    "\n",
    "# Utilisation de la fonction ajustée\n",
    "resultats_varmax = ajuster_varmax_horaire(train_data, p_range,range_d, q_range, n_components=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31885ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfecafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_filtred = train_data[['wave_amplitude_(m)', 'wave_period_(s)', 'direction_de_surface_(°)', 'pression_(bar)',  'direction_(°)', 'vitesse_(km/h)', 'rafale_(km/h)']]\n",
    "# resultats_varmax = ajuster_varmax_horaire(data_filtred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42030b1",
   "metadata": {},
   "source": [
    "**Qualité du modèle**\n",
    "Log Likelihood : La log-vraisemblance **élevée**  peut indiquer un bon ajustement du modèle aux données. Cependant, ce critère seul n'est pas suffisant pour évaluer la qualité du modèle.\n",
    "\n",
    "AIC/BIC/HQIC : Les critères d'information d'Akaike (AIC), Bayesian (BIC), et Hannan-Quinn (HQIC) sont des mesures permettant d'évaluer la qualité du modèle tout en prenant en compte le nombre de paramètres. Plus ces valeurs sont **basses**, mieux c'est. Dans ce cas, elles semblent indiquer que le modèle est relativement complexe, avec potentiellement un risque de surajustement.\n",
    "\n",
    "**Tests statistiques**\n",
    "\n",
    "Ljung-Box (Q) et Prob(Q) : Ces tests servent à vérifier l'autocorrélation des résidus. Des valeurs de Prob(Q) **élevées** (proches de 1) suggèrent qu'il n'y a pas d'autocorrélation significative, ce qui est généralement bon pour un modèle VARMA.\n",
    "\n",
    "**Jarque-Bera (JB)** et **Prob(JB)** : Le test de Jarque-Bera examine la normalité des résidus en se basant sur leur skewness (asymétrie) et kurtosis (aplatissement). \n",
    "\n",
    "Des valeurs de **Prob(JB)** proches de **0** indiquent une non-normalité des résidus, ce qui est le cas ici, suggérant que les résidus ne suivent pas une distribution normale.\n",
    "\n",
    "**Heteroskedasticity (H)** et **Prob(H)** : Le test d'hétéroscédasticité évalue si la variance des résidus est constante dans le temps. Les valeurs de Prob(H) **proches de 0 ou 1** indiquent respectivement une présence ou une absence d'hétéroscédasticité. Des résultats  mixtes,  pourraient indiquer des variations dans la variance des résidus pour certaines variables.\n",
    "\n",
    "**Coefficients du modèle**\n",
    "Les coefficients pour chaque variable et pour chaque retard (L1, L2) sont accompagnés de leur erreur standard, de la valeur de **test (z)** et de la p-value (P>|z|). Dans ce cas, la plupart des coefficients semblent ne pas être statistiquement significatifs **(p-values élevées)**, ce qui peut indiquer que certaines variables explicatives ne sont **pas pertinentes** pour prédire les variables dépendantes.\n",
    "\n",
    "**Conclusions**\n",
    "Ajustement du modèle : Bien que la log-vraisemblance soit élevée, l'absence de normalité des résidus (probabilités JB proches de 0) et la non-significativité statistique de nombreux coefficients suggèrent que le modèle pourrait ne pas être bien ajusté.\n",
    "\n",
    "Complexité vs Pertinence : Les valeurs élevées d'AIC, BIC et HQIC suggèrent que le modèle est complexe, possiblement trop pour les données à disposition. Il peut être utile de simplifier le modèle ou d'examiner d'autres structures de modèle.\n",
    "\n",
    "Prédictions et implications : Malgré ces limitations, le modèle VARMA peut encore fournir des insights utiles, notamment en identifiant des relations dynamiques entre les variables. Cependant, il serait prudent de ne pas se fier uniquement à ce modèle pour des prédictions ou des décisions importantes sans examiner d'autres données ou modèles.\n",
    "\n",
    "Une analyse plus approfondie, peut-être avec des modèles alternatifs ou une réduction de la complexité du modèle actuel, pourrait être bénéfique pour améliorer la compréhension et la prédiction des séries temporelles analysées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c51e566",
   "metadata": {},
   "source": [
    "### Prediction de varmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b8d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Supposons que 'scaler' est votre MinMaxScaler qui a été ajusté sur les données d'entraînement\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "scaler.fit(df[['vitesse_(km/h)']])\n",
    "\n",
    "\n",
    "# Préparer les prédictions pour les prochaines 5 heures\n",
    "nombre_de_pas_a_prevoir = 5  # Par exemple pour prévoir les 5 prochaines heures\n",
    "predictions = resultats_varmax.get_forecast(steps=nombre_de_pas_a_prevoir)\n",
    "predictions_df = predictions.predicted_mean\n",
    "\n",
    "# Inverser la transformation de mise à l'échelle sur les prédictions\n",
    "predictions_inverses = scaler.inverse_transform(predictions_df)\n",
    "\n",
    "# Si vous avez également mis à l'échelle les données d'entraînement, vous voudrez les inverser également pour le tracé\n",
    "train_data_inverses = scaler.inverse_transform(train_data[['vitesse_(km/h)']])\n",
    "\n",
    "# # Tracer les données d'entraînement inversées et les prédictions inversées\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# plt.plot(train_data.index[-120:], train_data_inverses[-120:], label='Données réelles', color='blue')  # Les 5 derniers jours (24*5=120)\n",
    "\n",
    "# # Calculer l'index pour les prédictions\n",
    "index_predictions = pd.date_range(start=train_data.index[-1], periods=nombre_de_pas_a_prevoir + 1, freq='6H')[1:]\n",
    "\n",
    "\n",
    "# Tracer les données d'entraînement inversées\n",
    "plt.plot(train_data.index[-120:], train_data_inverses[-120:], label='Données réelles', color='blue')  # Les dernières 120 heures\n",
    "\n",
    "# Tracer les prédictions\n",
    "# Supposons que index_predictions est correctement défini pour couvrir la période de prédiction\n",
    "plt.plot(index_predictions, predictions_inverses[:, 0], label='Prédictions', color='red', linestyle='--')\n",
    "\n",
    "# Formater l'axe des x pour afficher les dates et les heures\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=12))  # Interval d'une étiquette toutes les 12 heures\n",
    "plt.gcf().autofmt_xdate()  # Rotation automatique des dates pour améliorer la lisibilité\n",
    "\n",
    "# Ajouter des titres et légendes\n",
    "plt.title('Vitesse du vent - Données réelles vs Prédictions')\n",
    "plt.xlabel('Date et heure')\n",
    "plt.ylabel('Vitesse du vent (km/h)')\n",
    "plt.legend()\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c79a0c",
   "metadata": {},
   "source": [
    "### modèle VAR (Vector Autoregression) \n",
    "\n",
    "pour le dataset de meteo france et ses 40 colonnes.  A utiliser suelement aprés traitement des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def preparer_et_ajuster_var(df):\n",
    "    \"\"\"\n",
    "    Prépare le dataset et ajuste un modèle VAR.\n",
    "\n",
    "    :param df: DataFrame pandas contenant les données.\n",
    "    :return: Résultats de l'ajustement du modèle VAR.\n",
    "    \"\"\"\n",
    "    # Conversion des séparateurs décimaux et gestion des types de données\n",
    "    df = df.replace(',', '.', regex=True).astype(float)\n",
    "\n",
    "    # Suppression des colonnes non numériques ou non pertinentes si nécessaire\n",
    "    # df.drop(['POSTE', 'DATE', ...], axis=1, inplace=True)\n",
    "\n",
    "    # Vérification de la stationnarité avec le test de Dickey-Fuller augmenté\n",
    "    def tester_stationnarite(series):\n",
    "        resultat = adfuller(series, autolag='AIC')\n",
    "        return resultat[1]  # p-value\n",
    "\n",
    "    # Appliquer le test de stationnarité et différencier les séries non stationnaires\n",
    "    for column in df.columns:\n",
    "        if tester_stationnarite(df[column]) > 0.05:  # Seuil de p-value à 0.05\n",
    "            df[column] = df[column].diff().dropna()\n",
    "\n",
    "    # Suppression des éventuelles lignes contenant des NaN après différenciation\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Ajustement du modèle VAR\n",
    "    modele_var = VAR(df)\n",
    "    resultats_var = modele_var.fit(maxlags=15, ic='aic')\n",
    "\n",
    "    print(resultats_var.summary())\n",
    "    return resultats_var\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "\n",
    "# Assurez-vous d'avoir converti les colonnes DATE en datetime et de les avoir éventuellement exclues du VAR si elles ne sont pas numériques\n",
    "# df['DATE'] = pd.to_datetime(df['DATE'], format='%Y%m%d%H')\n",
    "\n",
    "# Ajuster le modèle VAR\n",
    "# resultats_var = preparer_et_ajuster_var(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f3f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_var = preparer_et_ajuster_var(df.drop(\"date\", axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bee0a5",
   "metadata": {},
   "source": [
    "### Modele VECM (Vector Error Correction Model)\n",
    "\n",
    "Un VECM est approprié si vos données sont cointégrées, ce qui signifie qu'il existe une relation à long terme entre les séries temporelles même si elles sont non stationnaires.\n",
    "Il est particulièrement utile pour les séries temporelles économiques où les variables sont susceptibles d'avoir des relations d'équilibre à long terme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bdc88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import VECM\n",
    "\n",
    "\n",
    "def vecm_model(data):\n",
    "    \"\"\"\n",
    "    Ajuster un modèle VECM.\n",
    "    \"\"\"\n",
    "    model = VECM(data, k_ar_diff=1, coint_rank=1)\n",
    "    results = model.fit()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f451ace4",
   "metadata": {},
   "source": [
    "### Modele GARCH (Generalized Autoregressive Conditional Heteroskedasticity )\n",
    "\n",
    "Si la volatilité des données est une caractéristique importante, par exemple dans les séries temporelles financières, un modèle GARCH peut être utilisé pour modéliser et prévoir la variance conditionnelle (c'est-à-dire la volatilité)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdaca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model\n",
    "\n",
    "\n",
    "def garch_model(data):\n",
    "    \"\"\"\n",
    "    Ajuster un modèle GARCH.\n",
    "    \"\"\"\n",
    "    model = arch_model(data, vol='GARCH', p=1, q=1)\n",
    "    results = model.fit()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ebb316",
   "metadata": {},
   "source": [
    "### Modele Dynamic Factor Model (DFM) :\n",
    "\n",
    "Les DFM sont utilisés pour modéliser les séries temporelles multivariées où quelques facteurs communs sous-jacents influencent toutes les séries. Ils sont utiles pour réduire la dimensionnalité des données et pour capturer les interactions entre les séries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.dynamic_factor import DynamicFactor\n",
    "\n",
    "\n",
    "def dfm_model(data):\n",
    "    \"\"\"\n",
    "    Ajuster un modèle DFM.\n",
    "    \"\"\"\n",
    "    model = DynamicFactor(data, k_factors=1, factor_order=1)\n",
    "    results = model.fit()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac527c",
   "metadata": {},
   "source": [
    "### Modele Modèle à espace d'états avec filtre de Kalman :\n",
    "\n",
    "modèles à espace d'états, souvent utilisés avec le filtre de Kalman, sont très flexibles et permettent de modéliser des séries temporelles multivariées où des relations complexes et changeantes existent entre les variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabcf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "\n",
    "def statespace_model(data):\n",
    "    \"\"\"\n",
    "    Ajuster un modèle à espace d'états avec filtre de Kalman.\n",
    "    \"\"\"\n",
    "    model = SARIMAX(data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "    results = model.fit()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b77647a",
   "metadata": {},
   "source": [
    "### Modele Random Forest :\n",
    "\n",
    " forêts aléatoires (**Random Forests**) ou les machines à vecteurs de support (Support Vector Machines) pour les séries temporelles peuvent être appliquées en créant des caractéristiques retardées (lag features) comme entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f144c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "def random_forest_model(data, target):\n",
    "    \"\"\"\n",
    "    Ajuster un modèle Random Forest.\n",
    "    \"\"\"\n",
    "    model = RandomForestRegressor(n_estimators=100)\n",
    "    model.fit(data, target)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ea70d",
   "metadata": {},
   "source": [
    "### Modele XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d2b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def xgboost_model(data, target):\n",
    "    \"\"\"\n",
    "    Ajuster un modèle XGBoost.\n",
    "    \"\"\"\n",
    "    model = xgb.XGBRegressor(n_estimators=100)\n",
    "    model.fit(data, target)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d74a6",
   "metadata": {},
   "source": [
    "### Modele Support Vector Machine (SVM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "def svm_model(data, target):\n",
    "    \"\"\"\n",
    "    Ajuster un modèle SVM.\n",
    "    \"\"\"\n",
    "    model = SVR(kernel='rbf')\n",
    "    model.fit(data, target)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4469fc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13056002_01jan2014_01mar2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34562726",
   "metadata": {},
   "source": [
    "## Deep learning\n",
    "### Modele LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30137531",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_13056002_01jan2014_01mar2024.copy()\n",
    "train_data, val_data, test_data = split_time_series(df, train_size=0.7, val_size=0.2, date_column='date')\n",
    "\n",
    "logger.debug(f\"\\n liste colonnes:\\n{train_data.columns.tolist()} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99024c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def prepare_data(data, n_steps):\n",
    "    \"\"\"\n",
    "    Préparer les données pour le modèle LSTM.\n",
    "\n",
    "    :param data: DataFrame contenant les données à utiliser.\n",
    "    :param n_steps: Nombre de pas de temps à utiliser comme entrée pour la prédiction.\n",
    "    :return: X, y où X est un tableau de données d'entrée et y est le tableau des cibles à prédire.\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(data)):\n",
    "        # Trouver la fin de ce pattern\n",
    "        end_ix = i + n_steps\n",
    "        # Vérifier si on est au-delà de la séquence\n",
    "        if end_ix > len(data) - 1:\n",
    "            break\n",
    "        # Rassembler les entrées et les sorties\n",
    "        seq_x, seq_y = data[i:end_ix, :-1], data[end_ix - 1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def lstm_model(train_data, n_steps):\n",
    "    \"\"\"\n",
    "    Ajuster un modèle LSTM sur les données d'entraînement.\n",
    "\n",
    "    :param train_data: Les données d'entraînement préparées par la fonction `prepare_data`.\n",
    "    :param n_steps: Nombre de pas de temps utilisés pour chaque séquence d'entrée.\n",
    "    :return: Un modèle LSTM ajusté.\n",
    "    \"\"\"\n",
    "    # La forme des données est [échantillons, pas de temps, caractéristiques]\n",
    "    n_features = train_data.shape[2]\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Diviser les données d'entraînement en X (entrées) et y (cibles)\n",
    "    X, y = prepare_data(train_data, n_steps)\n",
    "\n",
    "    # Ajuster le modèle\n",
    "    model.fit(X, y, epochs=50, verbose=0)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c94f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Préparation des données d'entraînement\n",
    "# Supposons que `train_data_np` est votre DataFrame converti en un tableau numpy, avec la dernière colonne étant celle que vous voulez prédire.\n",
    "# Vous devriez adapter cette partie pour extraire les données de votre DataFrame.\n",
    "n_steps = 3  # Par exemple, utiliser les 3 dernières heures pour prédire la prochaine.\n",
    "# train_data_np = train_data.to_numpy()  # Convertir le DataFrame en numpy array si ce n'est pas déjà fait.\n",
    "\n",
    "# Appel de la fonction\n",
    "model = lstm_model(train_data_np, n_steps)\n",
    "model = lstm_model(train_data, n_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f77e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target=\n",
    "lstm_model(train_data,target,\"10T\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d046c8e3",
   "metadata": {},
   "source": [
    "### Modele GRU \n",
    "\n",
    "les GRU (Gated Recurrent Units) sont conçus pour traiter des séquences de données comme les séries temporelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e467d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense\n",
    "\n",
    "\n",
    "def gru_model(data, target, n_steps):\n",
    "    \"\"\"\n",
    "    Ajuster un modèle GRU.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(GRU(50, activation='relu', input_shape=(n_steps, data.shape[1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(data, target, epochs=50, verbose=0)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ce79e",
   "metadata": {},
   "source": [
    "### Modele CNN\n",
    "\n",
    "Convolutional Neural Networks (**CNN**) :\n",
    "\n",
    " Bien que principalement utilisés pour l'analyse d'image, ils peuvent être appliqués aux séries temporelles pour capturer des motifs locaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0a64b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "\n",
    "def cnn_model(data, target, n_steps):\n",
    "    \"\"\"\n",
    "    Ajuster un modèle CNN.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Conv1D(filters=64,\n",
    "               kernel_size=2,\n",
    "               activation='relu',\n",
    "               input_shape=(n_steps, data.shape[1])))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(data, target, epochs=50, verbose=0)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77936da",
   "metadata": {},
   "source": [
    "### Modele  MARS\n",
    "\n",
    "Multivariate Adaptive Regression Splines **(MARS)** \n",
    "\n",
    "MARS est un modèle non-paramétrique qui peut être utilisé pour modéliser des relations complexes et non linéaires entre les variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyearth import Earth\n",
    "\n",
    "\n",
    "def mars_model(data, target):\n",
    "    \"\"\"\n",
    "    Ajuster un modèle MARS.\n",
    "    \"\"\"\n",
    "    model = Earth()\n",
    "    model.fit(data, target)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9516a8c",
   "metadata": {},
   "source": [
    "### Modele RNN\n",
    "\n",
    " Les réseaux de neurones récurrents (**RNN**), en particulier les variantes telles que les LSTM (Long Short-Term Memory) et les GRU (Gated Recurrent Unit), sont bien adaptés pour modéliser des données séquentielles et peuvent capturer des relations complexes dans des séries temporelles multivariées.\n",
    "Machine Learning Traditionnel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0254bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU\n",
    "\n",
    "\n",
    "def ajuster_rnn(data, n_neurons, n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neurons, input_shape=(n_inputs, data.shape[1])))\n",
    "    # Pour GRU: model.add(GRU(n_neurons, input_shape=(n_inputs, data.shape[1])))\n",
    "    model.add(Dense(n_outputs))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8ab10",
   "metadata": {},
   "source": [
    "## Modele VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33136bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import warnings\n",
    "\n",
    "\n",
    "def ajuster_modele_var(df):\n",
    "    \"\"\"\n",
    "    Ajuste un modèle VAR sur un DataFrame contenant plusieurs séries temporelles interdépendantes.\n",
    "\n",
    "    :param df: DataFrame pandas contenant les séries temporelles avec une colonne de date en index.\n",
    "    :return: Objet VARResults après l'ajustement du modèle.\n",
    "    \"\"\"\n",
    "\n",
    "    # Vérifier la stationnarité de chaque série temporelle\n",
    "    def tester_stationnarite(serie):\n",
    "        resultat_test = adfuller(serie, autolag='AIC')\n",
    "        return resultat_test[1]  # p-value du test ADF\n",
    "\n",
    "    # Tester toutes les séries pour la stationnarité\n",
    "    p_values = df.apply(tester_stationnarite, axis=0)\n",
    "    non_stationnaires = p_values[p_values > 0.05].index.tolist()\n",
    "\n",
    "    if non_stationnaires:\n",
    "        warnings.warn(\n",
    "            f\"Les séries suivantes ne sont pas stationnaires : {\n",
    "                non_stationnaires}\",\n",
    "            UserWarning)\n",
    "\n",
    "    # Ajuster le modèle VAR sur les données\n",
    "    modele_var = VAR(df)\n",
    "    resultats_var = modele_var.fit(maxlags=15, ic='aic')\n",
    "\n",
    "    # Afficher le résumé du modèle ajusté\n",
    "    print(resultats_var.summary())\n",
    "\n",
    "    return resultats_var\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# Assurez-vous que 'df' est votre DataFrame avec des séries temporelles et qu'il a une colonne de date comme index\n",
    "# df = pd.read_csv('votre_fichier.csv', index_col='date', parse_dates=['date'])\n",
    "# resultats_var = ajuster_modele_var(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e7110",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_var = ajuster_modele_var(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b6a5cc",
   "metadata": {},
   "source": [
    "## autres modéles à exploiter\n",
    "\n",
    "Modèles VARMA (Vector Autoregressive Moving Average):\n",
    "\n",
    "- Le modèle VARMA combine les modèles VAR et VMA et peut être utile si les séries temporelles présentent à la fois une autocorrélation et une moyenne mobile.\n",
    "- Modèles **VARMAX** (Vector Autoregressive Moving-Average with eXogenous inputs):\n",
    "\n",
    "Ce modèle étend le VARMA en incluant des variables exogènes. Si vous avez des données supplémentaires qui pourraient influencer vos séries temporelles, comme des facteurs économiques ou des événements spéciaux, le modèle VARMAX peut les prendre en compte.\n",
    "\n",
    "- Modèles **VECM** (Vector Error Correction Model):\n",
    "\n",
    "Le VECM est approprié pour les séries temporelles non stationnaires qui sont cointégrées. Si vos séries temporelles ont des tendances stochastiques communes sur le long terme, le VECM peut être un bon choix.\n",
    "Réseaux de Neurones et Deep Learning:\n",
    "\n",
    "- Les réseaux de neurones récurrents (**RNN**), en particulier les variantes telles que les LSTM (Long Short-Term Memory) et les GRU (Gated Recurrent Unit), sont bien adaptés pour modéliser des données séquentielles et peuvent capturer des relations complexes dans des séries temporelles multivariées.\n",
    "Machine Learning Traditionnel:\n",
    "\n",
    "-  forêts aléatoires (**Random Forests**) ou les machines à vecteurs de support (Support Vector Machines) pour les séries temporelles peuvent être appliquées en créant des caractéristiques retardées (lag features) comme entrée.\n",
    "\n",
    "- Modèles Basés sur les Ensembles (Ensemble Models):\n",
    "\n",
    "- Generalized Autoregressive Conditional Heteroskedasticity (**GARCH**) :\n",
    "\n",
    "Si la volatilité des données est une caractéristique importante, par exemple dans les séries temporelles financières, un modèle GARCH peut être utilisé pour modéliser et prévoir la variance conditionnelle (c'est-à-dire la volatilité).\n",
    "\n",
    "- Dynamic Factor Models (**DFM**) :\n",
    "\n",
    "Les DFM sont utilisés pour modéliser les séries temporelles multivariées où quelques facteurs communs sous-jacents influencent toutes les séries. Ils sont utiles pour réduire la dimensionnalité des données et pour capturer les interactions entre les séries.\n",
    "State Space Models and Kalman Filter :\n",
    "\n",
    "-  modèles à espace d'états, souvent utilisés avec le filtre de Kalman, sont très flexibles et permettent de modéliser des séries temporelles multivariées où des relations complexes et changeantes existent entre les variables.\n",
    "\n",
    "**Machine Learning and Ensemble Methods** :\n",
    "\n",
    "- Random Forests, \n",
    "- Gradient Boosting Machines (comme XGBoost, LightGBM), ou \n",
    "- Support Vector Machines pour les séries temporelles (SVM) : Ces méthodes peuvent être utilisées pour capter des relations non linéaires et des interactions complexes entre les variables.\n",
    "\n",
    "**Deep Learning : Des réseaux de neurones comme**\n",
    "- les LSTM (Long Short-Term Memory) ou \n",
    "- les GRU (Gated Recurrent Units) qui sont conçus pour traiter des séquences de données comme les séries temporelles.\n",
    "- Convolutional Neural Networks (CNN) : Bien que principalement utilisés pour l'analyse d'image, ils peuvent être appliqués aux séries temporelles pour capturer des motifs locaux.\n",
    "\n",
    "- Multivariate Adaptive Regression Splines **(MARS)** :\n",
    "\n",
    "MARS est un modèle non-paramétrique qui peut être utilisé pour modéliser des relations complexes et non linéaires entre les variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d7232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f49b6db",
   "metadata": {},
   "source": [
    "## approche par utilisation de xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcdef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Supposons que train_data soit votre DataFrame\n",
    "\n",
    "# Préparation des données\n",
    "X = train_data[['temperature_(°)', 'rafale_(km/h)',\n",
    "                'direction']]  # caractéristiques\n",
    "y = train_data['vitesse_(km/h)']  # cible\n",
    "\n",
    "# Division du dataset en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# Convertir les ensembles de données en DMatrix, une structure de données optimisée par XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Paramètres du modèle XGBoost\n",
    "param = {\n",
    "    'max_depth': 5,  # profondeur maximale de chaque arbre\n",
    "    'eta': 0.3,  # taux d'apprentissage\n",
    "    'objective':\n",
    "    'reg:squarederror',  # objectif de régression avec erreur quadratique\n",
    "    'eval_metric':\n",
    "    'rmse'  # utiliser la racine de l'erreur quadratique moyenne comme métrique d'évaluation\n",
    "}\n",
    "num_round = 100  # nombre d'itérations de boosting\n",
    "\n",
    "# Entraînement du modèle\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "# Évaluer le modèle\n",
    "rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Vous pouvez ajuster les paramètres et le nombre d'itérations pour améliorer le modèle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d826772",
   "metadata": {},
   "source": [
    "## Creation d'un wrapper pour utiliser un model SARIMAX dans scikit-learn\n",
    "\n",
    "car SARIMAX de statsmodels ne suit pas directement l'API de scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d2a2c",
   "metadata": {},
   "source": [
    "### Fonction optimisation serie temporelle utilisant le wrapper SARIMAX\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0f52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "# from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "# from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.base import BaseEstimator, RegressorMixin\n",
    "# import numpy as np\n",
    "\n",
    "# # Définition du wrapper SARIMAX pour l'intégrer dans scikit-learn\n",
    "# class SARIMAXWrapper(BaseEstimator, RegressorMixin):\n",
    "#     def __init__(self, order=(1, 0, 0), seasonal_order=(0, 0, 0, 0)):\n",
    "#         self.order = order\n",
    "#         self.seasonal_order = seasonal_order\n",
    "#         self.model = None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         self.model = SARIMAX(y, order=self.order, seasonal_order=self.seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n",
    "#         self.model = self.model.fit(disp=False)\n",
    "#         return self\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         # Dans SARIMAX, X est généralement le nombre de périodes à prédire, mais cela peut être ajusté en fonction de vos besoins\n",
    "#         forecast = self.model.get_forecast(steps=len(X))\n",
    "#         return forecast.predicted_mean\n",
    "\n",
    "# def optimiser_serie_temporelle(df, date_train_end, date_test_start,\n",
    "#                                nom_colonne_predire, params):\n",
    "#     \"\"\"\n",
    "#     Fonction pour optimiser un modèle de série temporelle SARIMAX sur un DataFrame.\n",
    "\n",
    "#     :param df: DataFrame contenant les données de la série temporelle.\n",
    "#     :param date_train_end: Date de fin de l'entraînement (format 'AAAA-MM-JJ').\n",
    "#     :param date_test_start: Date de début du test (format 'AAAA-MM-JJ').\n",
    "#     :param params: Dictionnaire des paramètres pour GridSearchCV.\n",
    "#     :return: Le modèle optimisé, scores sur les ensembles d'entraînement, de test et de validation.\n",
    "#     \"\"\"\n",
    "#     # # Séparer les données en ensembles d'entraînement, de validation, et de test\n",
    "#     # train_data = df[:date_train_end]\n",
    "#     # test_data = df[date_test_start:]\n",
    "#     df.index = pd.to_datetime(df.index)\n",
    "#     df = df.asfreq('D')  # Utilisez 'D', 'M', ou toute autre fréquence adaptée à vos données\n",
    "\n",
    "#     # Séparer les données en ensembles d'entraînement et de test\n",
    "#     train_data = df[:date_train_end][nom_colonne_predire]\n",
    "#     test_data = df[date_test_start:][nom_colonne_predire]\n",
    "#     # Définir les hyperparamètres et le modèle à utiliser dans le GridSearch\n",
    "#     tscv = TimeSeriesSplit(n_splits=3)\n",
    "#     # Définition des métriques d'évaluation\n",
    "#     scoring = {\n",
    "#         'RMSE': make_scorer(mean_squared_error, squared=False),\n",
    "#         'MAE': make_scorer(mean_absolute_error)\n",
    "#     }\n",
    "\n",
    "#     # Création de l'instance GridSearchCV\n",
    "#     grid_search = GridSearchCV(SARIMAXWrapper(),\n",
    "#                                param_grid=params,\n",
    "#                                cv=tscv,\n",
    "#                                scoring=scoring,\n",
    "#                                refit='RMSE')\n",
    "\n",
    "#     # Entraînement du modèle\n",
    "#     grid_search.fit(np.arange(len(train_data)), train_data)\n",
    "\n",
    "#     # Prédiction et évaluation sur l'ensemble de test\n",
    "#     y_pred_test = grid_search.predict(np.arange(len(test_data)))\n",
    "\n",
    "#     # Calcul des scores\n",
    "#     scores_test = {\n",
    "#         'RMSE': mean_squared_error(test_data, y_pred_test, squared=False),\n",
    "#         'MAE': mean_absolute_error(test_data, y_pred_test)\n",
    "#     }\n",
    "\n",
    "#     return grid_search.best_estimator_, scores_test, train_data, test_data, y_pred_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37405ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "# from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# #\n",
    "\n",
    "# def optimiser_serie_temporelle(df, date_train_end, date_test_start, params):\n",
    "#     \"\"\"\n",
    "#     Fonction pour optimiser un modèle de série temporelle sur un DataFrame.\n",
    "#     Importation du wrapper SARIMAX compatible avec scikit-learn au sein de la fonction\n",
    "#     :param df: DataFrame contenant les données.\n",
    "#     :param date_train_end: Date de fin de l'entraînement (format 'AAAA-MM-JJ').\n",
    "#     :param date_test_start: Date de début du test (format 'AAAA-MM-JJ').\n",
    "#     :param params: Dictionnaire des paramètres pour GridSearchCV.\n",
    "#     :return: Le modèle optimisé, scores sur les ensembles d'entraînement, de test et de validation.\n",
    "#     \"\"\"\n",
    "#     # Conversion des colonnes de date en datetime\n",
    "#     df['date'] = pd.to_datetime(df['date'])\n",
    "#     df.set_index('date', inplace=True)\n",
    "\n",
    "#     # Séparation des données\n",
    "#     train_data = df[:date_train_end]\n",
    "#     test_data = df[date_test_start:]\n",
    "#     val_data = df[date_train_end:date_test_start]\n",
    "\n",
    "#     # Prétraitement des colonnes\n",
    "#     numeric_preprocessing = make_pipeline(SimpleImputer(strategy='median'),\n",
    "#                                           StandardScaler())\n",
    "#     categorical_preprocessing = make_pipeline(\n",
    "#         SimpleImputer(strategy='constant', fill_value='missing'),\n",
    "#         OneHotEncoder())\n",
    "#     preprocessor = ColumnTransformer(transformers=[\n",
    "#         ('num', numeric_preprocessing,\n",
    "#          make_column_selector(\n",
    "#              dtype_include=['int64', 'float64'])),  # type: ignore\n",
    "#         ('cat', categorical_preprocessing,\n",
    "#          make_column_selector(dtype_include='object'))\n",
    "#     ])\n",
    "\n",
    "#     # # Création du pipeline final (à adapter avec votre modèle SARIMAX)\n",
    "#     # final_pipeline = make_pipeline(\n",
    "#     #     preprocessor,\n",
    "#     #     DummyRegressor())  # Remplacez DummyRegressor par votre modèle\n",
    "\n",
    "#     # Utilisation du wrapper SARIAMX\n",
    "#     final_pipeline = make_pipeline(\n",
    "#         preprocessor,\n",
    "#         SARIMAXWrapper(order=(1, 1, 1),\n",
    "#                        seasonal_order=(1, 1, 1, 12))  # Exemple d'ordre SARIMAX\n",
    "#     )\n",
    "#     # Définition des scores\n",
    "#     scoring = {\n",
    "#         'RMSE': make_scorer(mean_squared_error, squared=False),\n",
    "#         'MAE': make_scorer(mean_absolute_error),\n",
    "#         # Ajoutez d'autres scorers si nécessaire\n",
    "#     }\n",
    "\n",
    "#     # Optimisation des hyperparamètres avec GridSearchCV\n",
    "#     tscv = TimeSeriesSplit(\n",
    "#         n_splits=5)  # Pour la validation croisée dans le temps\n",
    "#     grid_search = GridSearchCV(final_pipeline,\n",
    "#                                param_grid=params,\n",
    "#                                cv=tscv,\n",
    "#                                scoring=scoring,\n",
    "#                                refit='RMSE')\n",
    "\n",
    "#     # Entraînement et optimisation\n",
    "#     grid_search.fit(train_data.drop('target', axis=1), train_data['target'])\n",
    "\n",
    "#     # Évaluation sur l'ensemble de test\n",
    "#     y_pred_test = grid_search.predict(test_data.drop('target', axis=1))\n",
    "#     y_pred_val = grid_search.predict(val_data.drop('target', axis=1))\n",
    "#     y_pred_train = grid_search.predict(train_data.drop('target', axis=1))\n",
    "\n",
    "#     # Calcul des scores\n",
    "#     scores_test = {\n",
    "#         score: scorer(test_data['target'], y_pred_test)\n",
    "#         for score, scorer in scoring.items()\n",
    "#     }\n",
    "#     scores_val = {\n",
    "#         score: scorer(val_data['target'], y_pred_val)\n",
    "#         for score, scorer in scoring.items()\n",
    "#     }\n",
    "#     scores_train = {\n",
    "#         score: scorer(train_data['target'], y_pred_train)\n",
    "#         for score, scorer in scoring.items()\n",
    "#     }\n",
    "\n",
    "#     return grid_search.best_estimator_, scores_train, scores_val, scores_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "\n",
    "class SARIMAXWrapper(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Wrapper SARIMAX pour l'intégration avec scikit-learn.\"\"\"\n",
    "\n",
    "    def __init__(self, order=(1, 0, 0), seasonal_order=(0, 0, 0, 0)):\n",
    "        self.order = order\n",
    "        self.seasonal_order = seasonal_order\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Adapter le modèle aux données.\"\"\"\n",
    "        logger.debug(\"Début de l'ajustement du modèle SARIMAX.\")\n",
    "        try:\n",
    "            y = pd.Series(y).asfreq('10T', method='pad')\n",
    "            self.model = SARIMAX(y,\n",
    "                                 order=self.order,\n",
    "                                 seasonal_order=self.seasonal_order,\n",
    "                                 enforce_stationarity=False,\n",
    "                                 enforce_invertibility=False)\n",
    "            self.model = self.model.fit(disp=False)\n",
    "        except Exception as e:\n",
    "            logger.exception(\n",
    "                \"Erreur lors de l'ajustement du modèle SARIMAX: %s\", e)\n",
    "            raise\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Prédire avec le modèle.\"\"\"\n",
    "        logger.debug(\"Début de la prédiction avec le modèle SARIMAX.\")\n",
    "        try:\n",
    "            forecast = self.model.get_forecast(steps=len(X))\n",
    "        except Exception as e:\n",
    "            logger.exception(\n",
    "                \"Erreur lors de la prédiction avec le modèle SARIMAX: %s\", e)\n",
    "            raise\n",
    "        return forecast.predicted_mean\n",
    "\n",
    "\n",
    "def optimiser_serie_temporelle(df, date_train_end, date_test_start,\n",
    "                               nom_colonne_predire, params):\n",
    "    \"\"\"Optimiser un modèle SARIMAX sur des données de série temporelle.\"\"\"\n",
    "    logger.debug(\"Début de l'optimisation du modèle SARIMAX.\")\n",
    "    try:\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df.asfreq('10T')\n",
    "\n",
    "        train_data = df[:date_train_end][nom_colonne_predire]\n",
    "        test_data = df[date_test_start:][nom_colonne_predire]\n",
    "        # Assurez-vous que les dates définissent correctement la plage d'entraînement\n",
    "        print(f\"Taille de l'ensemble d'entraînement: {len(train_data)}\")\n",
    "\n",
    "        # Ajustez le nombre de splits en fonction de la taille de l'ensemble d'entraînement\n",
    "        if len(train_data\n",
    "               ) > 3:  # Nombre minimum pour TimeSeriesSplit avec 3 splits\n",
    "            tscv = TimeSeriesSplit(n_splits=3)\n",
    "        else:\n",
    "            # Réduire le nombre de splits si la taille de l'ensemble d'entraînement est petite\n",
    "            tscv = TimeSeriesSplit(n_splits=1)  # Ou un autre nombre approprié\n",
    "\n",
    "        # tscv = TimeSeriesSplit(n_splits=3)\n",
    "        scoring = {\n",
    "            'RMSE': make_scorer(mean_squared_error, squared=False),\n",
    "            'MAE': make_scorer(mean_absolute_error)\n",
    "        }\n",
    "\n",
    "        grid_search = GridSearchCV(SARIMAXWrapper(),\n",
    "                                   param_grid=params,\n",
    "                                   cv=tscv,\n",
    "                                   scoring=scoring,\n",
    "                                   refit='RMSE')\n",
    "\n",
    "        grid_search.fit(np.arange(len(train_data)), train_data.values)\n",
    "\n",
    "        y_pred_test = grid_search.predict(np.arange(len(test_data)))\n",
    "\n",
    "        scores_test = {\n",
    "            'RMSE':\n",
    "            mean_squared_error(test_data.values, y_pred_test, squared=False),\n",
    "            'MAE':\n",
    "            mean_absolute_error(test_data.values, y_pred_test)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Erreur lors de l'optimisation du modèle SARIMAX: %s\",\n",
    "                         e)\n",
    "        raise\n",
    "    return grid_search.best_estimator_, scores_test, train_data, test_data, y_pred_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71759e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_normalized_12col.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des hyperparameters pour`params`\n",
    "nom_colonne_predire = 'vitesse_(km/h)'\n",
    "# Utilisation de la fonction\n",
    "\n",
    "#  hyperparamtres\n",
    "params = {\n",
    "    'order': [(1, 1, 1), (0, 1, 1)],\n",
    "    'seasonal_order': [(1, 1, 1, 12), (0, 1, 1, 12)]\n",
    "}\n",
    "date_train_end = \"2024-01-01\"\n",
    "date_test_start = \"2024-02-02\"\n",
    "\n",
    "best_estimator, scores_test, train_data, test_data, y_pred = optimiser_serie_temporelle(\n",
    "    df, date_train_end, date_test_start, nom_colonne_predire, params)\n",
    "\n",
    "print(\"Meilleur estimateur:\", best_estimator)\n",
    "print(\"Scores sur l'ensemble de test:\", scores_test)\n",
    "\n",
    "# Meilleur estimateur: SARIMAXWrapper(order=(0, 1, 1), seasonal_order=(0, 1, 1, 12))\n",
    "# Scores sur l'ensemble de test: {'RMSE': 12.389360468518772, 'MAE': 10.830748779134245}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65798c54",
   "metadata": {},
   "source": [
    "### courbe de prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435764b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_predictions(train_data,\n",
    "                     test_data,\n",
    "                     y_pred,\n",
    "                     title=\"Prédictions vs Données Réelles\"):\n",
    "    \"\"\"\n",
    "    Trace les données d'entraînement et de test ainsi que les prédictions du modèle.\n",
    "\n",
    "    :param train_data: DataFrame ou Series contenant les données d'entraînement.\n",
    "    :param test_data: DataFrame ou Series contenant les données de test.\n",
    "    :param y_pred: Les prédictions du modèle pour la période de test.\n",
    "    :param title: Titre du graphique.\n",
    "    \"\"\"\n",
    "    # Concaténer train_data et test_data pour avoir un index continu\n",
    "    full_data = pd.concat([train_data, test_data])\n",
    "\n",
    "    # Calculer l'index où commencent les données de test\n",
    "    test_start = len(train_data)\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # Tracer les données d'entraînement\n",
    "    plt.plot(full_data.index[:test_start],\n",
    "             train_data,\n",
    "             color='blue',\n",
    "             label='Données d\\'entraînement')\n",
    "\n",
    "    # Tracer les données de test\n",
    "    plt.plot(full_data.index[test_start:],\n",
    "             test_data,\n",
    "             color='green',\n",
    "             label='Vérité terrain')\n",
    "\n",
    "    # Tracer les prédictions sur les données de test\n",
    "    # Assurez-vous que l'index de test_data et y_pred correspondent\n",
    "    plt.plot(full_data.index[test_start:test_start + len(y_pred)],\n",
    "             y_pred,\n",
    "             color='red',\n",
    "             linestyle='--',\n",
    "             label='Prédictions')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Valeur')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ab50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=\n",
    "# # Supposons que 'train_data' soit vos données d'entraînement et 'test_data' vos données de test.\n",
    "# 'y_pred' sont les prédictions de votre modèle pour la période de test.\n",
    "plot_predictions(train_data, test_data, y_pred,\n",
    "                 \"Prédictions SARIMAX vs Données Réelles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18590352",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48abbeb7",
   "metadata": {},
   "source": [
    "lstm pour predire le vent et sa direction de la station windsup donc des  colonnes :\n",
    "direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup \n",
    "Avec  normalisation et un ACP\n",
    "\n",
    "extract du df:\n",
    "\n",
    "83552 rows × 230 columns\n",
    "\n",
    "date\tprecipitation_1_heures_(mm)_13005003\ttemperature_(°C)_13005003\ttemperature_point_de_rosee_(°C)_13005003\ttemperature_mini_(°C)_13005003\theure_tps_mini_13005003\ttemperature_maxi_(°C)_13005003\theure_tps_max_13005003\tduration_gel_(mn)_13005003\ttemperature_mini_at_10cm_(°C)_13005003\t...\thumidity_(%)_mobilis\tpression_(bar)_mobilis\ttemperature_(°C)_mobilis\tdirection_(°)_mobilis\tvitesse_(km/h)_mobilis\trafale_(km/h)_mobilis\tdirection_(°)_windsup\tvitesse_vent_(km/h)_windsup\tvitesse_vent_mini_(km/h)_windsup\tvitesse_vent_max_(km/h)_windsup\n",
    "0\t2022-02-15 09:12:00\t0.0\t10.5\t2.30\t10.00\t901.0\t10.50\t955.0\t0.0\t10.50\t...\t74.89\t1018.0\t10.22\t289.0\t45.670\t58.931\t270.0\t86.4\t79.2\t100.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ccb01a",
   "metadata": {},
   "source": [
    "### fonctions pour preparer le dataset avec diffénretes approches  dont :\n",
    "\n",
    "- filtrage des mois et des heures\n",
    "- changement de type de normalisation minMaxScaler, StandardScaler, RobustSQscaller, OnehotEncoder, dummies\n",
    "- test avec et sans ACP\n",
    "- transformation de fourier\n",
    "- moyennage des colonnes de même catégorie en 1 seule colonne "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d898f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Chargement et préparation du dataframe\n",
    "df = df_concatenated_1h.copy()\n",
    "\n",
    "def filter_data(df):\n",
    "    \"\"\"\n",
    "    Filtre un DataFrame pour inclure uniquement les données des mois de juillet et août\n",
    "    et uniquement les heures entre 8h et 20h.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame contenant une colonne 'date' avec des timestamps.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame filtré selon les mois et les heures spécifiées.\n",
    "    Raises:\n",
    "        ValueError: Si la colonne 'date' ne peut pas être convertie en datetime.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Conversion de la colonne 'date' en type datetime\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        # Vérification si la conversion a produit des NaT (Not a Time)\n",
    "        if df['date'].isnull().any():\n",
    "            logging.warning(\"Certains timestamps n'ont pas pu être convertis et sont devenus NaT.\")\n",
    "        # Filtrage par mois\n",
    "        df_filtered_month = df[df['date'].dt.month.isin([7, 8])]\n",
    "        logging.debug(f\"DataFrame après filtrage par mois (juillet et août) : {df_filtered_month.shape[0]} lignes\")\n",
    "        # Filtrage par heure\n",
    "        df_filtered_month_hour = df_filtered_month[(df_filtered_month['date'].dt.hour >= 8) & (df_filtered_month['date'].dt.hour <= 20)]\n",
    "        logging.debug(f\"DataFrame après filtrage par heure (8h à 20h) : {df_filtered_month_hour.shape[0]} lignes\")\n",
    "        return df_filtered_month_hour\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erreur lors du filtrage des données : {e}\")\n",
    "        raise ValueError(\"Erreur lors de la conversion de la colonne 'date'. Assurez-vous qu'elle contient des timestamps valides.\") from e\n",
    "\n",
    "\n",
    "def apply_fourier_transform(df, features):\n",
    "    \"\"\"\n",
    "    Applique la transformation de Fourier sur les caractéristiques spécifiées et enregistre les amplitudes.\n",
    "    Fréquence et Amplitude: Les transformations de Fourier permettent d'analyser les fréquences présentes dans les données de séries temporelles et d'extraire\n",
    "    des informations sur l'amplitude des cycles. Cela peut être particulièrement utile pour la prédiction de phénomènes périodiques tels que les motifs météorologiques.\n",
    "    Extraction de Caractéristiques: Ces transformations peuvent aider à distinguer les caractéristiques qui sont peut-être moins évidentes dans le domaine\n",
    "    temporel mais plus prononcées dans le domaine fréquentiel, fournissant ainsi des informations supplémentaires qui pourraient améliorer la performance du modèle.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Le dataframe source.\n",
    "    features (list): Liste des noms des caractéristiques à transformer.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Le dataframe avec les caractéristiques transformées de Fourier ajoutées.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        fft_values = np.fft.fft(df[feature].fillna(0))  # Utiliser fillna(0) ou une autre méthode pour gérer les NaN\n",
    "        df[f'{feature}_fft_amplitude'] = np.abs(fft_values)\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_features(df, features_to_include):\n",
    "    \"\"\"\n",
    "    Préparation des caractéristiques météorologiques supplémentaires et application des moyennes si nécessaire.\n",
    "    \"\"\"\n",
    "    logging.debug(\"Début de la préparation des caractéristiques.\")\n",
    "    for feature in features_to_include:\n",
    "        feature_cols = [col for col in df.columns if feature in col]\n",
    "        if feature_cols:\n",
    "            df[feature] = df[feature_cols].mean(axis=1)\n",
    "            logging.debug(f\"Caractéristique {feature} créée par moyenne.\")\n",
    "    return df\n",
    "\n",
    "def construction_df(df):\n",
    "    \"\"\"\n",
    "    Construit un DataFrame en filtrant les données, en extrayant certaines colonnes cibles et en générant de nouvelles colonnes de caractéristiques.\n",
    "    Args:\n",
    "        df (pd.DataFrame): Le DataFrame original à traiter.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame traité avec les colonnes cibles et les nouvelles caractéristiques.\n",
    "    Raises:\n",
    "        KeyError: Si les colonnes nécessaires ne sont pas présentes dans le DataFrame.\n",
    "        Exception: Pour toute autre erreur rencontrée lors du traitement des données.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = filter_data(df)\n",
    "        logging.debug(f\"DataFrame après filtrage :\\n{df.head()}\")\n",
    "        #------\n",
    "        # features_to_transform = ['direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']\n",
    "        features_to_include = ['temperature', 'pression', 'humidity', 'Enthalpie']\n",
    "        df = prepare_features(df, features_to_include)\n",
    "        # targets_columns = ['direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']\n",
    "        # df_final = df[targets_columns + features_to_include + [f'{feat}_fft_amplitude' for feat in features_to_transform]]\n",
    "        logging.debug(\"DataFrame final prêt pour l'entraînement.\")\n",
    "\n",
    "        #------\n",
    "\n",
    "        targets_columns = ['direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']\n",
    "        df = apply_fourier_transform(df, targets_columns)\n",
    "        df_targets_columns = df[targets_columns]\n",
    "        logging.debug(\"Colonnes cibles extraites pour la prédiction.\")\n",
    "\n",
    "        # features_to_include = ['temperature', 'pression', 'humidity']\n",
    "        for feature in features_to_include:\n",
    "            feature_cols = [col for col in df.columns if feature in col]\n",
    "            if feature_cols:\n",
    "                df[feature] = df[feature_cols].mean(axis=1)\n",
    "                logging.debug(f\"Feature {feature} calculée comme moyenne des colonnes {feature_cols}\")\n",
    "\n",
    "        # Suppression des colonnes avec un underscore dans leur nom (colonnes originales, non agrégées)\n",
    "        cols_to_drop = [col for col in df.columns if '_' in col]\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "        logging.debug(\"Colonnes inutilisées supprimées.\")\n",
    "\n",
    "        # Concaténation des colonnes cibles de retour avec les nouvelles caractéristiques\n",
    "        df_final = pd.concat([df, df_targets_columns], axis=1)\n",
    "        logging.debug(\"DataFrame final prêt avec les nouvelles caractéristiques et les cibles.\")\n",
    "\n",
    "        return df_final\n",
    "\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"KeyError: {e}\")\n",
    "        raise KeyError(f\"Column not found in DataFrame: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        raise Exception(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0180891",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def construction_df(df):\n",
    "#     df = filter_data(df)\n",
    "#     logger.debug(f\"\\n liste colonnes df:\\n{df.columns.tolist()} \")\n",
    "#     logger.debug(f\"\\ndf :\\n{df.head(5)} \")\n",
    "#     targets_columns = ['direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']\n",
    "#     df_targets_columns = df[targets_columns]\n",
    "#     logger.debug(f\"\\n liste colonnes df_targets_columns:\\n{df_targets_columns.columns.tolist()} \")\n",
    "#     # Assurez-vous que ces colonnes sont présentes dans votre DataFrame\n",
    "#     features_to_include = ['temperature', 'pression', 'humidity']\n",
    "\n",
    "#     if any('temperature' in col for col in df.columns):\n",
    "#         # Si plusieurs colonnes contiennent 'temperature', vous pourriez vouloir les moyenne ou prendre la première\n",
    "#         temperature_cols = [col for col in df.columns if 'temperature' in col]\n",
    "#         df['temperature'] = df[temperature_cols].mean(axis=1)  # Prend la moyenne de toutes les colonnes de température\n",
    "\n",
    "#     if any('pression' in col for col in df.columns):\n",
    "#         pression_cols = [col for col in df.columns if 'pression' in col]\n",
    "#         df['pression'] = df[pression_cols].mean(axis=1)  # Idem pour pression\n",
    "\n",
    "#     if any('humidity' in col for col in df.columns):\n",
    "#         humidity_cols = [col for col in df.columns if 'humidity' in col]\n",
    "#         df['humidite'] = df[humidity_cols].mean(axis=1)\n",
    "#     if any('Enthalpie' in col for col in df.columns):\n",
    "#         humidity_cols = [col for col in df.columns if 'Enthalpie' in col]\n",
    "#         df['enthalpie'] = df[humidity_cols].mean(axis=1)\n",
    "\n",
    "#     logger.debug(f\"\\ndf['temperature'] :\\n{df['temperature'].head(5)} \")\n",
    "#     logger.debug(f\"\\ndf['pression']  :\\n{df['pression'].head(5) } \")\n",
    "#     logger.debug(f\"\\n df['humidite'] :\\n{df['humidite'].head(5)} \")\n",
    "#     logger.debug(f\"\\n df['enthalpie'] :\\n{df['enthalpie'].head(5)} \")\n",
    "#     # Suppression colonne\n",
    "#     col = [col for col in df.columns if '_' in col]\n",
    "#     df.drop(columns=col, inplace=True)\n",
    "#     df = pd.concat([df, df_targets_columns], axis=1)\n",
    "#     logger.debug(f\"\\n liste colonnes df:\\n{df.columns.tolist()} \")\n",
    "#     logger.debug(f\"\\n liste colonnes df:\\n{df.head(5)} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff5575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "\n",
    "# Sélection des colonnes pertinentes\n",
    "df=df_concatenated_1h.copy()\n",
    "\n",
    "#----------------------\n",
    "# Étape 1: Préparation et ACP\n",
    "#----------------------\n",
    "\n",
    "# Pour appliquer l'ACP, nous devons d'abord normaliser les données puis appliquer l'ACP pour réduire la dimensionnalité.\n",
    "\n",
    "# Normalisation des données\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df.drop(columns=['date']))  # Exclure la colonne date pour la normalisation\n",
    "\n",
    "# Application de l'ACP\n",
    "pca = PCA(n_components=0.95)  # Conserver 95% de la variance expliquée\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Vérifier le nombre de composantes principales retenues\n",
    "print(f\"Nombre de composantes principales : {pca.n_components_}\")\n",
    "\n",
    "# Ajout de la colonne 'date' pour garder la référence temporelle\n",
    "df_pca = pd.DataFrame(df_pca, columns=[f\"PC{i+1}\" for i in range(pca.n_components_)])\n",
    "df_pca['date'] = df['date'].values\n",
    "df_pca.set_index('date', inplace=True)\n",
    "\n",
    "# Sélectionner les données de vitesse et de direction du vent après ACP pour LSTM\n",
    "columns = ['PC1', 'PC2']  # Les deux premières composantes principales\n",
    "df_pca = df_pca[columns]\n",
    "\n",
    "# Conversion des données en séquences pour l'entraînement LSTM\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:(i + seq_length)]\n",
    "        y = data[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "\n",
    "#----------------------\n",
    "# Étape 2: Préparation des données pour LSTM\n",
    "#----------------------\n",
    "\n",
    "# Maintenant que les données sont transformées par ACP, nous pouvons créer les séquences pour le LSTM.\n",
    "\n",
    "# Normalisation des données PCA\n",
    "scaler_pca = MinMaxScaler(feature_range=(0, 1))\n",
    "df_pca_scaled = scaler_pca.fit_transform(df_pca)\n",
    "\n",
    "# Création de séquences\n",
    "X, y = create_sequences(df_pca_scaled, seq_length=10)  # Utiliser la même fonction 'create_sequences' définie précédemment\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "#----------------------\n",
    "# Étape 3: Construction et entraînement du modèle LSTM\n",
    "#----------------------\n",
    "\n",
    "# Construction du modèle LSTM\n",
    "epoch=50\n",
    "batch_size=32\n",
    "model_pca = Sequential([\n",
    "    Input(shape=(X_train.shape[1], X_train.shape[2])),  # Utiliser Input pour définir la forme d'entrée\n",
    "    LSTM(50, activation='relu', return_sequences=True),\n",
    "    LSTM(50, activation='relu'),\n",
    "    Dense(2)  # Prédire deux sorties, adaptées aux données après ACP\n",
    "])\n",
    "# Compilation du modèle\n",
    "model_pca.compile(optimizer='adam', loss='mse')\n",
    "model_pca.summary()\n",
    "\n",
    "# Entraînement du modèle\n",
    "history_pca = model_pca.fit(X_train, y_train, epochs=epoch, batch_size=batch_size, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Évaluation du modèle\n",
    "model_pca.evaluate(X_test, y_test)\n",
    "#----------------------\n",
    "# Étape 4: Prédiction et évaluation visuelle\n",
    "#----------------------\n",
    "\n",
    "# Prédiction\n",
    "predicted_pca = model_pca.predict(X_test)\n",
    "predicted_pca_values = scaler_pca.inverse_transform(predicted_pca)  # Retour aux valeurs originales\n",
    "\n",
    "# Comparaison visuelle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(y_test[:, 0], label='Réel PC1')\n",
    "plt.plot(predicted_pca_values[:, 0], label='Prédit PC1')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(y_test[:, 1], label='Réel PC2')\n",
    "plt.plot(predicted_pca_values[:, 1], label='Prédit PC2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86fd89b",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e45068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sélection des colonnes pertinentes\n",
    "df = df_concatenated_1h.copy()\n",
    "\n",
    "\n",
    "# Remplacer les valeurs manquantes et normaliser\n",
    "# df_target.fillna(method='ffill', inplace=True)  # Exemple simple de gestion des valeurs manquantes\n",
    "\n",
    "# scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "# df_scaled_target = scaler_target.fit_transform(df_target.drop(columns=['date']))\n",
    "\n",
    "\n",
    "#----------------------\n",
    "# Étape 1: Préparation et ACP\n",
    "#----------------------\n",
    "# Sélectionner les colonnes spécifiques pour la prédiction\n",
    "df_target = df[['date', 'direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']].copy()\n",
    "\n",
    "# Normalisation des données avec les colonnes spécifiques pour la prédiction\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "df_scaled_target = scaler_target.fit_transform(df_target.drop(columns=['date']))\n",
    "\n",
    "# Normalisation des données\n",
    "# scaler = StandardScaler()\n",
    "# df_scaled = scaler.fit_transform(df.drop(columns=['date']))  # Exclure la colonne date pour la normalisation\n",
    "\n",
    "# Application de l'ACP\n",
    "pca = PCA(n_components=0.95)  # Conserver 95% de la variance expliquée\n",
    "# df_pca = pca.fit_transform(df_scaled)\n",
    "df_pca = pca.fit_transform(df_scaled_target)  # integratio des colonnes à predire\n",
    "# Vérifier le nombre de composantes principales retenues\n",
    "print(f\"Nombre de composantes principales : {pca.n_components_}\")\n",
    "\n",
    "# Ajout de la colonne 'date' pour garder la référence temporelle\n",
    "df_pca = pd.DataFrame(df_pca, columns=[f\"PC{i+1}\" for i in range(pca.n_components_)])\n",
    "df_pca['date'] = df['date'].values\n",
    "df_pca.set_index('date', inplace=True)\n",
    "\n",
    "# Sélectionner les données de vitesse et de direction du vent après ACP pour LSTM\n",
    "columns = ['PC1', 'PC2']  # Les deux premières composantes principales\n",
    "df_pca = df_pca[columns]\n",
    "\n",
    "\n",
    "# Conversion des données en séquences pour l'entraînement LSTM\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:(i + seq_length)]\n",
    "        y = data[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "\n",
    "#----------------------\n",
    "# Étape 2: Préparation des données pour LSTM\n",
    "#----------------------\n",
    "\n",
    "# Normalisation des données PCA\n",
    "scaler_pca = MinMaxScaler(feature_range=(0, 1))\n",
    "df_pca_scaled = scaler_pca.fit_transform(df_pca)\n",
    "\n",
    "# Création de séquences\n",
    "# X, y = create_sequences(df_pca_scaled, seq_length=10)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Créer les séquences avec integration ds 2 colonnes à predires\n",
    "X_target, y_target = create_sequences(df_scaled_target, seq_length=10)\n",
    "X_train_target, X_test_target, y_train_target, y_test_target = train_test_split(X_target, y_target, test_size=0.2, random_state=0)\n",
    "\n",
    "#----------------------\n",
    "# Étape 3: Construction et entraînement du modèle LSTM\n",
    "#----------------------\n",
    "\n",
    "# model_pca = Sequential(\n",
    "#     [Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "#      LSTM(50, activation='tanh', return_sequences=True),\n",
    "#      LSTM(50, activation='tanh'),\n",
    "#      Dense(2)])\n",
    "model_pca = Sequential([\n",
    "    Input(shape=(X_train_target.shape[1], X_train_target.shape[2])),\n",
    "    LSTM(50, activation='tanh', return_sequences=True),\n",
    "    LSTM(50, activation='tanh'),\n",
    "    Dense(2)\n",
    "])\n",
    "\n",
    "model_pca.compile(optimizer='adam', loss='mse')\n",
    "model_pca.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "history_pca = model_pca.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping, model_checkpoint], verbose=1)\n",
    "\n",
    "model_pca.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a620eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pca.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d343dbe",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f85147",
   "metadata": {},
   "source": [
    " #### log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5571d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconfigurer_logging(params={\"niveau_log\": 'DEBUG'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babdc42b",
   "metadata": {},
   "source": [
    "#### test de selection des colonnes pertinentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f12c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconfigurer_logging(params={\"niveau_log\": 'DEBUG'})\n",
    "\n",
    "df = df_concatenated_1h.copy()\n",
    "logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "\n",
    "\n",
    "# Filtrage des colonnes les plus pertienentes\n",
    "# useful_columns = [col for col in df.columns if 'vitesse' in col or 'direction' in col]\n",
    "useful_columns = ['date'] + [col for col in df.columns if 'vitesse' in col or 'direction' in col]\n",
    "df_for_pca = df[useful_columns]\n",
    "logger.debug(f\"\\n liste colonnes df_for_pca:\\n{df_for_pca.columns.tolist()} \")\n",
    "# Sélectionner les colonnes spécifiques pour la prédiction\n",
    "df_target = df[['date', 'direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']].copy()\n",
    "logger.debug(f\"\\n liste colonnes df_target:\\n{df_target.columns.tolist()} \")\n",
    "\n",
    "\n",
    "X = df_for_pca.drop(\n",
    "    columns=['date', 'direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup', 'vitesse_vent_mini_(km/h)_windsup', 'vitesse_vent_max_(km/h)_windsup'], axis=1)\n",
    "logger.debug(f\"\\n liste colonnes Xt:\\n{X.columns.tolist()} \")\n",
    "\n",
    "y = df_for_pca[['direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']]\n",
    "logger.debug(f\"\\n liste colonnes y:\\n{y.columns.tolist()} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9511a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import LSTM, Dense, Input\n",
    "# from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# # Sélection des colonnes pertinentes\n",
    "# df = df_concatenated_1h.copy()\n",
    "\n",
    "# #----------------------\n",
    "# # Étape 1: Préparation et ACP\n",
    "# #----------------------\n",
    "\n",
    "# # Filtrage des colonnes les plus pertienentes\n",
    "# useful_columns = [col for col in df.columns if 'vitesse' in col or 'direction' in col]\n",
    "# df_for_pca = df[useful_columns]\n",
    "# # Sélectionner les colonnes spécifiques pour la prédiction\n",
    "# df_target = df[['date', 'direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']].copy()\n",
    "\n",
    "# # Gestion des valeurs manquantes\n",
    "# # df_target.fillna(method='ffill', inplace=True)  # Exemple simple de gestion des valeurs manquantes\n",
    "\n",
    "# # Exclure la colonne date pour la normalisation\n",
    "# scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "# df_scaled_target = scaler_target.fit_transform(df_target.drop(columns=['date']))\n",
    "\n",
    "# # Application de l'ACP\n",
    "# pca = PCA(n_components=0.95)  # Conserver 95% de la variance expliquée\n",
    "# # df_pca = pca.fit_transform(df_scaled_target)\n",
    "# df_pca = pca.fit_transform(df_for_pca)\n",
    "\n",
    "# # Vérifier le nombre de composantes principales retenues\n",
    "# print(f\"Nombre de composantes principales : {pca.n_components_}\")\n",
    "\n",
    "# # Ajout de la colonne 'date' pour garder la référence temporelle après ACP\n",
    "# df_pca = pd.DataFrame(df_pca, columns=[f\"PC{i+1}\" for i in range(pca.n_components_)])\n",
    "# df_pca['date'] = df_target['date'].values\n",
    "# df_pca.set_index('date', inplace=True)\n",
    "\n",
    "# #----------------------\n",
    "# # Étape 2: Préparation des données pour LSTM\n",
    "# #----------------------\n",
    "\n",
    "# # Normalisation des données PCA après avoir enlevé la colonne 'date'\n",
    "# scaler_pca = MinMaxScaler(feature_range=(0, 1))\n",
    "# df_pca_scaled = scaler_pca.fit_transform(df_pca)  # La colonne 'date' n'est plus dans les données à ce point\n",
    "\n",
    "\n",
    "# # Création de séquences avec intégration des 2 colonnes à prédire\n",
    "# def create_sequences(data, seq_length):\n",
    "#     xs, ys = [], []\n",
    "#     for i in range(len(data) - seq_length):\n",
    "#         x = data[i:(i + seq_length)]\n",
    "#         y = data[i + seq_length]\n",
    "#         xs.append(x)\n",
    "#         ys.append(y)\n",
    "#     return np.array(xs), np.array(ys)\n",
    "\n",
    "\n",
    "# X_target, y_target = create_sequences(df_pca_scaled, seq_length=10)\n",
    "# X_train_target, X_test_target, y_train_target, y_test_target = train_test_split(X_target, y_target, test_size=0.2, random_state=0)\n",
    "\n",
    "# #----------------------\n",
    "# # Étape 3: Construction et entraînement du modèle LSTM\n",
    "# #----------------------\n",
    "\n",
    "# # model_pca = Sequential([\n",
    "# #     Input(shape=(X_train_target.shape[1], X_train_target.shape[2])),\n",
    "# #     LSTM(50, activation='tanh', return_sequences=True),\n",
    "# #     LSTM(50, activation='tanh'),\n",
    "# #     Dense(2)\n",
    "# # ])\n",
    "\n",
    "# model_pca = Sequential([\n",
    "#     LSTM(100, return_sequences=True, input_shape=(X_train_target.shape[1], X_train_target.shape[2])),\n",
    "#     LSTM(100, return_sequences=True),\n",
    "#     LSTM(50),\n",
    "#     Dense(50, activation='relu'),\n",
    "#     Dense(2)  # Sortie pour deux prédictions: vitesse et direction\n",
    "# ])\n",
    "\n",
    "# model_pca.compile(optimizer='adam', loss='mse')\n",
    "# model_pca.summary()\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='min', restore_best_weights=True)\n",
    "# model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "# history_pca = model_pca.fit(X_train_target,\n",
    "#                             y_train_target,\n",
    "#                             epochs=200,\n",
    "#                             batch_size=64,\n",
    "#                             validation_split=0.2,\n",
    "#                             callbacks=[early_stopping, model_checkpoint],\n",
    "#                             verbose=1)\n",
    "\n",
    "# model_pca.evaluate(X_test_target, y_test_target)\n",
    "\n",
    "# # 0.0010788545478135347\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c88cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Chargement et préparation du dataframe\n",
    "df = df_concatenated_1h.copy()\n",
    "\n",
    "\n",
    "# Filtrer pour obtenir uniquement les mois de juillet et août\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df_filtered_month = df[df['date'].dt.month.isin([7, 8])]\n",
    "logger.debug(f\"\\n df_filtered_data:\\n{df_filtered_month} \")\n",
    "\n",
    "# Filtrer pour obtenir uniquement les mois de juillet et août ET les heures entre 8h et 20h\n",
    "# df_filtered_month_hour = df_filtered_month[(df_filtered_month['date'].dt.hour >= 8) & (df_filtered_month['date'].dt.hour <= 20)]\n",
    "# Ajout du filtrage horaire entre 8h et 20h\n",
    "df_filtered_month_hour = df_filtered_month[(df_filtered_month['date'].dt.hour >= 8) & (df_filtered_month['date'].dt.hour <= 20)]\n",
    "\n",
    "#\n",
    "logger.debug(f\"\\n df_filtered_data:\\n{df_filtered_month_hour} \")\n",
    "\n",
    "\n",
    "\n",
    "# # Préparation des données\n",
    "# df = df_concatenated_1h.copy()\n",
    "\n",
    "# Séparation des features (pour l'ACP) et des targets (non affectées par l'ACP)\n",
    "features_columns = [col for col in df.columns if ('vitesse' in col or 'direction' in col) and 'windsup' not in col]\n",
    "targets_columns = ['direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']\n",
    "logger.debug(f\"\\n features_columns:\\n{features_columns} \")\n",
    "logger.debug(f\"\\n targets_columns:\\n{targets_columns} \")\n",
    "\n",
    "df_features = df[features_columns]\n",
    "df_targets = df[['date'] + targets_columns].copy()\n",
    "df['date'] = pd.to_datetime(df['date'])  # Assurer que 'date' est au format datetime\n",
    "dates = df['date']\n",
    "logger.debug(f\"\\n dates :\\n{dates} \")\n",
    "\n",
    "# Normalisation des features\n",
    "scaler = MinMaxScaler()\n",
    "df_features_scaled = scaler.fit_transform(df_features)\n",
    "\n",
    "# Application de l'ACP pour réduire la dimensionnalité des features\n",
    "pca = PCA(n_components=0.95)\n",
    "df_pca = pca.fit_transform(df_features_scaled)\n",
    "\n",
    "# Reconstruction du DataFrame après ACP\n",
    "df_pca = pd.DataFrame(df_pca, index=dates)\n",
    "df_pca.reset_index(inplace=True)\n",
    "df_pca.rename(columns={'index': 'date'}, inplace=True)\n",
    "df_pca['date'] = pd.to_datetime(df_pca['date'])  # Convertir 'date' en datetime\n",
    "\n",
    "# Joindre les targets avec le DataFrame transformé par ACP\n",
    "df_targets.loc[:, 'date'] = pd.to_datetime(df_targets['date'])  # Convertir 'date' en datetime\n",
    "\n",
    "# Définition des  index  pour la concaténation\n",
    "df_pca.set_index('date', inplace=True)\n",
    "df_targets.set_index('date', inplace=True)\n",
    "\n",
    "# Concaténation le long de l'axe des colonnes\n",
    "df_final = pd.concat([df_pca, df_targets], axis=1)\n",
    "\n",
    "# df_final = pd.merge(df_pca, df_targets, on='date', how='left')\n",
    "\n",
    "# Utilisation de df_final pour la formation de modèle\n",
    "X = df_final.drop(targets_columns, axis=1)\n",
    "y = df_final[targets_columns]\n",
    "\n",
    "logger.debug(f\"\\nX :\\n{X} \")\n",
    "\n",
    "\n",
    "\n",
    "# Création des séquences pour LSTM\n",
    "def create_sequences(X, y, sequence_length=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - sequence_length):\n",
    "        Xs.append(X.iloc[i:(i + sequence_length)].values)\n",
    "        ys.append(y.iloc[i + sequence_length].values)\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "\n",
    "sequence_length = 10  # Longueur de la séquence pour LSTM\n",
    "X_seq, y_seq = create_sequences(X, y, sequence_length)\n",
    "\n",
    "# Split des données en train et test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
    "\n",
    "# Construction du modèle LSTM\n",
    "model = Sequential(\n",
    "    [Input(shape=(sequence_length, X_train.shape[2])),\n",
    "\n",
    "    LSTM(100, return_sequences=True),\n",
    "    LSTM(100),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(2)])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, verbose=1)\n",
    "\n",
    "# Entraînement du modèle\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=64, validation_split=0.2, callbacks=[early_stopping, model_checkpoint], verbose=1)\n",
    "\n",
    "# Évaluation du modèle\n",
    "model.evaluate(X_test, y_test)\n",
    "# model.evaluate(X_test, y_test) 572.81396484375 avec epoch 200 en 32 min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc0520",
   "metadata": {},
   "source": [
    "#### amelioration avec iteration sur plusieurs hyperparametre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016debff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Chargement et préparation du dataframe\n",
    "df = df_concatenated_1h.copy()\n",
    "\n",
    "# Filtrer pour obtenir uniquement les mois de juillet et août\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df_filtered_month = df[df['date'].dt.month.isin([7, 8])]\n",
    "logger.debug(f\"\\n df_filtered_data:\\n{df_filtered_month} \")\n",
    "\n",
    "# Filtrer pour obtenir uniquement les mois de juillet et août ET les heures entre 8h et 20h\n",
    "# df_filtered_month_hour = df_filtered_month[(df_filtered_month['date'].dt.hour >= 8) & (df_filtered_month['date'].dt.hour <= 20)]\n",
    "# Ajout du filtrage horaire entre 8h et 20h\n",
    "df_filtered_month_hour = df_filtered_month[(df_filtered_month['date'].dt.hour >= 8) & (df_filtered_month['date'].dt.hour <= 20)]\n",
    "\n",
    "#\n",
    "logger.debug(f\"\\n df_filtered_data:\\n{df_filtered_month_hour} \")\n",
    "df = df_filtered_month_hour.copy()\n",
    "\n",
    "# Séparation des features (pour l'ACP) et des targets (non affectées par l'ACP)\n",
    "features_columns = [col for col in df.columns if ('vitesse' in col or 'direction' in col) and 'windsup' not in col]\n",
    "targets_columns = ['direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']\n",
    "logger.debug(f\"\\n features_columns:\\n{features_columns} \")\n",
    "logger.debug(f\"\\n targets_columns:\\n{targets_columns} \")\n",
    "\n",
    "df_features = df[features_columns]\n",
    "df_targets = df[['date'] + targets_columns].copy()\n",
    "df['date'] = pd.to_datetime(df['date'])  # Assurer que 'date' est au format datetime\n",
    "dates = df['date']\n",
    "logger.debug(f\"\\n dates :\\n{dates} \")\n",
    "\n",
    "# Normalisation des features\n",
    "scaler = MinMaxScaler()\n",
    "df_features_scaled = scaler.fit_transform(df_features)\n",
    "\n",
    "# Application de l'ACP pour réduire la dimensionnalité des features\n",
    "pca = PCA(n_components=0.95)\n",
    "df_pca = pca.fit_transform(df_features_scaled)\n",
    "\n",
    "# Reconstruction du DataFrame après ACP\n",
    "df_pca = pd.DataFrame(df_pca, index=dates)\n",
    "df_pca.reset_index(inplace=True)\n",
    "df_pca.rename(columns={'index': 'date'}, inplace=True)\n",
    "df_pca['date'] = pd.to_datetime(df_pca['date'])  # Convertir 'date' en datetime\n",
    "\n",
    "# Joindre les targets avec le DataFrame transformé par ACP\n",
    "df_targets.loc[:, 'date'] = pd.to_datetime(df_targets['date'])  # Convertir 'date' en datetime\n",
    "\n",
    "# Définition des  index  pour la concaténation\n",
    "df_pca.set_index('date', inplace=True)\n",
    "df_targets.set_index('date', inplace=True)\n",
    "\n",
    "# Concaténation le long de l'axe des colonnes\n",
    "df_final = pd.concat([df_pca, df_targets], axis=1)\n",
    "\n",
    "# df_final = pd.merge(df_pca, df_targets, on='date', how='left')\n",
    "\n",
    "# Utilisation de df_final pour la formation de modèle\n",
    "X = df_final.drop(targets_columns, axis=1)\n",
    "y = df_final[targets_columns]\n",
    "\n",
    "logger.debug(f\"\\nX :\\n{X} \")\n",
    "\n",
    "# Index des colonnes des cibles avant de transformer en séquences\n",
    "targets_columns = ['direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']\n",
    "index_direction = df_targets.columns.get_loc('direction_(°)_windsup')\n",
    "index_speed = df_targets.columns.get_loc('vitesse_vent_(km/h)_windsup')\n",
    "\n",
    "\n",
    "# Création des séquences pour LSTM\n",
    "def create_sequences(X, y, sequence_length=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - sequence_length):\n",
    "        Xs.append(X.iloc[i:(i + sequence_length)].values)\n",
    "        ys.append(y.iloc[i + sequence_length].values)\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "\n",
    "sequence_length = 10\n",
    "X_seq, y_seq = create_sequences(X, y, sequence_length)\n",
    "\n",
    "# Séparation des données en ensembles de formation et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définition des hyperparamètres pour la recherche\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [32, 64, 128]\n",
    "epochs = 200\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        model = Sequential([\n",
    "            Input(shape=(sequence_length, X_train.shape[2])),\n",
    "            LSTM(100, return_sequences=True),\n",
    "            LSTM(100),\n",
    "            Dense(50, activation='relu'),\n",
    "            Dense(2)  # Assurez-vous que la sortie a la bonne taille (2 dans ce cas pour direction et vitesse)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n",
    "\n",
    "        # Entraînement du modèle\n",
    "        history = model.fit(X_train,\n",
    "                            y_train,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            validation_split=0.2,\n",
    "                            verbose=1,\n",
    "                            callbacks=[EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)])\n",
    "\n",
    "        # Évaluation du modèle\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        # Calcul des métriques pour la direction et la vitesse\n",
    "        mse_direction = mean_squared_error(y_test[:, index_direction], predictions[:, index_direction])\n",
    "        rmse_direction = sqrt(mse_direction)\n",
    "        mae_direction = mean_absolute_error(y_test[:, index_direction], predictions[:, index_direction])\n",
    "\n",
    "        mse_speed = mean_squared_error(y_test[:, index_speed], predictions[:, index_speed])\n",
    "        rmse_speed = sqrt(mse_speed)\n",
    "        mae_speed = mean_absolute_error(y_test[:, index_speed], predictions[:, index_speed])\n",
    "\n",
    "        print(f\"Learning Rate: {lr}, Batch Size: {batch_size}\")\n",
    "        print(\"Direction - MSE:\", mse_direction, \"RMSE:\", rmse_direction, \"MAE:\", mae_direction)\n",
    "        print(\"Speed - MSE:\", mse_speed, \"RMSE:\", rmse_speed, \"MAE:\", mae_speed)\n",
    "\n",
    "        # Tracé des pertes\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efbd06a",
   "metadata": {},
   "source": [
    "#### version multimodeles et filtage mois et heures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8eb9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import LSTM, SimpleRNN, GRU, Conv1D, MaxPooling1D, Dense, Flatten, Input\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Fonction pour filtrer les données par mois et heure\n",
    "def filter_data(df):\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df_filtered_month = df[df['date'].dt.month.isin([7, 8])]\n",
    "    df_filtered_month_hour = df_filtered_month[(df_filtered_month['date'].dt.hour >= 8) & (df_filtered_month['date'].dt.hour <= 20)]\n",
    "    return df_filtered_month_hour\n",
    "\n",
    "# Fonction pour créer des séquences pour l'entrainement des modèles de séries temporelles\n",
    "def create_sequences(X, y, sequence_length=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - sequence_length):\n",
    "        Xs.append(X.iloc[i:(i + sequence_length)].values)\n",
    "        ys.append(y.iloc[i + sequence_length].values)\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Fonction principale pour entraîner et évaluer le modèle\n",
    "def train_and_evaluate_model(model_type, df, features_columns, targets_columns, sequence_length=10, learning_rate=0.001, batch_size=64, epochs=100):\n",
    "    df_filtered = filter_data(df)\n",
    "    logger.debug(f\"\\n df_filtered:\\n{df_filtered.head(5)} \")\n",
    "    df_features = df_filtered[features_columns]\n",
    "    logger.debug(f\"\\n df_features:\\n{df_features.head(5)} \")\n",
    "    df_targets = df_filtered[['date'] + targets_columns].copy()\n",
    "    df_filtered['date'] = pd.to_datetime(df_filtered['date'])  # Assurer que 'date' est au format datetime\n",
    "    dates = df_filtered['date']\n",
    "    logger.debug(f\"\\n dates:\\n{dates.head(5)} \")\n",
    "\n",
    "    #---------\n",
    "    # Séparation des features (pour l'ACP) et des targets (non affectées par l'ACP)\n",
    "    # features_columns = [col for col in df.columns if ('vitesse' in col or 'direction' in col) and 'windsup' not in col]\n",
    "    # targets_columns = ['direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']\n",
    "    logger.debug(f\"\\n features_columns:\\n{features_columns} \")\n",
    "    logger.debug(f\"\\n targets_columns:\\n{targets_columns} \")\n",
    "\n",
    "    # df_features = df[features_columns]\n",
    "    # df_targets = df[['date'] + targets_columns].copy()\n",
    "    # dates = df['date']\n",
    "    # logger.debug(f\"\\n dates :\\n{dates} \")\n",
    "    #---------\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_features_scaled = scaler.fit_transform(df_features)\n",
    "\n",
    "    df_pca = PCA(n_components=0.95).fit_transform(df_features_scaled)\n",
    "    df_pca = pd.DataFrame(df_pca, index=dates)\n",
    "    df_pca.reset_index(inplace=True)\n",
    "    df_pca.rename(columns={'index': 'date'}, inplace=True)\n",
    "    df_pca.set_index('date', inplace=True)\n",
    "    df_targets.set_index('date', inplace=True)\n",
    "\n",
    "    df_final = pd.concat([df_pca, df_targets], axis=1)\n",
    "    X = df_final.drop(targets_columns, axis=1)\n",
    "    y = df_final[targets_columns]\n",
    "\n",
    "    X_seq, y_seq = create_sequences(X, y, sequence_length)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Choix du modèle en fonction du type spécifié\n",
    "    if model_type == 'LSTM':\n",
    "        model = Sequential([Input(shape=(sequence_length, X_train.shape[2])), LSTM(100, return_sequences=True), LSTM(100), Dense(50, activation='relu'), Dense(len(targets_columns))])\n",
    "    elif model_type == 'RNN':\n",
    "        model = Sequential([Input(shape=(sequence_length, X_train.shape[2])), SimpleRNN(100, return_sequences=True), SimpleRNN(100), Dense(50, activation='relu'), Dense(len(targets_columns))])\n",
    "    elif model_type == 'GRU':\n",
    "        model = Sequential([Input(shape=(sequence_length, X_train.shape[2])), GRU(100, return_sequences=True), GRU(100), Dense(50, activation='relu'), Dense(len(targets_columns))])\n",
    "    elif model_type == 'CNN':\n",
    "        model = Sequential([Input(shape=(sequence_length, X_train.shape[2])), Conv1D(filters=64, kernel_size=3, activation='relu'), MaxPooling1D(pool_size=2), Flatten(), Dense(50, activation='relu'), Dense(len(targets_columns))])\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.2,\n",
    "                        verbose=1,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)])\n",
    "\n",
    "    # Évaluation du modèle\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Calcul des métriques pour la direction et la vitesse\n",
    "    mse_direction = mean_squared_error(y_test[:, index_direction], predictions[:, index_direction])\n",
    "    rmse_direction = sqrt(mse_direction)\n",
    "    mae_direction = mean_absolute_error(y_test[:, index_direction], predictions[:, index_direction])\n",
    "\n",
    "    mse_speed = mean_squared_error(y_test[:, index_speed], predictions[:, index_speed])\n",
    "    rmse_speed = sqrt(mse_speed)\n",
    "    mae_speed = mean_absolute_error(y_test[:, index_speed], predictions[:, index_speed])\n",
    "\n",
    "    print(f\"Learning Rate: {lr}, Batch Size: {batch_size}\")\n",
    "    print(\"Direction - MSE:\", mse_direction, \"RMSE:\", rmse_direction, \"MAE:\", mae_direction)\n",
    "    print(\"Speed - MSE:\", mse_speed, \"RMSE:\", rmse_speed, \"MAE:\", mae_speed)\n",
    "\n",
    "    # Tracé des pertes\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf331e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TEST\n",
    "# Chargement et préparation du dataframe\n",
    "df = df_concatenated_1h.copy()\n",
    "\n",
    "model_type = ['LSTM','GRU','Conv1D','SimpleRNN']\n",
    "\n",
    "\n",
    "# Séparation des features (pour l'ACP) et des targets (non affectées par l'ACP)\n",
    "features_columns = [col for col in df.columns if ('vitesse' in col or 'direction' in col) and 'windsup' not in col]\n",
    "targets_columns = ['direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']\n",
    "logger.debug(f\"\\n features_columns:\\n{features_columns} \")\n",
    "logger.debug(f\"\\n targets_columns:\\n{targets_columns} \")\n",
    "\n",
    "# df_features = df[features_columns]\n",
    "# df_targets = df[['date'] + targets_columns].copy()\n",
    "# df['date'] = pd.to_datetime(df['date'])  # Assurer que 'date' est au format datetime\n",
    "# dates = df['date']\n",
    "# logger.debug(f\"\\n dates :\\n{dates} \")\n",
    "\n",
    "\n",
    "# # Index des colonnes des cibles avant de transformer en séquences\n",
    "# targets_columns = ['direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']\n",
    "# index_direction = df_targets.columns.get_loc('direction_(°)_windsup')\n",
    "# index_speed = df_targets.columns.get_loc('vitesse_vent_(km/h)_windsup')\n",
    "\n",
    "train_and_evaluate_model(model_type, df, features_columns, targets_columns, sequence_length=10, learning_rate=0.001, batch_size=64, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import LSTM, GRU, Conv1D, MaxPooling1D, Dense, Flatten, Input, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from math import sqrt\n",
    "\n",
    "def filter_data(df):\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df_filtered_month = df[df['date'].dt.month.isin([7, 8])]\n",
    "    df_filtered_month_hour = df_filtered_month[(df_filtered_month['date'].dt.hour >= 8) & (df_filtered_month['date'].dt.hour <= 20)]\n",
    "    return df_filtered_month_hour\n",
    "\n",
    "\n",
    "def create_sequences(X, y, sequence_length=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - sequence_length):\n",
    "        Xs.append(X.iloc[i:(i + sequence_length)].values)\n",
    "        ys.append(y.iloc[i + sequence_length].values)\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(model_type, df, features_columns, targets_columns, sequence_length=10, learning_rate=0.001, batch_size=64, epochs=100):\n",
    "    df_filtered = filter_data(df)\n",
    "    df_features = df_filtered[features_columns]\n",
    "    df_targets = df_filtered[targets_columns]\n",
    "    logger.debug(f\"\\n df_filtered:\\n{df_filtered.head(5)} \")\n",
    "    logger.debug(f\"\\n df_filtered:\\n{df_filtered.head(5)} \")\n",
    "    logger.debug(f\"\\n liste colonnes df_features:\\n{df_features.columns.tolist()} \")\n",
    "    logger.debug(f\"\\n liste colonnes df_targets:\\n{df_targets.columns.tolist()} \")\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_features_scaled = scaler.fit_transform(df_features)\n",
    "    pca = PCA(n_components=0.95).fit_transform(df_features_scaled)\n",
    "    df_pca = pd.DataFrame(pca, index=df_filtered.index)\n",
    "    df_final = pd.concat([df_pca, df_targets], axis=1)\n",
    "    logger.debug(f\"\\n df_final:\\n{df_final} \")\n",
    "    logger.debug(f\"\\n df_features_scaled:\\n{df_features_scaled} \")\n",
    "\n",
    "    X = df_final.drop(targets_columns, axis=1)\n",
    "    y = df_final[targets_columns]\n",
    "\n",
    "    X_seq, y_seq = create_sequences(X, y, sequence_length)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
    "\n",
    "    logger.debug(f\"\\n X_train:\\n{X_train[0:10]} \")\n",
    "    logger.debug(f\"\\n X_test:\\n{X_test[0:10]} \")\n",
    "    logger.debug(f\"\\n y_train:\\n{y_train[0:10]} \")\n",
    "    logger.debug(f\"\\n y_test:\\n{y_test[0:10]} \")\n",
    "\n",
    "\n",
    "    # model = Sequential()\n",
    "    # if model_type == 'LSTM':\n",
    "    #     model.add(Input(shape=(sequence_length, X_train.shape[2])))\n",
    "    #     model.add(LSTM(100, return_sequences=True))\n",
    "    #     model.add(LSTM(100))\n",
    "    # elif model_type == 'GRU':\n",
    "    #     model.add(Input(shape=(sequence_length, X_train.shape[2])))\n",
    "    #     model.add(GRU(100, return_sequences=True))\n",
    "    #     model.add(GRU(100))\n",
    "    # elif model_type == 'SimpleRNN':\n",
    "    #     model.add(Input(shape=(sequence_length, X_train.shape[2])))\n",
    "    #     model.add(SimpleRNN(100, return_sequences=True))\n",
    "    #     model.add(SimpleRNN(100))\n",
    "    # elif model_type == 'Conv1D':\n",
    "    #     model.add(Input(shape=(sequence_length, X_train.shape[2])))\n",
    "    #     model.add(Conv1D(64, 3, activation='relu'))\n",
    "    #     model.add(MaxPooling1D(2))\n",
    "    #     model.add(Flatten())\n",
    "    # model.add(Dense(50, activation='relu'))\n",
    "    # model.add(Dense(len(targets_columns)))\n",
    "\n",
    "    # model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    # history = model.fit(X_train,\n",
    "    #                     y_train,\n",
    "    #                     epochs=epochs,\n",
    "    #                     batch_size=batch_size,\n",
    "    #                     validation_split=0.2,\n",
    "    #                     verbose=1,\n",
    "    #                     callbacks=[EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)])\n",
    "    model = Sequential()\n",
    "    if model_type == 'LSTM':\n",
    "        model.add(Input(shape=(sequence_length, X_train.shape[2])))\n",
    "        model.add(LSTM(100, return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(100))\n",
    "    elif model_type == 'GRU':\n",
    "        model.add(Input(shape=(sequence_length, X_train.shape[2])))\n",
    "        model.add(GRU(100, return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(GRU(100))\n",
    "    elif model_type == 'Conv1D':\n",
    "        model.add(Input(shape=(sequence_length, X_train.shape[2])))\n",
    "        model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "        model.add(MaxPooling1D(2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(len(targets_columns)))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.2,\n",
    "                        verbose=1,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)])\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "\n",
    "    # Calcul des métriques pour la direction et la vitesse\n",
    "    mse_direction = mean_squared_error(y_test[:, index_direction], predictions[:, index_direction])\n",
    "    rmse_direction = sqrt(mse_direction)\n",
    "    mae_direction = mean_absolute_error(y_test[:, index_direction], predictions[:, index_direction])\n",
    "\n",
    "    mse_speed = mean_squared_error(y_test[:, index_speed], predictions[:, index_speed])\n",
    "    rmse_speed = sqrt(mse_speed)\n",
    "    mae_speed = mean_absolute_error(y_test[:, index_speed], predictions[:, index_speed])\n",
    "\n",
    "    print(f\"Learning Rate: {lr}, Batch Size: {batch_size}\")\n",
    "    print(\"Direction - MSE:\", mse_direction, \"RMSE:\", rmse_direction, \"MAE:\", mae_direction)\n",
    "    print(\"Speed - MSE:\", mse_speed, \"RMSE:\", rmse_speed, \"MAE:\", mae_speed)\n",
    "\n",
    "\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "    print(f\"Model Type: {model_type} - MSE: {mse}, RMSE: {rmse}, MAE: {mae}\")\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_type} Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Test multiple models\n",
    "#  TEST\n",
    "# Chargement et préparation du dataframe\n",
    "df = df_concatenated_1h.copy()\n",
    "\n",
    "\n",
    "# Séparation des features (pour l'ACP) et des targets (non affectées par l'ACP)\n",
    "features_columns = [col for col in df.columns if ('vitesse' in col or 'direction' in col) and 'windsup' not in col]\n",
    "targets_columns = ['direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']\n",
    "logger.debug(f\"\\n features_columns:\\n{features_columns} \")\n",
    "# logger.debug(f\"\\n targets_columns:\\n{targets_columns} \")\n",
    "model_types = ['LSTM', 'GRU', 'Conv1D', 'SimpleRNN']\n",
    "\n",
    "for m_type in model_types:\n",
    "    print(f\"Testing {m_type}\")\n",
    "    train_and_evaluate_model(m_type, df, features_columns, targets_columns, sequence_length=10, learning_rate=0.001, batch_size=64, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdefa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconfigurer_logging(params={\"niveau_log\": 'DEBUG'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39623246",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e80f84",
   "metadata": {},
   "source": [
    "### filtre le dataframe\n",
    "\n",
    "filtre les mois de juillet et aout sur les heures de 8h à 20h\n",
    "\n",
    "regroupe les colonnes en prenant la moyenne des colonnes temperature, pression, vitesse, direction... en gardant les colonnes  à predire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49122c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filter_data(df):\n",
    "    \"\"\"\n",
    "    Filtrer le DataFrame pour inclure seulement les données des mois de juillet et août\n",
    "    et les heures entre 8h et 20h.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame contenant les données météorologiques.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame filtré selon les critères spécifiés.\n",
    "    \"\"\"\n",
    "    logging.debug(\"Début du filtrage des données par date.\")\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df_filtered_month = df[df['date'].dt.month.isin([7, 8])]\n",
    "    df_filtered_month_hour = df_filtered_month[(df_filtered_month['date'].dt.hour >= 8) & (df_filtered_month['date'].dt.hour <= 20)]\n",
    "    logging.debug(f\"DataFrame après filtrage par mois et heures (5 premières lignes) :\\n{df_filtered_month_hour.head()}\")\n",
    "    logging.debug(f\"Colonnes après filtrage par mois et heures: {df_filtered_month_hour.columns.tolist()}\")\n",
    "    return df_filtered_month_hour\n",
    "\n",
    "features_to_include = ['temperature', 'pression', 'humidity', 'Enthalpie']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def construction_df(df, features_to_include):\n",
    "    \"\"\"\n",
    "    Créer un DataFrame en filtrant les données et en ajoutant des caractéristiques météorologiques, incluant les colonnes à predire et les colonnes à regrouper\n",
    "    Créer des colonnes à partir de \"feature_to_include' en prenant la moyenne de toutes les colonnes traitant la même valeur\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame initial contenant les données météorologiques.\n",
    "        features_to_include(list): Liste des colonnes que l'on veut regrouper\n",
    "    Returns:\n",
    "        DataFrame: DataFrame préparé avec les caractéristiques nécessaires pour l'analyse.\n",
    "    \"\"\"\n",
    "    logging.debug(\"Début de la construction du DataFrame préparé.\")\n",
    "    df_filtered_month_hour = filter_data(df)\n",
    "    df = df_filtered_month_hour.copy()\n",
    "    logging.debug(f\"Colonnes cibles extraites (5 premières lignes) :\\n{df.head(5)}\")\n",
    "    logger.debug(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n",
    "    targets_columns = ['direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']\n",
    "    df_targets_columns = df[targets_columns]\n",
    "    logging.debug(f\"Colonnes cibles extraites (5 premières lignes) :\\n{df_targets_columns.head(5)}\")\n",
    "\n",
    "    # Traitement et ajout des caractéristiques météorologiques\n",
    "\n",
    "    for feature in features_to_include:\n",
    "        feature_cols = [col for col in df.columns if (feature in col) and  'windsup' not in col ]\n",
    "        if feature_cols:\n",
    "            df[feature] = df[feature_cols].mean(axis=1)\n",
    "            logging.debug(f\"Moyenne calculée pour {feature} (5 premières lignes) :\\n{df[feature].head(5)}\")\n",
    "\n",
    "    # Nettoyage des colonnes inutiles\n",
    "    cols_to_drop = [col for col in df.columns if '_' in col and col ]\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "    logging.debug(f\"DataFrame après suppression des colonnes inutiles :\\n{df.head(5)}\")\n",
    "    logging.debug(f\"Colonnes finales du DataFrame préparé: {df.columns.tolist()}\")\n",
    "\n",
    "    # Fusion avec les colonnes cibles\n",
    "    df = pd.concat([df, df_targets_columns], axis=1)\n",
    "    logging.debug(f\"DataFrame final après fusion avec les colonnes cibles :\\n{df.head(5)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Chargement et préparation du dataframe\n",
    "# Chargement et préparation du dataframe\n",
    "df = df_concatenated_1h.copy()\n",
    "df_prepared = construction_df(df)\n",
    "logger.debug(f\"\\n df_prepared:\\n{df_prepared.head(5)} \")\n",
    "logger.debug(f\"\\n liste colonnes df_prepared:\\n{df_prepared.columns.tolist()} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, Conv1D, Dense, Dropout, Flatten, Input, MaxPooling1D, SimpleRNN, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def filter_data(df):\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df_filtered_month = df[df['date'].dt.month.isin([7, 8])]\n",
    "    df_filtered_month_hour = df_filtered_month[(df_filtered_month['date'].dt.hour >= 8) & (df_filtered_month['date'].dt.hour <= 20)]\n",
    "    return df_filtered_month_hour\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assurez-vous que ces colonnes sont présentes dans votre DataFrame\n",
    "features_to_include = ['temperature', 'pression', 'humidity']\n",
    "df['temperature'] = [col for col in df.columns if 'temperature' in col ]\n",
    "df['pression'] = [col for col in df.columns if 'pression' in col]\n",
    "df['humidite'] = [col for col in df.columns if 'humidity' in col]\n",
    "logger.debug(f\"\\ndf['temperature'] :\\n{df['temperature']} \")\n",
    "logger.debug(f\"\\ndf['pression']  :\\n{df['pression'] } \")\n",
    "logger.debug(f\"\\n df['humidite']:\\n{df['humidite']} \")\n",
    "\n",
    "def apply_fourier_transform(df, features):\n",
    "    \"\"\"\n",
    "    Applique la transformation de Fourier sur les caractéristiques spécifiées et enregistre les amplitudes.\n",
    "    Fréquence et Amplitude: Les transformations de Fourier permettent d'analyser les fréquences présentes dans les données de séries temporelles et d'extraire\n",
    "    des informations sur l'amplitude des cycles. Cela peut être particulièrement utile pour la prédiction de phénomènes périodiques tels que les motifs météorologiques.\n",
    "    Extraction de Caractéristiques: Ces transformations peuvent aider à distinguer les caractéristiques qui sont peut-être moins évidentes dans le domaine\n",
    "    temporel mais plus prononcées dans le domaine fréquentiel, fournissant ainsi des informations supplémentaires qui pourraient améliorer la performance du modèle.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Le dataframe source.\n",
    "    features (list): Liste des noms des caractéristiques à transformer.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Le dataframe avec les caractéristiques transformées de Fourier ajoutées.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        fft_values = np.fft.fft(df[feature].fillna(0))  # Utiliser fillna(0) ou une autre méthode pour gérer les NaN\n",
    "        df[f'{feature}_fft_amplitude'] = np.abs(fft_values)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Application de la transformation de Fourier\n",
    "features_to_transform = ['direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']\n",
    "df = apply_fourier_transform(df, features_to_transform)\n",
    "\n",
    "def create_sequences(X, y, sequence_length):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - sequence_length):\n",
    "        Xs.append(X.iloc[i:(i + sequence_length)].values)\n",
    "        ys.append(y.iloc[i + sequence_length].values)\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def train_and_evaluate_model(model_type, df, features_columns, targets_columns, sequence_length=10, learning_rate=0.001, batch_size=64, epochs=100):\n",
    "    # Data filtering and feature extraction\n",
    "    df_filtered = filter_data(df)\n",
    "    df_features = df_filtered[features_columns]\n",
    "    df_targets = df_filtered[targets_columns]\n",
    "\n",
    "    # Normalization and PCA\n",
    "    # scaler = MinMaxScaler()\n",
    "    # scaler = StandardScaler()\n",
    "    scaler = RobustScaler()\n",
    "    df_features_scaled = scaler.fit_transform(df_features)\n",
    "    # df_features_scaled = scaler.fit_transform(df_features)\n",
    "\n",
    "    # Instead of PCA, directly use scaled features\n",
    "    df_final = pd.concat([pd.DataFrame(df_features_scaled, index=df_filtered.index), df_targets], axis=1)\n",
    "\n",
    "    # df_pca = PCA(n_components=0.95).fit_transform(df_features_scaled)\n",
    "    # df_pca = pd.DataFrame(df_pca, index=df_filtered.index)\n",
    "    # df_final = pd.concat([df_pca, df_targets], axis=1)\n",
    "\n",
    "    X = df_final.drop(targets_columns, axis=1)\n",
    "    y = df_final[targets_columns]\n",
    "\n",
    "    # Creating sequences\n",
    "    X_seq, y_seq = create_sequences(X, y, sequence_length)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Building the model\n",
    "    # model = build_model(model_type, (sequence_length, X_train.shape[2]), len(targets_columns))\n",
    "    model = build_model(model_type, (sequence_length, X_train.shape[2]), len(targets_columns))\n",
    "\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.2,\n",
    "                        verbose=1,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)])\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Calculating metrics for each target\n",
    "    direction_index = targets_columns.index('direction_(°)_windsup')\n",
    "    speed_index = targets_columns.index('vitesse_vent_(km/h)_windsup')\n",
    "\n",
    "    mse_direction = mean_squared_error(y_test[:, direction_index], predictions[:, direction_index])\n",
    "    rmse_direction = sqrt(mse_direction)\n",
    "    mae_direction = mean_absolute_error(y_test[:, direction_index], predictions[:, direction_index])\n",
    "\n",
    "    mse_speed = mean_squared_error(y_test[:, speed_index], predictions[:, speed_index])\n",
    "    rmse_speed = sqrt(mse_speed)\n",
    "    mae_speed = mean_absolute_error(y_test[:, speed_index], predictions[:, speed_index])\n",
    "\n",
    "    print(f\"Model Type: {model_type}\")\n",
    "    print(\"Direction - MSE: {:.2f}, RMSE: {:.2f}, MAE: {:.2f}\".format(mse_direction, rmse_direction, mae_direction))\n",
    "    print(\"Speed - MSE: {:.2f}, RMSE: {:.2f}, MAE: {:.2f}\".format(mse_speed, rmse_speed, mae_speed))\n",
    "\n",
    "    # Plotting loss\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_type} Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#1\n",
    "# def build_model(model_type, input_shape, output_dim):\n",
    "#     model = Sequential()\n",
    "#     if model_type == 'LSTM':\n",
    "#         model.add(LSTM(100, return_sequences=True, input_shape=input_shape))\n",
    "#         model.add(Dropout(0.2))\n",
    "#         model.add(LSTM(100))\n",
    "#     elif model_type == 'GRU':\n",
    "#         model.add(GRU(100, return_sequences=True, input_shape=input_shape))\n",
    "#         model.add(Dropout(0.2))\n",
    "#         model.add(GRU(100))\n",
    "#     elif model_type == 'SimpleRNN':\n",
    "#         model.add(SimpleRNN(100, return_sequences=True, input_shape=input_shape))\n",
    "#         model.add(Dropout(0.2))\n",
    "#         model.add(SimpleRNN(100))\n",
    "#     elif model_type == 'Conv1D':\n",
    "#         model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "#         model.add(MaxPooling1D(2))\n",
    "#         model.add(Flatten())\n",
    "#     model.add(Dense(50, activation='relu'))\n",
    "\n",
    "#     model.add(Dense(output_dim))\n",
    "#     return model\n",
    "\n",
    "# 2\n",
    "# def build_model(model_type, input_shape, output_dim):\n",
    "#     model = Sequential()\n",
    "\n",
    "#     # Input layer\n",
    "#     model.add(Input(shape=input_shape))\n",
    "\n",
    "#     # Adding model specific layers\n",
    "#     if model_type == 'LSTM':\n",
    "#         model.add(LSTM(100, return_sequences=True))\n",
    "#         model.add(Dropout(0.2))\n",
    "#         model.add(LSTM(100))\n",
    "#     elif model_type == 'GRU':\n",
    "#         model.add(GRU(100, return_sequences=True))\n",
    "#         model.add(Dropout(0.2))\n",
    "#         model.add(GRU(100))\n",
    "#     elif model_type == 'SimpleRNN':\n",
    "#         model.add(SimpleRNN(100, return_sequences=True))\n",
    "#         model.add(Dropout(0.2))\n",
    "#         model.add(SimpleRNN(100))\n",
    "#     elif model_type == 'Conv1D':\n",
    "#         model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "#         model.add(MaxPooling1D(2))\n",
    "#         model.add(Flatten())\n",
    "\n",
    "#     # Common layers\n",
    "#     model.add(Dense(50, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "#     # Output layer\n",
    "#     model.add(Dense(output_dim, activation='linear'))  # Use linear activation for output layer for regression tasks\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "def build_model(model_type, input_shape, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    if model_type == 'LSTM':\n",
    "        model.add(LSTM(128, return_sequences=True))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(LSTM(64))\n",
    "    elif model_type == 'GRU':\n",
    "        model.add(GRU(128, return_sequences=True))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(GRU(64))\n",
    "    elif model_type == 'SimpleRNN':\n",
    "        model.add(SimpleRNN(128, return_sequences=True))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(SimpleRNN(64))\n",
    "    elif model_type == 'Conv1D':\n",
    "        model.add(Conv1D(64, kernel_size=3, activation='tanh'))\n",
    "        model.add(MaxPooling1D(2))\n",
    "        model.add(Conv1D(128, kernel_size=3, activation='tanh'))\n",
    "        model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(output_dim, activation='linear'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Test multiple models\n",
    "#  TEST\n",
    "# Chargement et préparation du dataframe\n",
    "df = df_concatenated_1h.copy()\n",
    "\n",
    "\n",
    "# Séparation des features (pour l'ACP) et des targets (non affectées par l'ACP)\n",
    "# features_columns = [col for col in df.columns if ('vitesse' in col or 'direction' in col) and 'windsup' not in col]\n",
    "features_columns = [\n",
    "    col for col in df.columns if ('vitesse' in col or 'direction' in col or 'pre' in col or 'temp' in col or 'enth' in col or 'humi' in col) and 'windsup' not in col\n",
    "]\n",
    "targets_columns = ['direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']\n",
    "logger.debug(f\"\\n features_columns:\\n{features_columns} \")\n",
    "# logger.debug(f\"\\n targets_columns:\\n{targets_columns} \")\n",
    "# model_types = ['LSTM', 'GRU', 'Conv1D', 'SimpleRNN']\n",
    "model_types = ['LSTM', 'GRU']\n",
    "\n",
    "# # Example of usage\n",
    "# features_columns = [col for col in df.columns if ('vitesse' in col or 'direction' in col) and 'windsup' not in col]\n",
    "# targets_columns = ['direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']\n",
    "\n",
    "# model_types = ['LSTM', 'GRU', 'Conv1D', 'SimpleRNN']\n",
    "\n",
    "for m_type in model_types:\n",
    "    print(f\"Testing {m_type}\")\n",
    "    train_and_evaluate_model(m_type, df, features_columns, targets_columns, sequence_length=10, learning_rate=0.001, batch_size=64, epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c120d9",
   "metadata": {},
   "source": [
    "### Conclusions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c82f05",
   "metadata": {},
   "source": [
    "\n",
    "les resusltats sont meilleurs sans ACP et avec un standardScaler plutot qu'un mixmaxScaler et sur un LSTM et GRU\n",
    "robustScaler à peu d'impact\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02dd1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.error(f\"\\n liste colonnes:\\n{df.columns.tolist()} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a965b8",
   "metadata": {},
   "source": [
    "### scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "#-----------------\n",
    "# ETAPE 6 calcul des score\n",
    "#-----------------\n",
    "# # Fonction pour calculer RMSE\n",
    "# def rmse(y_true, y_pred):\n",
    "#     return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "# # Inverse scaling for a forecasted values\n",
    "# y_pred_rescaled = scaler_pca.inverse_transform(model_pca.predict(X_test))\n",
    "# y_test_rescaled = scaler_pca.inverse_transform(y_test)\n",
    "\n",
    "# # Calculating errors in original units\n",
    "# mse = mean_squared_error(y_test_rescaled, y_pred_rescaled)\n",
    "# rmse_val = rmse(y_test_rescaled, y_pred_rescaled)\n",
    "# mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\n",
    "\n",
    "# print(f\"MSE: {mse}\")\n",
    "# print(f\"RMSE: {rmse_val}\")\n",
    "# print(f\"MAE: {mae}\")\n",
    "\n",
    "# Inverser la normalisation\n",
    "y_pred_rescaled_target = scaler.inverse_transform(model.predict(X))\n",
    "y_test_rescaled_target = scaler.inverse_transform(y)\n",
    "\n",
    "# Calculer MSE, RMSE pour chaque caractéristique vitesse et direction\n",
    "mse_target = mean_squared_error(y_test_rescaled_target, y_pred_rescaled_target, multioutput='raw_values')\n",
    "rmse_target = np.sqrt(mse_target)\n",
    "mae_target = mean_absolute_error(y_test_rescaled_target, y_pred_rescaled_target, multioutput='raw_values')\n",
    "\n",
    "print(f\"# MSE Direction: {mse_target[0]}, MSE Vitesse: {mse_target[1]}\")\n",
    "print(f\"# RMSE Direction (°): {rmse_target[0]}, RMSE Vitesse (km/h): {rmse_target[1]}\")\n",
    "print(f\"# MAE Direction (°): {mae_target[0]}, MAE Vitesse (km/h): {mae_target[1]}\")\n",
    "\n",
    "# 457/523 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step\n",
    "# 523/523 ━━━━━━━━━━━━━━━━━━━━ 2s 3ms/step\n",
    "# MSE Direction: 364.1226662162421, MSE Vitesse: 21.62525023393399\n",
    "# RMSE Direction (°): 19.081998485909228, RMSE Vitesse (km/h): 4.650295714676002\n",
    "# MAE Direction (°): 3.3725974977607716, MAE Vitesse (km/h): 1.1353853343340423\n",
    "\n",
    "# MSE Direction: 159.70801079208314, MSE Vitesse: 13.21785107380873\n",
    "# RMSE Direction (°): 12.637563483206845, RMSE Vitesse (km/h): 3.635636268084134\n",
    "# MAE Direction (°): 2.137341936099574, MAE Vitesse (km/h): 0.8747371484824725\n",
    "\n",
    "# MSE Direction: 161.2048521557615, MSE Vitesse: 13.308952142407545\n",
    "# RMSE Direction (°): 12.696647280119327, RMSE Vitesse (km/h): 3.6481436570408716\n",
    "# MAE Direction (°): 2.1149303995351416, MAE Vitesse (km/h): 0.7793581791361492\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535bc73",
   "metadata": {},
   "source": [
    "### graphique LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270ed119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#----------------------\n",
    "# Étape 4: Prédiction et évaluation visuelle\n",
    "#----------------------\n",
    "\n",
    "# Prédiction\n",
    "predicted_pca = model_pca.predict(X_test)\n",
    "predicted_pca_values = scaler_pca.inverse_transform(predicted_pca)  # Retour aux valeurs originales\n",
    "\n",
    "# Comparaison visuelle\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(y_test[:, 0], label='Réel PC1')\n",
    "plt.plot(predicted_pca_values[:, 0], label='Prédit PC1')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(y_test[:, 1], label='Réel PC2')\n",
    "plt.plot(predicted_pca_values[:, 1], label='Prédit PC2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fdaa36",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb9435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Supposons que vous avez les DataFrames suivants (il faut créer ces DataFrames avec vos vraies données)\n",
    "# predictions_df : avec les colonnes 'timestamp', 'predicted_speed' (vitesse prédite), 'predicted_direction' (direction prédite)\n",
    "# true_values_df : avec les colonnes 'timestamp', 'true_speed' (vitesse réelle), 'true_direction' (direction réelle)\n",
    "\n",
    "# Prédiction\n",
    "predicted_pca = model_pca.predict(X_test)\n",
    "predicted_pca_values = scaler_pca.inverse_transform(predicted_pca)  # Retour aux valeurs originales\n",
    "\n",
    "# Déduire les timestamps des données de test basés sur la structure de vos données\n",
    "# Vous devrez ajuster cette partie selon la façon dont les séquences ont été générées\n",
    "timestamps_test = pd.date_range(start='2022-02-15 09:12:00', periods=len(predicted_pca_values), freq='6min')\n",
    "\n",
    "# Générer des index de 8h à 20h\n",
    "indices = [(time.hour >= 8) & (time.hour <= 20) for time in timestamps_test]\n",
    "\n",
    "# Filtrer les prédictions et les timestamps pour la période de 8h à 20h\n",
    "filtered_predicted_pca_values = predicted_pca_values[indices]\n",
    "filtered_timestamps = timestamps_test[indices]\n",
    "\n",
    "# Assurez-vous que vos y_test sont en format approprié pour l'inversion de la normalisation\n",
    "true_values_scaled = scaler_pca.transform(y_test)  # Si y_test n'est pas déjà normalisé\n",
    "filtered_true_values = scaler_pca.inverse_transform(true_values_scaled[indices])\n",
    "\n",
    "# Créer les DataFrames pour les prédictions et les vraies valeurs avec les timestamps filtrés\n",
    "predicted_df = pd.DataFrame({\n",
    "    'timestamp': filtered_timestamps,\n",
    "    'predicted_speed': filtered_predicted_pca_values[:, 1],  # Assurez-vous que c'est la bonne colonne\n",
    "    'predicted_direction': filtered_predicted_pca_values[:, 0]  # Assurez-vous que c'est la bonne colonne\n",
    "})\n",
    "\n",
    "true_values_df = pd.DataFrame({\n",
    "    'timestamp': filtered_timestamps,\n",
    "    'true_speed': filtered_true_values[:, 1],  # Assurez-vous que c'est la bonne colonne\n",
    "    'true_direction': filtered_true_values[:, 0]  # Assurez-vous que c'est la bonne colonne\n",
    "})\n",
    "\n",
    "# Tracé des graphiques en utilisant les nouveaux DataFrames\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "# Tracé de la vitesse du vent réelle et prédite\n",
    "axs[0].plot(predicted_df['timestamp'], predicted_df['predicted_speed'], label='Prédiction de la vitesse du vent (km/h)', color='orange')\n",
    "axs[0].plot(true_values_df['timestamp'], true_values_df['true_speed'], label='Vitesse réelle du vent (km/h)', color='blue', alpha=0.5)\n",
    "axs[0].set_ylabel('Vitesse (km/h)')\n",
    "axs[0].legend()\n",
    "\n",
    "# Tracé de la direction du vent réelle et prédite\n",
    "axs[1].plot(predicted_df['timestamp'], predicted_df['predicted_direction'], label='Prédiction de la direction du vent (°)', color='orange')\n",
    "axs[1].plot(true_values_df['timestamp'], true_values_df['true_direction'], label='Direction réelle du vent (°)', color='blue', alpha=0.5)\n",
    "axs[1].set_ylabel('Direction (°)')\n",
    "axs[1].legend()\n",
    "\n",
    "# Formatage des dates sur l'axe des x\n",
    "axs[1].xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%Y_%H:%M'))\n",
    "\n",
    "# Rotation des dates sur l'axe des x\n",
    "plt.setp(axs[1].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368772e9",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb93e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7936988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Assurez-vous que 'date' est la colonne d'index et qu'elle est de type DatetimeIndex\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Trouver les jours uniques où des données existent\n",
    "unique_days = df.index.normalize().unique()\n",
    "\n",
    "# Choisir un jour au hasard parmi les jours uniques\n",
    "random_day = np.random.choice(unique_days)\n",
    "\n",
    "# Filtrer les données pour ce jour spécifique de 8h à 20h\n",
    "selected_data = df.loc[random_day].between_time('08:00', '20:00')\n",
    "\n",
    "# Assurez-vous d'avoir suffisamment de données pour cette journée pour créer les séquences\n",
    "if len(selected_data) >= seq_length:\n",
    "    # Générer les séquences pour la journée choisie\n",
    "    # Vous devrez utiliser la même fonction de création de séquences que celle utilisée lors de la préparation des données\n",
    "    X_day, _ = create_sequences(selected_data[['direction_(°)_windsup', 'vitesse_vent_(km/h)_windsup']].values, seq_length)\n",
    "\n",
    "    # Prédiction pour la journée sélectionnée\n",
    "    predicted_day = model_pca.predict(X_day)\n",
    "    predicted_day_values = scaler_pca.inverse_transform(predicted_day)\n",
    "\n",
    "    # Prendre les timestamps pour le graphe\n",
    "    timestamps_day = selected_data.index[seq_length - 1:]\n",
    "\n",
    "    # Tracé des prédictions\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
    "\n",
    "    # Tracé pour la vitesse du vent\n",
    "    axs[0].plot(timestamps_day, predicted_day_values[:, 1], label='Prédiction de la vitesse du vent (km/h)', color='orange')\n",
    "    axs[0].set_ylabel('Vitesse (km/h)')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Tracé pour la direction du vent\n",
    "    axs[1].plot(timestamps_day, predicted_day_values[:, 0], label='Prédiction de la direction du vent (°)', color='orange')\n",
    "    axs[1].set_ylabel('Direction (°)')\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Formatage des dates sur l'axe des x\n",
    "    axs[1].xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%Y_%Hh%M'))\n",
    "\n",
    "    # Rotation des dates sur l'axe des x\n",
    "    plt.setp(axs[1].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Pas assez de données pour ce jour pour créer les séquences.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3e6822",
   "metadata": {},
   "source": [
    "## TSFM(ZS)  modele de google  Zero  shot modele sur des séries temporelles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e30729",
   "metadata": {},
   "source": [
    "voir sur https://huggingface.co/datasets/monash_tsf\n",
    "\n",
    "papier: \n",
    "\n",
    "https://arxiv.org/html/2310.10688v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be480bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "! pip3 install --upgrade --quiet google-cloud-aiplatform\n",
    "!setx PATH \"%PATH%;C:\\Users\\romar\\AppData\\Roaming\\Python\\Python311\\Scripts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9745ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "url = \"https://github.com/google-research/timesfm/archive/refs/heads/main.zip\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"timesfm.zip\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Modèles pré-entrainés\n",
    "\n",
    "\n",
    "model_name = \"timesfm_zs_wind\"\n",
    "\n",
    "model_url = \"https://storage.googleapis.com/timesfm/models/\" + model_name + \".h5\"\n",
    "\n",
    "model = tf.keras.models.load_model(model_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f873d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from timesfm import TimesFM\n",
    "\n",
    "# Chargement du modèle TimesFM(ZS)\n",
    "model = TimesFM.load(\"path/to/model\")\n",
    "\n",
    "# Définition des données d'entrée\n",
    "features = {\n",
    "    \"date\": tf.constant(\"2023-03-15 14:05:00\"),\n",
    "    \"latitude\": tf.constant(43.296482),\n",
    "    \"longitude\": tf.constant(5.370078),\n",
    "}\n",
    "\n",
    "# Prédiction de la vitesse du vent\n",
    "prediction = model.predict(features)\n",
    "\n",
    "# Affichage de la prédiction\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850cf09b",
   "metadata": {},
   "source": [
    "# Et la suite...\n",
    "\n",
    "\n",
    "Poursuite des tests à faire sur: \n",
    "- Gp\t\n",
    "- Arima\t\n",
    "- TCN\t\n",
    "- N-BATTEMENTS\t\n",
    "- N-HiTS\t\n",
    "- llmtime(ZS)\t\n",
    "- TimesFM(ZS)\t\n",
    "- NAÏF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
