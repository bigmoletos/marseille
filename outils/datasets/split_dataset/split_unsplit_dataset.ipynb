{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset en fichiers de 50 mo afin d'être compatible avec github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# def split_dataset_50mo(dataset, output_dir=\"dataset_50mo\"):\n",
    "#     \"\"\"\n",
    "#     Divise un dataset en plusieurs fichiers plus petits.\n",
    "\n",
    "#     Paramètres :\n",
    "#     - dataset (str) : Le chemin vers le fichier du dataset.\n",
    "#     - output_dir (str) : Le répertoire pour stocker les fichiers divisés.\n",
    "#     - chunk_size (int) : Le nombre de lignes par fichier divisé.\n",
    "#     \"\"\"\n",
    "#     chunk_size=50000\n",
    "#     try:\n",
    "#         # Crée le répertoire de sortie s'il n'existe pas\n",
    "#         if not os.path.exists(output_dir):\n",
    "#             os.makedirs(output_dir)\n",
    "\n",
    "#         # Détermine le format du fichier\n",
    "#         _, file_extension = os.path.splitext(dataset)\n",
    "#         file_format = file_extension[1:]\n",
    "\n",
    "#         # Lit et divise le dataset\n",
    "#         chunk_number = 0\n",
    "#         for chunk in pd.read_csv(dataset, chunksize=chunk_size):\n",
    "#             chunk_file = os.path.join(output_dir, f\"chunk_{chunk_number}.{file_format}\")\n",
    "#             chunk.to_csv(chunk_file, index=False)\n",
    "#             chunk_number += 1\n",
    "#     except Exception as e:\n",
    "#         print(f\"Une erreur est survenue lors de la division du dataset : {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  test\n",
    "# dataset = 'Station44_test.csv'\n",
    "# dossier_cible=\"Datasets/dataset_splited_50mo\"\n",
    "# split_dataset_50mo(dataset, dossier_cible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def split_dataset(dataset, chunk_size=50000):\n",
    "    \"\"\"\n",
    "    Divise un dataset en plusieurs fichiers plus petits.\n",
    "\n",
    "    Paramètres :\n",
    "    - dataset (str) : Le chemin vers le fichier du dataset.\n",
    "    - chunk_size (int) : Le nombre de lignes ou d'enregistrements par fichier divisé.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Récupère le nom de base du fichier sans l'extension\n",
    "        base_name = os.path.basename(dataset)\n",
    "        base_name_without_ext = os.path.splitext(base_name)[0]\n",
    "\n",
    "        # Crée le répertoire de sortie nommé d'après le fichier original suivi de \"_splitted\"\n",
    "        output_dir = f\"{base_name_without_ext}_splitted\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # Détermine le format du fichier et sélectionne la méthode de lecture appropriée\n",
    "        _, file_extension = os.path.splitext(dataset)\n",
    "        if file_extension.lower() == '.csv':\n",
    "            read_function = pd.read_csv\n",
    "            save_function = pd.DataFrame.to_csv\n",
    "            read_args = {}\n",
    "            save_args = {'index': False}\n",
    "        elif file_extension.lower() == '.json':\n",
    "            read_function = pd.read_json\n",
    "            save_function = pd.DataFrame.to_json\n",
    "            read_args = {'lines': True}\n",
    "            save_args = {'orient': 'records', 'lines': True}\n",
    "        elif file_extension.lower() == '.txt' or file_extension.lower() == '.tsv':\n",
    "            read_function = pd.read_csv\n",
    "            save_function = pd.DataFrame.to_csv\n",
    "            read_args = {'sep': '\\t'}\n",
    "            save_args = {'sep': '\\t', 'index': False}\n",
    "        else:\n",
    "            raise ValueError(f\"Format de fichier non supporté : {file_extension}\")\n",
    "\n",
    "        # Lit et divise le dataset\n",
    "        chunk_number = 0\n",
    "        for chunk in read_function(dataset, chunksize=chunk_size, **read_args):\n",
    "            chunk_file = os.path.join(output_dir, f\"{base_name_without_ext}_{chunk_number}{file_extension}\")\n",
    "            save_function(chunk, chunk_file, **save_args)\n",
    "            chunk_number += 1\n",
    "        print(f\"Dataset divisé en {chunk_number} fichiers dans le répertoire '{output_dir}'.\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue lors de la division du dataset : {e}\")\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# split_dataset('chemin/vers/le/fichier/Station44_test.csv', chunk_size=100)\n",
    "# split_dataset('chemin/vers/le/fichier/data.json', chunk_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset divisé en 3 fichiers dans le répertoire 'Station44_test_splitted'.\n"
     ]
    }
   ],
   "source": [
    "dataset = 'Station44_test.csv'\n",
    "# dataset = 'clean_urls.json'\n",
    "# dataset = 'primer-dataset.json'\n",
    "# dossier_cible=\"Datasets/dataset_splited_50mo\"\n",
    "# split_dataset_50mo(dataset)\n",
    "split_dataset_50mo(dataset, chunk_size=500000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def unsplit_dataset(output_dir):\n",
    "    \"\"\"\n",
    "    Combine plusieurs fichiers plus petits en un seul dataset et le nomme d'après le répertoire d'origine.\n",
    "\n",
    "    Paramètres :\n",
    "    - output_dir (str) : Le répertoire contenant les fichiers divisés.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Vérifie si le répertoire de sortie existe\n",
    "        if not os.path.exists(output_dir):\n",
    "            raise FileNotFoundError(f\"Le répertoire spécifié n'existe pas : {output_dir}\")\n",
    "\n",
    "        # Obtient une liste de tous les fichiers divisés\n",
    "        chunk_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if not f.startswith('.')]\n",
    "\n",
    "        # Trie les fichiers divisés\n",
    "        chunk_files.sort()\n",
    "\n",
    "        # Vérifie si des fichiers à combiner ont été trouvés\n",
    "        if not chunk_files:\n",
    "            raise FileNotFoundError(f\"Aucun fichier à combiner n'a été trouvé dans : {output_dir}\")\n",
    "\n",
    "        # Déduit l'extension de fichier pour déterminer la méthode de lecture et d'écriture\n",
    "        _, file_extension = os.path.splitext(chunk_files[0])\n",
    "\n",
    "        # Crée le nom pour le fichier de sortie\n",
    "        base_output_dir = os.path.basename(output_dir)\n",
    "        base_name_unsplitted = base_output_dir.replace(\"_splitted\", \"_unsplitted\")\n",
    "        output_file = f\"{base_name_unsplitted}{file_extension}\"\n",
    "\n",
    "        # Détermine la méthode d'écriture en fonction de l'extension du fichier\n",
    "        if file_extension.lower() == '.json':\n",
    "            df_list = [pd.read_json(f, lines=True) for f in chunk_files]\n",
    "            df = pd.concat(df_list)\n",
    "            df.to_json(output_file, orient='records', lines=True)\n",
    "        elif file_extension.lower() == '.csv':\n",
    "            df_list = [pd.read_csv(f) for f in chunk_files]\n",
    "            df = pd.concat(df_list)\n",
    "            df.to_csv(output_file, index=False)\n",
    "        elif file_extension.lower() == '.txt' or file_extension.lower() == '.tsv':\n",
    "            sep = '\\t' if file_extension.lower() == '.tsv' else ' '\n",
    "            df_list = [pd.read_csv(f, sep=sep) for f in chunk_files]\n",
    "            df = pd.concat(df_list)\n",
    "            df.to_csv(output_file, sep=sep, index=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Extension de fichier non prise en charge : {file_extension}\")\n",
    "\n",
    "        print(f\"Dataset combiné avec succès et enregistré sous : {output_file}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur est survenue lors de la combinaison des fichiers : {e}\")\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# dossier_cible = \"Station44_test_splitted\"\n",
    "# unsplit_dataset(dossier_cible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset combiné avec succès et enregistré sous : primer-dataset_unsplitted.json\n"
     ]
    }
   ],
   "source": [
    "#  test unsplit\n",
    "# dossier_cible=\"Station44_test_splitted\"\n",
    "dossier_cible=\"primer-dataset_splitted\"\n",
    "unsplit_dataset(output_dir=dossier_cible)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test des differences entre les datasets apres reconstitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df1 :\n",
      "(1297315, 1) \n",
      "\n",
      "\n",
      " df2 :\n",
      "(1297315, 1) \n",
      "\n",
      "Les deux datasets sont identiques : True\n",
      "Les différences entre les deux datasets sont :\n",
      "Empty DataFrame\n",
      "Columns: [Station Id;Date Timestamp [UTC];Date Txt [UTC]; Average Wind Speed [nds];Min Wind Speed [nds];Max Wind Speed [nds];Wind Direction [degree];Wind Direction [txt], _merge]\n",
      "Index: []\n",
      "\n",
      "differences_df1 :\n",
      "Empty DataFrame\n",
      "Columns: [Station Id;Date Timestamp [UTC];Date Txt [UTC]; Average Wind Speed [nds];Min Wind Speed [nds];Max Wind Speed [nds];Wind Direction [degree];Wind Direction [txt], _merge]\n",
      "Index: [] \n",
      "\n",
      "\n",
      " differences_df2:\n",
      "Empty DataFrame\n",
      "Columns: [Station Id;Date Timestamp [UTC];Date Txt [UTC]; Average Wind Speed [nds];Min Wind Speed [nds];Max Wind Speed [nds];Wind Direction [degree];Wind Direction [txt], _merge]\n",
      "Index: [] \n",
      "\n",
      "df1 colonnes  ['Station Id;Date Timestamp [UTC];Date Txt [UTC]; Average Wind Speed [nds];Min Wind Speed [nds];Max Wind Speed [nds];Wind Direction [degree];Wind Direction [txt]']\n",
      "df2 colonnes  ['Station Id;Date Timestamp [UTC];Date Txt [UTC]; Average Wind Speed [nds];Min Wind Speed [nds];Max Wind Speed [nds];Wind Direction [degree];Wind Direction [txt]']\n"
     ]
    }
   ],
   "source": [
    "df1=pd.read_csv(\"Station44_test.csv\")\n",
    "df2=pd.read_csv(\"Station44_test_unsplitted.csv\")\n",
    "\n",
    "print(f\"\\ndf1 :\\n{df1.shape} \\n\")\n",
    "print(f\"\\n df2 :\\n{df2.shape} \\n\")\n",
    "\n",
    "sont_identiques = df1.equals(df2)\n",
    "print(\"Les deux datasets sont identiques :\", sont_identiques)\n",
    "\n",
    "# Fusionner les deux DataFrames et indiquer l'origine de chaque ligne\n",
    "df = pd.merge(df1, df2, how='outer', indicator=True)\n",
    "\n",
    "# Trouver les lignes qui sont différentes\n",
    "differences = df[df['_merge'] != 'both']\n",
    "\n",
    "print(\"Les différences entre les deux datasets sont :\")\n",
    "print(differences)\n",
    "\n",
    "differences_df1 = differences[differences['_merge'] == 'left_only']\n",
    "\n",
    "print(f\"\\ndifferences_df1 :\\n{differences_df1} \\n\")\n",
    "\n",
    "differences_df2 = differences[differences['_merge'] == 'right_only']\n",
    "print(f\"\\n differences_df2:\\n{differences_df2} \\n\")\n",
    "\n",
    "# print(f\"df1  {df1.describe()}\")\n",
    "print(f\"df1 colonnes  {df1.columns.tolist()}\")\n",
    "# print(f\"df2  {df2.describe()}\")\n",
    "print(f\"df2 colonnes  {df2.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
